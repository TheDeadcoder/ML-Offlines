{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n",
      "Number of test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "train_data_path = 'data/train.csv'\n",
    "test_data_path = 'data/test.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "print(\"Number of training examples:\", train_data.shape[0])\n",
    "print(\"Number of test examples:\", test_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**# Separate features and labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop('label', axis=1).values\n",
    "y_train = train_data['label'].values\n",
    "\n",
    "X_test = test_data.drop('label', axis=1).values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize the pixel values (scale to [0, 1])**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert labels to one-hot encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "num_classes = 10\n",
    "y_train_one_hot = one_hot_encode(y_train, num_classes)\n",
    "y_test_one_hot = one_hot_encode(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the training data into training and validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train_one_hot, y_val_one_hot = train_test_split(\n",
    "    X_train, y_train_one_hot, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Neural Network Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dense Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "\n",
    "### Initialization:\n",
    "- **Weights (`self.w`):**\n",
    "  - Initialized with a scaled random normal distribution.\n",
    "  - `np.sqrt(2.0 / input_dim)`: He initialization, suitable for ReLU activations.\n",
    "\n",
    "- **Biases (`self.b`):**\n",
    "  - Initialized to zeros.\n",
    "\n",
    "- **Gradients (`self.dw`, `self.db`):**\n",
    "  - Placeholders for gradients computed during backpropagation.\n",
    "\n",
    "### Adam Parameters:\n",
    "- **First Moment Vectors (`self.m_w`, `self.m_b`):**\n",
    "  - Moving averages of the gradients.\n",
    "\n",
    "- **Second Moment Vectors (`self.v_w`, `self.v_b`):**\n",
    "  - Moving averages of the squared gradients.\n",
    "\n",
    "### Cache (`self.input`):\n",
    "- Stores the input to the layer for use during backpropagation.\n",
    "\n",
    "## Theory:\n",
    "\n",
    "### Dense (Fully Connected) Layer:\n",
    "- Each neuron receives input from all neurons in the previous layer.\n",
    "- Computes `output = X * W + b`.\n",
    "\n",
    "### He Initialization:\n",
    "- Addresses the problem of vanishing/exploding gradients.\n",
    "- Suitable for layers followed by ReLU activation.\n",
    "\n",
    "\n",
    "### **Forward Pass Explanation:**\n",
    "  - Computes the linear transformation.\n",
    "  - Stores the input \\( X \\) for use in backpropagation.\n",
    "\n",
    "**Mathematical Operation:**\n",
    "  -  `output = X * W + b`.\n",
    "\n",
    "### **Backward Pass Explanation**:\n",
    "\n",
    "- **Gradients w.r.t Weights (`self.dw`):**\n",
    "  - Computed as the dot product of the transposed input and `grad_output`.\n",
    "\n",
    "- **Gradients w.r.t Biases (`self.db`):**\n",
    "  - Sum of `grad_output` along the batch dimension.\n",
    "\n",
    "- **Gradient w.r.t Input (`grad_input`):**\n",
    "  - Propagated backward to previous layers.\n",
    "  - Computed as the dot product of `grad_output` and transposed weights.\n",
    "\n",
    "## Theory:\n",
    "- Backpropagation involves computing the gradients of the loss with respect to each parameter.\n",
    "- Uses the chain rule from calculus.\n",
    "\n",
    "**Explanation for updating parameters:**\n",
    "  - **Computing Moving Averages:**\n",
    "    - `self.m_w`, `self.m_b`: Exponential moving averages of gradients (first moment).\n",
    "    - `self.v_w`, `self.v_b`: Exponential moving averages of squared gradients (second moment).\n",
    "  \n",
    "  - **Bias Correction:**\n",
    "    - Adjusts the moments to account for their initialization at zero.\n",
    "    - `m_w_hat`, `v_w_hat`: Corrected moments.\n",
    "\n",
    "  - **Parameter Updates:**\n",
    "    - Updates weights and biases using the Adam update rule.\n",
    "\n",
    "# Parameter Update Theory:\n",
    "\n",
    "- **Adam Optimizer:**\n",
    "  - Combines the advantages of both AdaGrad and RMSProp.\n",
    "\n",
    "  - **First Moment Estimate (Mean):**\n",
    "    - \\( m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\)\n",
    "\n",
    "  - **Second Moment Estimate (Variance):**\n",
    "    - \\( v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\)\n",
    "\n",
    "  - **Bias Correction:**\n",
    "    - Adjusts for the initial bias towards zero moments.\n",
    "\n",
    "  - **Parameter Update:**\n",
    "    - \\( \\theta_t = \\theta_{t-1} - \\alpha \\frac{m_t}{\\sqrt{v_t} + \\epsilon} \\)\n",
    "\n",
    "- **Parameters:**\n",
    "  - `beta1` and `beta2`:\n",
    "    - Hyperparameters controlling the decay rates of the moving averages.\n",
    "    - Commonly set to `beta1=0.9` and `beta2=0.999`.\n",
    "\n",
    "  - `epsilon`:\n",
    "    - Small constant to prevent division by zero.\n",
    "\n",
    "  - `t`:\n",
    "    - Timestep, incremented after each parameter update.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialize weights and biases\n",
    "        self.W = np.random.randn(input_dim, output_dim) * np.sqrt(2.0 / input_dim)\n",
    "        self.b = np.zeros((1, output_dim))\n",
    "        \n",
    "        # Gradients\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "        # Adam parameters\n",
    "        self.m_W = np.zeros_like(self.W)\n",
    "        self.v_W = np.zeros_like(self.W)\n",
    "        self.m_b = np.zeros_like(self.b)\n",
    "        self.v_b = np.zeros_like(self.b)\n",
    "        \n",
    "        # Cache for backpropagation\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        output = np.dot(X, self.W) + self.b\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # Compute gradients\n",
    "        self.dW = np.dot(self.input.T, grad_output)\n",
    "        self.db = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        grad_input = np.dot(grad_output, self.W.T)\n",
    "        return grad_input\n",
    "    \n",
    "    def update_params(self, learning_rate, beta1, beta2, epsilon, t):\n",
    "        # Update weights and biases using Adam optimizer\n",
    "        self.m_W = beta1 * self.m_W + (1 - beta1) * self.dW\n",
    "        self.v_W = beta2 * self.v_W + (1 - beta2) * (self.dW ** 2)\n",
    "        m_W_hat = self.m_W / (1 - beta1 ** t)\n",
    "        v_W_hat = self.v_W / (1 - beta2 ** t)\n",
    "        self.W -= learning_rate * m_W_hat / (np.sqrt(v_W_hat) + epsilon)\n",
    "        \n",
    "        self.m_b = beta1 * self.m_b + (1 - beta1) * self.db\n",
    "        self.v_b = beta2 * self.v_b + (1 - beta2) * (self.db ** 2)\n",
    "        m_b_hat = self.m_b / (1 - beta1 ** t)\n",
    "        v_b_hat = self.v_b / (1 - beta2 ** t)\n",
    "        self.b -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Scale Parameter (`self.gamma`):**\n",
    "  - Learns how much to scale the normalized output.\n",
    "\n",
    "- **Shift Parameter (`self.beta`):**\n",
    "  - Learns how much to shift the normalized output.\n",
    "\n",
    "- **`epsilon`:**\n",
    "  - Small constant to prevent division by zero.\n",
    "\n",
    "- **`momentum`:**\n",
    "  - Controls the updating of running estimates.\n",
    "\n",
    "- **Running Estimates:**\n",
    "  - `self.running_mean` and `self.running_var`:\n",
    "    - Used during inference to normalize data with global statistics.\n",
    "\n",
    "- **Gradients and Caches:**\n",
    "  - **Gradients (`self.dgamma`, `self.dbeta`):**\n",
    "    - For updating `gamma` and `beta`.\n",
    "  \n",
    "  - **Caches:**\n",
    "    - Stores intermediate variables needed for backpropagation.\n",
    "\n",
    "- **Adam Parameters:**\n",
    "  - For optimizing `gamma` and `beta`.\n",
    "\n",
    "\n",
    "## Batch Normalization:\n",
    "- Normalizes the input of each mini-batch to have zero mean and unit variance.\n",
    "- Helps in stabilizing the learning process and allows for higher learning rates.\n",
    "- Learns `gamma` and `beta` to restore the representation power.\n",
    "\n",
    "### Forward Pass:\n",
    "**Explanation:**\n",
    "  - **Training Mode:**\n",
    "    - **Compute Batch Statistics:**\n",
    "      - Mean (`self.batch_mean`) and variance (`self.batch_var`) over the mini-batch.\n",
    "      \n",
    "    - **Normalize the Batch:**\n",
    "      - Subtract mean and divide by standard deviation (`self.std_inv`).\n",
    "      \n",
    "    - **Scale and Shift:**\n",
    "      - Apply learned `gamma` and `beta`.\n",
    "      \n",
    "    - **Update Running Estimates:**\n",
    "      - For use during inference.\n",
    "\n",
    "  - **Inference Mode:**\n",
    "    - Uses the running mean and variance to normalize data.\n",
    "\n",
    "**Theory:**\n",
    "  - **Normalization Formula:**\n",
    "    - \\( \\hat{X} = \\frac{X - \\mu_{\\text{batch}}}{\\sqrt{\\sigma_{\\text{batch}}^2 + \\epsilon}} \\)\n",
    "\n",
    "  - **Scale and Shift:**\n",
    "    - \\( Y = \\gamma \\hat{X} + \\beta \\)\n",
    "\n",
    "  - **Running Estimates:**\n",
    "    - \\( \\text{running\\_mean} = \\text{momentum} \\times \\text{running\\_mean} + (1 - \\text{momentum}) \\times \\mu_{\\text{batch}} \\)\n",
    "    - Similar for running variance.\n",
    "\n",
    "## backward Pass\n",
    "- **Explanation:**\n",
    "  - **Gradients w.r.t Parameters:**\n",
    "    - `self.dgamma`:\n",
    "      - Gradient of the loss with respect to `gamma`.\n",
    "    - `self.dbeta`:\n",
    "      - Gradient of the loss with respect to `beta`.\n",
    "\n",
    "  - **Gradient w.r.t Input (`dX`):**\n",
    "    - Computed using chain rule and intermediate variables.\n",
    "    - Ensures that gradients flow correctly through the normalization step.\n",
    "\n",
    "- **Theory:**\n",
    "  - **Backpropagation Through Batch Norm:**\n",
    "    - Involves computing derivatives through the normalization and scaling steps.\n",
    "    - Requires careful calculation to maintain numerical stability.\n",
    "\n",
    "\n",
    "## update_params\n",
    "- Similar to the Dense Layer, but updates `gamma` and `beta` parameters.\n",
    "- Uses the Adam optimization algorithm for parameter updates.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, input_dim, epsilon=1e-5, momentum=0.9):\n",
    "        self.gamma = np.ones((1, input_dim))\n",
    "        self.beta = np.zeros((1, input_dim))\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.running_mean = np.zeros((1, input_dim))\n",
    "        self.running_var = np.zeros((1, input_dim))\n",
    "        \n",
    "        # Gradients\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "        \n",
    "        # Cache for backpropagation\n",
    "        self.X_centered = None\n",
    "        self.std_inv = None\n",
    "        self.batch_mean = None\n",
    "        self.batch_var = None\n",
    "        \n",
    "        # Adam parameters\n",
    "        self.m_gamma = np.zeros_like(self.gamma)\n",
    "        self.v_gamma = np.zeros_like(self.gamma)\n",
    "        self.m_beta = np.zeros_like(self.beta)\n",
    "        self.v_beta = np.zeros_like(self.beta)\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            self.batch_mean = np.mean(X, axis=0, keepdims=True)\n",
    "            self.batch_var = np.var(X, axis=0, keepdims=True)\n",
    "            \n",
    "            self.X_centered = X - self.batch_mean\n",
    "            self.std_inv = 1.0 / np.sqrt(self.batch_var + self.epsilon)\n",
    "            \n",
    "            X_norm = self.X_centered * self.std_inv\n",
    "            out = self.gamma * X_norm + self.beta\n",
    "            \n",
    "            # Update running estimates\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.batch_var\n",
    "        else:\n",
    "            # Use running mean and var during inference\n",
    "            X_centered = X - self.running_mean\n",
    "            std_inv = 1.0 / np.sqrt(self.running_var + self.epsilon)\n",
    "            X_norm = X_centered * std_inv\n",
    "            out = self.gamma * X_norm + self.beta\n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        N, D = grad_output.shape\n",
    "        \n",
    "        # Step-wise computation for gradients\n",
    "        X_norm = self.X_centered * self.std_inv\n",
    "        dX_norm = grad_output * self.gamma\n",
    "        \n",
    "        dvar = np.sum(dX_norm * self.X_centered * -0.5 * self.std_inv ** 3, axis=0)\n",
    "        dmean = np.sum(dX_norm * -self.std_inv, axis=0) + dvar * np.mean(-2.0 * self.X_centered, axis=0)\n",
    "        \n",
    "        dX = (dX_norm * self.std_inv) + (dvar * 2.0 * self.X_centered / N) + (dmean / N)\n",
    "        self.dgamma = np.sum(grad_output * X_norm, axis=0, keepdims=True)\n",
    "        self.dbeta = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def update_params(self, learning_rate, beta1, beta2, epsilon, t):\n",
    "        # Update gamma and beta using Adam optimizer\n",
    "        self.m_gamma = beta1 * self.m_gamma + (1 - beta1) * self.dgamma\n",
    "        self.v_gamma = beta2 * self.v_gamma + (1 - beta2) * (self.dgamma ** 2)\n",
    "        m_gamma_hat = self.m_gamma / (1 - beta1 ** t)\n",
    "        v_gamma_hat = self.v_gamma / (1 - beta2 ** t)\n",
    "        self.gamma -= learning_rate * m_gamma_hat / (np.sqrt(v_gamma_hat) + epsilon)\n",
    "        \n",
    "        self.m_beta = beta1 * self.m_beta + (1 - beta1) * self.dbeta\n",
    "        self.v_beta = beta2 * self.v_beta + (1 - beta2) * (self.dbeta ** 2)\n",
    "        m_beta_hat = self.m_beta / (1 - beta1 ** t)\n",
    "        v_beta_hat = self.v_beta / (1 - beta2 ** t)\n",
    "        self.beta -= learning_rate * m_beta_hat / (np.sqrt(v_beta_hat) + epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ReLU Activation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Activation\n",
    "- **Forward Pass:**\n",
    "  - `np.maximum(0, X)`:\n",
    "    - Applies the ReLU function element-wise.\n",
    "    - Sets all negative inputs to zero.\n",
    "\n",
    "- **Backward Pass:**\n",
    "  - **Gradient w.r.t Input (`grad_input`):**\n",
    "    - For inputs where \\( X > 0 \\), gradient is unchanged.\n",
    "    - For inputs where \\( X \\leq 0 \\), gradient is zero.\n",
    "\n",
    "- **Theory:**\n",
    "  - **ReLU Function:**\n",
    "    - \\( \\text{ReLU}(x) = \\max(0, x) \\)\n",
    "\n",
    "  - **Derivative:**\n",
    "    - \\( \\frac{d}{dx} \\text{ReLU}(x) = \\begin{cases} 1, & x > 0 \\\\ 0, & x \\leq 0 \\end{cases} \\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_input = grad_output.copy()\n",
    "        grad_input[self.input <= 0] = 0\n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "- Regularization technique to prevent overfitting.\n",
    "- During training, randomly drops units along with their connections.\n",
    "- **Scaling**:\n",
    "- - By scaling the activations during training, there's no need to adjust them during inference.\n",
    "- **Parameters:**\n",
    "  - `dropout_rate`:\n",
    "    - Probability of dropping a neuron (setting its output to zero).\n",
    "\n",
    "- **Forward Pass:**\n",
    "  - **Training Mode:**\n",
    "    - **Creating the Mask:**\n",
    "      - Randomly sets neurons to zero with probability `dropout_rate`.\n",
    "      - **Scaling:** Divides by \\( (1 - \\text{dropout_rate}) \\) to keep the expected value of the activations the same.\n",
    "      \n",
    "    - **Applying the Mask:**\n",
    "      - Multiplies the input \\( X \\) by the mask.\n",
    "\n",
    "  - **Inference Mode:**\n",
    "    - No dropout is applied during testing.\n",
    "\n",
    "- **Backward Pass:**\n",
    "  - **Gradient w.r.t Input (`grad_output * self.mask`):**\n",
    "    - Propagates gradients only through the neurons that were not dropped during the forward pass.\n",
    "- **Scaling**\n",
    "  By scaling the activations during training, there's no need to adjust them during inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            self.mask = (np.random.rand(*X.shape) > self.dropout_rate) / (1.0 - self.dropout_rate)\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X  # During inference, no dropout applied\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * self.mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax and Cross-Entropy Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_loss(logits, labels):\n",
    "    # logits: output of the network before softmax, shape (N, C)\n",
    "    # labels: one-hot encoded true labels, shape (N, C)\n",
    "    # returns loss and gradient with respect to logits\n",
    "    \n",
    "    # Compute softmax probabilities\n",
    "    exps = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    softmax_probs = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute loss\n",
    "    N = logits.shape[0]\n",
    "    loss = -np.sum(labels * np.log(softmax_probs + 1e-15)) / N\n",
    "    \n",
    "    # Compute gradient\n",
    "    grad_logits = (softmax_probs - labels) / N\n",
    "    return loss, grad_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization:\n",
    "- self.layers:\n",
    "  - List of layers constituting the network.\n",
    "- self.t:\n",
    "  - Global timestep used for Adam optimizer.\n",
    "\n",
    "### Forward Pass:\n",
    "- Iterates through each layer, passing the output of one as the input to the next.\n",
    "- **Conditional Forward Pass:**: For layers that behave differently during training and inference (Dropout, BatchNormalization), passes the training flag.\n",
    "\n",
    "### Backward Pass:\n",
    "- Iterates through layers in reverse order.\n",
    "- Passes the gradient from one layer to the previous.\n",
    "\n",
    "### Parameter Update:\n",
    "- Updates parameters of each layer that has an `update_params` method.\n",
    "- Increments the global timestep `self.t` after each update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.t = 1  # Timestep for Adam optimizer\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, (Dropout, BatchNormalization)):\n",
    "                X = layer.forward(X, training=training)\n",
    "            else:\n",
    "                X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(grad_output)\n",
    "    \n",
    "    def update_params(self, learning_rate, beta1, beta2, epsilon):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'update_params'):\n",
    "                layer.update_params(learning_rate, beta1, beta2, epsilon, self.t)\n",
    "        self.t += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Structure:\n",
    "- Input Layer: Receives flattened 28x28 images `(input_dim = 784)`.\n",
    "  \n",
    "### First Hidden Layer:\n",
    "- Dense Layer: 512 units. \n",
    "- Batch Normalization.\n",
    "- ReLU Activation.\n",
    "- Dropout with 30% rate.\n",
    "\n",
    "### Second Hidden Layer:\n",
    "- Dense Layer: 256 units. \n",
    "- Batch Normalization.\n",
    "- ReLU Activation.\n",
    "- Dropout with 30% rate.\n",
    "\n",
    "### Third Hidden Layer:\n",
    "- Dense Layer: 128 units. \n",
    "- Batch Normalization.\n",
    "- ReLU Activation.\n",
    "- Dropout with 30% rate.\n",
    "\n",
    "### Output Layer\n",
    "- Dense Layer: 10 units (one for each class).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784  # 28x28 images flattened\n",
    "layer1_units = 512\n",
    "layer2_units = 256\n",
    "layer3_units = 128 \n",
    "output_units = 10  # Number of classes\n",
    "\n",
    "layers = [\n",
    "    DenseLayer(input_dim, layer1_units),\n",
    "    BatchNormalization(layer1_units),\n",
    "    ReLU(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    DenseLayer(layer1_units, layer2_units),\n",
    "    BatchNormalization(layer2_units),\n",
    "    ReLU(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    DenseLayer(layer2_units, layer3_units),\n",
    "    BatchNormalization(layer3_units),\n",
    "    ReLU(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    DenseLayer(layer3_units, output_units)\n",
    "]\n",
    "\n",
    "# Initialize the network\n",
    "network = NeuralNetwork(layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting Hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Learning Rate (`learning_rate`):**\n",
    "  - Step size for parameter updates.\n",
    "\n",
    "- **Adam Hyperparameters:**\n",
    "  - `beta1`: Decay rate for first moment estimates.\n",
    "  - `beta2`: Decay rate for second moment estimates.\n",
    "  - `epsilon`: Small constant to prevent division by zero.\n",
    "\n",
    "- **Training Configuration:**\n",
    "  - `num_epochs`: Number of times the entire training dataset is passed through the network.\n",
    "  - `batch_size`: Number of samples processed before updating the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8\n",
    "num_epochs = 30\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(network, X_train, y_train_one_hot, X_val, y_val_one_hot, num_epochs, batch_size):\n",
    "#     num_train_samples = X_train.shape[0]\n",
    "#     num_batches = int(np.ceil(num_train_samples / batch_size))\n",
    "    \n",
    "#     train_losses = []\n",
    "#     val_losses = []\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Shuffle the training data\n",
    "#         indices = np.arange(num_train_samples)\n",
    "#         np.random.shuffle(indices)\n",
    "#         X_train_shuffled = X_train[indices]\n",
    "#         y_train_shuffled = y_train_one_hot[indices]\n",
    "        \n",
    "#         epoch_loss = 0.0\n",
    "#         for i in range(num_batches):\n",
    "#             start = i * batch_size\n",
    "#             end = min(start + batch_size, num_train_samples)\n",
    "#             X_batch = X_train_shuffled[start:end]\n",
    "#             y_batch = y_train_shuffled[start:end]\n",
    "            \n",
    "#             # Forward pass\n",
    "#             logits = network.forward(X_batch, training=True)\n",
    "            \n",
    "#             # Compute loss and gradient\n",
    "#             loss, grad_logits = softmax_cross_entropy_loss(logits, y_batch)\n",
    "#             epoch_loss += loss\n",
    "            \n",
    "#             # Backward pass\n",
    "#             network.backward(grad_logits)\n",
    "            \n",
    "#             # Update parameters\n",
    "#             network.update_params(learning_rate, beta1, beta2, epsilon)\n",
    "        \n",
    "#         # Compute average training loss\n",
    "#         avg_train_loss = epoch_loss / num_batches\n",
    "#         train_losses.append(avg_train_loss)\n",
    "        \n",
    "#         # Validation\n",
    "#         logits_val = network.forward(X_val, training=False)\n",
    "#         val_loss, _ = softmax_cross_entropy_loss(logits_val, y_val_one_hot)\n",
    "#         val_losses.append(val_loss)\n",
    "        \n",
    "#         # Compute validation accuracy\n",
    "#         predictions = np.argmax(logits_val, axis=1)\n",
    "#         true_labels = np.argmax(y_val_one_hot, axis=1)\n",
    "#         val_accuracy = np.mean(predictions == true_labels)\n",
    "        \n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, \"\n",
    "#               f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "#     return train_losses, val_losses\n",
    "def train(network, X_train, y_train_one_hot, X_val, y_val_one_hot, num_epochs, batch_size):\n",
    "    num_train_samples = X_train.shape[0]\n",
    "    num_batches = int(np.ceil(num_train_samples / batch_size))\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    initial_learning_rate = 0.001\n",
    "    for epoch in range(num_epochs):\n",
    "        # Decay learning rate\n",
    "        learning_rate_epoch = initial_learning_rate * (0.95 ** epoch)\n",
    "\n",
    "        # Shuffle the training data\n",
    "        indices = np.arange(num_train_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_train_shuffled = X_train[indices]\n",
    "        y_train_shuffled = y_train_one_hot[indices]\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, num_train_samples)\n",
    "            X_batch = X_train_shuffled[start:end]\n",
    "            y_batch = y_train_shuffled[start:end]\n",
    "\n",
    "            # Forward pass\n",
    "            logits = network.forward(X_batch, training=True)\n",
    "\n",
    "            # Compute loss and gradient\n",
    "            loss, grad_logits = softmax_cross_entropy_loss(logits, y_batch)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            # Backward pass\n",
    "            network.backward(grad_logits)\n",
    "\n",
    "            # Update parameters\n",
    "            network.update_params(learning_rate_epoch, beta1, beta2, epsilon)\n",
    "\n",
    "        # Compute average training loss\n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        logits_val = network.forward(X_val, training=False)\n",
    "        val_loss, _ = softmax_cross_entropy_loss(logits_val, y_val_one_hot)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Compute validation accuracy\n",
    "        predictions = np.argmax(logits_val, axis=1)\n",
    "        true_labels = np.argmax(y_val_one_hot, axis=1)\n",
    "        val_accuracy = np.mean(predictions == true_labels)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    return train_losses, val_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Training Loss: 0.5819, Validation Loss: 0.3757, Validation Accuracy: 0.8560\n",
      "Epoch 2/30, Training Loss: 0.4274, Validation Loss: 0.3336, Validation Accuracy: 0.8733\n",
      "Epoch 3/30, Training Loss: 0.3872, Validation Loss: 0.3168, Validation Accuracy: 0.8823\n",
      "Epoch 4/30, Training Loss: 0.3620, Validation Loss: 0.3022, Validation Accuracy: 0.8863\n",
      "Epoch 5/30, Training Loss: 0.3413, Validation Loss: 0.3184, Validation Accuracy: 0.8785\n",
      "Epoch 6/30, Training Loss: 0.3252, Validation Loss: 0.2836, Validation Accuracy: 0.8923\n",
      "Epoch 7/30, Training Loss: 0.3093, Validation Loss: 0.3001, Validation Accuracy: 0.8855\n",
      "Epoch 8/30, Training Loss: 0.2986, Validation Loss: 0.2738, Validation Accuracy: 0.8963\n",
      "Epoch 9/30, Training Loss: 0.2845, Validation Loss: 0.2913, Validation Accuracy: 0.8925\n",
      "Epoch 10/30, Training Loss: 0.2720, Validation Loss: 0.2820, Validation Accuracy: 0.8968\n",
      "Epoch 11/30, Training Loss: 0.2640, Validation Loss: 0.2725, Validation Accuracy: 0.8968\n",
      "Epoch 12/30, Training Loss: 0.2524, Validation Loss: 0.2680, Validation Accuracy: 0.9018\n",
      "Epoch 13/30, Training Loss: 0.2460, Validation Loss: 0.2678, Validation Accuracy: 0.9032\n",
      "Epoch 14/30, Training Loss: 0.2391, Validation Loss: 0.2865, Validation Accuracy: 0.8970\n",
      "Epoch 15/30, Training Loss: 0.2288, Validation Loss: 0.2609, Validation Accuracy: 0.9037\n",
      "Epoch 16/30, Training Loss: 0.2218, Validation Loss: 0.2666, Validation Accuracy: 0.9038\n",
      "Epoch 17/30, Training Loss: 0.2146, Validation Loss: 0.2602, Validation Accuracy: 0.9062\n",
      "Epoch 18/30, Training Loss: 0.2075, Validation Loss: 0.2620, Validation Accuracy: 0.9075\n",
      "Epoch 19/30, Training Loss: 0.2021, Validation Loss: 0.2644, Validation Accuracy: 0.9130\n",
      "Epoch 20/30, Training Loss: 0.1949, Validation Loss: 0.2611, Validation Accuracy: 0.9043\n",
      "Epoch 21/30, Training Loss: 0.1938, Validation Loss: 0.2568, Validation Accuracy: 0.9090\n",
      "Epoch 22/30, Training Loss: 0.1845, Validation Loss: 0.2696, Validation Accuracy: 0.9050\n",
      "Epoch 23/30, Training Loss: 0.1766, Validation Loss: 0.2750, Validation Accuracy: 0.9067\n",
      "Epoch 24/30, Training Loss: 0.1744, Validation Loss: 0.2631, Validation Accuracy: 0.9075\n",
      "Epoch 25/30, Training Loss: 0.1684, Validation Loss: 0.2708, Validation Accuracy: 0.9087\n",
      "Epoch 26/30, Training Loss: 0.1659, Validation Loss: 0.2654, Validation Accuracy: 0.9062\n",
      "Epoch 27/30, Training Loss: 0.1606, Validation Loss: 0.2655, Validation Accuracy: 0.9087\n",
      "Epoch 28/30, Training Loss: 0.1577, Validation Loss: 0.2730, Validation Accuracy: 0.9083\n",
      "Epoch 29/30, Training Loss: 0.1516, Validation Loss: 0.2763, Validation Accuracy: 0.9085\n",
      "Epoch 30/30, Training Loss: 0.1511, Validation Loss: 0.2736, Validation Accuracy: 0.9063\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train(\n",
    "    network, X_train, y_train_one_hot, X_val, y_val_one_hot, num_epochs, batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2754, Test Accuracy: 0.9103\n"
     ]
    }
   ],
   "source": [
    "def evaluate(network, X_test, y_test_one_hot):\n",
    "    logits = network.forward(X_test, training=False)\n",
    "    test_loss, _ = softmax_cross_entropy_loss(logits, y_test_one_hot)\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    true_labels = np.argmax(y_test_one_hot, axis=1)\n",
    "    test_accuracy = np.mean(predictions == true_labels)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "test_loss, test_accuracy = evaluate(network, X_test, y_test_one_hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "with open('trained_model_4.pkl', 'wb') as f:\n",
    "    pickle.dump(network, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**loading the trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "with open('trained_model.pkl', 'rb') as f:\n",
    "    loaded_network = pickle.load(f)\n",
    "\n",
    "# Use the loaded model for prediction\n",
    "logits_test = loaded_network.forward(X_test, training=False)\n",
    "predictions_test = np.argmax(logits_test, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing Training Progress**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, num_epochs + 1)\n",
    "plt.plot(epochs, train_losses, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
