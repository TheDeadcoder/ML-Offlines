{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n",
      "Number of test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "train_data_path = 'data/train.csv'\n",
    "test_data_path = 'data/test.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "print(\"Number of training examples:\", train_data.shape[0])\n",
    "print(\"Number of test examples:\", test_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**# Separate features and labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop('label', axis=1).values\n",
    "y_train = train_data['label'].values\n",
    "\n",
    "X_test = test_data.drop('label', axis=1).values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize the pixel values (scale to [0, 1])**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert labels to one-hot encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "num_classes = 10\n",
    "y_train_one_hot = one_hot_encode(y_train, num_classes)\n",
    "y_test_one_hot = one_hot_encode(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the training data into training and validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train_one_hot, y_val_one_hot = train_test_split(\n",
    "    X_train, y_train_one_hot, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Neural Network Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dense Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "\n",
    "### Initialization:\n",
    "- **Weights (`self.w`):**\n",
    "  - Initialized with a scaled random normal distribution.\n",
    "  - `np.sqrt(2.0 / input_dim)`: He initialization, suitable for ReLU activations.\n",
    "\n",
    "- **Biases (`self.b`):**\n",
    "  - Initialized to zeros.\n",
    "\n",
    "- **Gradients (`self.dw`, `self.db`):**\n",
    "  - Placeholders for gradients computed during backpropagation.\n",
    "\n",
    "### Adam Parameters:\n",
    "- **First Moment Vectors (`self.m_w`, `self.m_b`):**\n",
    "  - Moving averages of the gradients.\n",
    "\n",
    "- **Second Moment Vectors (`self.v_w`, `self.v_b`):**\n",
    "  - Moving averages of the squared gradients.\n",
    "\n",
    "### Cache (`self.input`):\n",
    "- Stores the input to the layer for use during backpropagation.\n",
    "\n",
    "## Theory:\n",
    "\n",
    "### Dense (Fully Connected) Layer:\n",
    "- Each neuron receives input from all neurons in the previous layer.\n",
    "- Computes `output = X * W + b`.\n",
    "\n",
    "### He Initialization:\n",
    "- Addresses the problem of vanishing/exploding gradients.\n",
    "- Suitable for layers followed by ReLU activation.\n",
    "\n",
    "\n",
    "### **Forward Pass Explanation:**\n",
    "  - Computes the linear transformation.\n",
    "  - Stores the input \\( X \\) for use in backpropagation.\n",
    "\n",
    "**Mathematical Operation:**\n",
    "  -  `output = X * W + b`.\n",
    "\n",
    "### **Backward Pass Explanation**:\n",
    "\n",
    "- **Gradients w.r.t Weights (`self.dw`):**\n",
    "  - Computed as the dot product of the transposed input and `grad_output`.\n",
    "\n",
    "- **Gradients w.r.t Biases (`self.db`):**\n",
    "  - Sum of `grad_output` along the batch dimension.\n",
    "\n",
    "- **Gradient w.r.t Input (`grad_input`):**\n",
    "  - Propagated backward to previous layers.\n",
    "  - Computed as the dot product of `grad_output` and transposed weights.\n",
    "\n",
    "## Theory:\n",
    "- Backpropagation involves computing the gradients of the loss with respect to each parameter.\n",
    "- Uses the chain rule from calculus.\n",
    "\n",
    "**Explanation for updating parameters:**\n",
    "  - **Computing Moving Averages:**\n",
    "    - `self.m_w`, `self.m_b`: Exponential moving averages of gradients (first moment).\n",
    "    - `self.v_w`, `self.v_b`: Exponential moving averages of squared gradients (second moment).\n",
    "  \n",
    "  - **Bias Correction:**\n",
    "    - Adjusts the moments to account for their initialization at zero.\n",
    "    - `m_w_hat`, `v_w_hat`: Corrected moments.\n",
    "\n",
    "  - **Parameter Updates:**\n",
    "    - Updates weights and biases using the Adam update rule.\n",
    "\n",
    "# Parameter Update Theory:\n",
    "\n",
    "- **Adam Optimizer:**\n",
    "  - Combines the advantages of both AdaGrad and RMSProp.\n",
    "\n",
    "  - **First Moment Estimate (Mean):**\n",
    "    - \\( m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\)\n",
    "\n",
    "  - **Second Moment Estimate (Variance):**\n",
    "    - \\( v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\)\n",
    "\n",
    "  - **Bias Correction:**\n",
    "    - Adjusts for the initial bias towards zero moments.\n",
    "\n",
    "  - **Parameter Update:**\n",
    "    - \\( \\theta_t = \\theta_{t-1} - \\alpha \\frac{m_t}{\\sqrt{v_t} + \\epsilon} \\)\n",
    "\n",
    "- **Parameters:**\n",
    "  - `beta1` and `beta2`:\n",
    "    - Hyperparameters controlling the decay rates of the moving averages.\n",
    "    - Commonly set to `beta1=0.9` and `beta2=0.999`.\n",
    "\n",
    "  - `epsilon`:\n",
    "    - Small constant to prevent division by zero.\n",
    "\n",
    "  - `t`:\n",
    "    - Timestep, incremented after each parameter update.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialize weights and biases\n",
    "        self.W = np.random.randn(input_dim, output_dim) * np.sqrt(2.0 / input_dim)\n",
    "        self.b = np.zeros((1, output_dim))\n",
    "        \n",
    "        # Gradients\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "        # Adam parameters\n",
    "        self.m_W = np.zeros_like(self.W)\n",
    "        self.v_W = np.zeros_like(self.W)\n",
    "        self.m_b = np.zeros_like(self.b)\n",
    "        self.v_b = np.zeros_like(self.b)\n",
    "        \n",
    "        # Cache for backpropagation\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        output = np.dot(X, self.W) + self.b\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # Compute gradients\n",
    "        self.dW = np.dot(self.input.T, grad_output)\n",
    "        self.db = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        grad_input = np.dot(grad_output, self.W.T)\n",
    "        return grad_input\n",
    "    \n",
    "    def update_params(self, learning_rate, beta1, beta2, epsilon, t):\n",
    "        # Update weights and biases using Adam optimizer\n",
    "        self.m_W = beta1 * self.m_W + (1 - beta1) * self.dW\n",
    "        self.v_W = beta2 * self.v_W + (1 - beta2) * (self.dW ** 2)\n",
    "        m_W_hat = self.m_W / (1 - beta1 ** t)\n",
    "        v_W_hat = self.v_W / (1 - beta2 ** t)\n",
    "        self.W -= learning_rate * m_W_hat / (np.sqrt(v_W_hat) + epsilon)\n",
    "        \n",
    "        self.m_b = beta1 * self.m_b + (1 - beta1) * self.db\n",
    "        self.v_b = beta2 * self.v_b + (1 - beta2) * (self.db ** 2)\n",
    "        m_b_hat = self.m_b / (1 - beta1 ** t)\n",
    "        v_b_hat = self.v_b / (1 - beta2 ** t)\n",
    "        self.b -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Scale Parameter (`self.gamma`):**\n",
    "  - Learns how much to scale the normalized output.\n",
    "\n",
    "- **Shift Parameter (`self.beta`):**\n",
    "  - Learns how much to shift the normalized output.\n",
    "\n",
    "- **`epsilon`:**\n",
    "  - Small constant to prevent division by zero.\n",
    "\n",
    "- **`momentum`:**\n",
    "  - Controls the updating of running estimates.\n",
    "\n",
    "- **Running Estimates:**\n",
    "  - `self.running_mean` and `self.running_var`:\n",
    "    - Used during inference to normalize data with global statistics.\n",
    "\n",
    "- **Gradients and Caches:**\n",
    "  - **Gradients (`self.dgamma`, `self.dbeta`):**\n",
    "    - For updating `gamma` and `beta`.\n",
    "  \n",
    "  - **Caches:**\n",
    "    - Stores intermediate variables needed for backpropagation.\n",
    "\n",
    "- **Adam Parameters:**\n",
    "  - For optimizing `gamma` and `beta`.\n",
    "\n",
    "\n",
    "## Batch Normalization:\n",
    "- Normalizes the input of each mini-batch to have zero mean and unit variance.\n",
    "- Helps in stabilizing the learning process and allows for higher learning rates.\n",
    "- Learns `gamma` and `beta` to restore the representation power.\n",
    "\n",
    "### Forward Pass:\n",
    "**Explanation:**\n",
    "  - **Training Mode:**\n",
    "    - **Compute Batch Statistics:**\n",
    "      - Mean (`self.batch_mean`) and variance (`self.batch_var`) over the mini-batch.\n",
    "      \n",
    "    - **Normalize the Batch:**\n",
    "      - Subtract mean and divide by standard deviation (`self.std_inv`).\n",
    "      \n",
    "    - **Scale and Shift:**\n",
    "      - Apply learned `gamma` and `beta`.\n",
    "      \n",
    "    - **Update Running Estimates:**\n",
    "      - For use during inference.\n",
    "\n",
    "  - **Inference Mode:**\n",
    "    - Uses the running mean and variance to normalize data.\n",
    "\n",
    "**Theory:**\n",
    "  - **Normalization Formula:**\n",
    "    - \\( \\hat{X} = \\frac{X - \\mu_{\\text{batch}}}{\\sqrt{\\sigma_{\\text{batch}}^2 + \\epsilon}} \\)\n",
    "\n",
    "  - **Scale and Shift:**\n",
    "    - \\( Y = \\gamma \\hat{X} + \\beta \\)\n",
    "\n",
    "  - **Running Estimates:**\n",
    "    - \\( \\text{running\\_mean} = \\text{momentum} \\times \\text{running\\_mean} + (1 - \\text{momentum}) \\times \\mu_{\\text{batch}} \\)\n",
    "    - Similar for running variance.\n",
    "\n",
    "## backward Pass\n",
    "- **Explanation:**\n",
    "  - **Gradients w.r.t Parameters:**\n",
    "    - `self.dgamma`:\n",
    "      - Gradient of the loss with respect to `gamma`.\n",
    "    - `self.dbeta`:\n",
    "      - Gradient of the loss with respect to `beta`.\n",
    "\n",
    "  - **Gradient w.r.t Input (`dX`):**\n",
    "    - Computed using chain rule and intermediate variables.\n",
    "    - Ensures that gradients flow correctly through the normalization step.\n",
    "\n",
    "- **Theory:**\n",
    "  - **Backpropagation Through Batch Norm:**\n",
    "    - Involves computing derivatives through the normalization and scaling steps.\n",
    "    - Requires careful calculation to maintain numerical stability.\n",
    "\n",
    "\n",
    "## update_params\n",
    "- Similar to the Dense Layer, but updates `gamma` and `beta` parameters.\n",
    "- Uses the Adam optimization algorithm for parameter updates.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, input_dim, epsilon=1e-5, momentum=0.9):\n",
    "        self.gamma = np.ones((1, input_dim))\n",
    "        self.beta = np.zeros((1, input_dim))\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.running_mean = np.zeros((1, input_dim))\n",
    "        self.running_var = np.zeros((1, input_dim))\n",
    "        \n",
    "        # Gradients\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "        \n",
    "        # Cache for backpropagation\n",
    "        self.X_centered = None\n",
    "        self.std_inv = None\n",
    "        self.batch_mean = None\n",
    "        self.batch_var = None\n",
    "        \n",
    "        # Adam parameters\n",
    "        self.m_gamma = np.zeros_like(self.gamma)\n",
    "        self.v_gamma = np.zeros_like(self.gamma)\n",
    "        self.m_beta = np.zeros_like(self.beta)\n",
    "        self.v_beta = np.zeros_like(self.beta)\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            self.batch_mean = np.mean(X, axis=0, keepdims=True)\n",
    "            self.batch_var = np.var(X, axis=0, keepdims=True)\n",
    "            \n",
    "            self.X_centered = X - self.batch_mean\n",
    "            self.std_inv = 1.0 / np.sqrt(self.batch_var + self.epsilon)\n",
    "            \n",
    "            X_norm = self.X_centered * self.std_inv\n",
    "            out = self.gamma * X_norm + self.beta\n",
    "            \n",
    "            # Update running estimates\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.batch_var\n",
    "        else:\n",
    "            # Use running mean and var during inference\n",
    "            X_centered = X - self.running_mean\n",
    "            std_inv = 1.0 / np.sqrt(self.running_var + self.epsilon)\n",
    "            X_norm = X_centered * std_inv\n",
    "            out = self.gamma * X_norm + self.beta\n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        N, D = grad_output.shape\n",
    "        \n",
    "        # Step-wise computation for gradients\n",
    "        X_norm = self.X_centered * self.std_inv\n",
    "        dX_norm = grad_output * self.gamma\n",
    "        \n",
    "        dvar = np.sum(dX_norm * self.X_centered * -0.5 * self.std_inv ** 3, axis=0)\n",
    "        dmean = np.sum(dX_norm * -self.std_inv, axis=0) + dvar * np.mean(-2.0 * self.X_centered, axis=0)\n",
    "        \n",
    "        dX = (dX_norm * self.std_inv) + (dvar * 2.0 * self.X_centered / N) + (dmean / N)\n",
    "        self.dgamma = np.sum(grad_output * X_norm, axis=0, keepdims=True)\n",
    "        self.dbeta = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def update_params(self, learning_rate, beta1, beta2, epsilon, t):\n",
    "        # Update gamma and beta using Adam optimizer\n",
    "        self.m_gamma = beta1 * self.m_gamma + (1 - beta1) * self.dgamma\n",
    "        self.v_gamma = beta2 * self.v_gamma + (1 - beta2) * (self.dgamma ** 2)\n",
    "        m_gamma_hat = self.m_gamma / (1 - beta1 ** t)\n",
    "        v_gamma_hat = self.v_gamma / (1 - beta2 ** t)\n",
    "        self.gamma -= learning_rate * m_gamma_hat / (np.sqrt(v_gamma_hat) + epsilon)\n",
    "        \n",
    "        self.m_beta = beta1 * self.m_beta + (1 - beta1) * self.dbeta\n",
    "        self.v_beta = beta2 * self.v_beta + (1 - beta2) * (self.dbeta ** 2)\n",
    "        m_beta_hat = self.m_beta / (1 - beta1 ** t)\n",
    "        v_beta_hat = self.v_beta / (1 - beta2 ** t)\n",
    "        self.beta -= learning_rate * m_beta_hat / (np.sqrt(v_beta_hat) + epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ReLU Activation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Activation\n",
    "- **Forward Pass:**\n",
    "  - `np.maximum(0, X)`:\n",
    "    - Applies the ReLU function element-wise.\n",
    "    - Sets all negative inputs to zero.\n",
    "\n",
    "- **Backward Pass:**\n",
    "  - **Gradient w.r.t Input (`grad_input`):**\n",
    "    - For inputs where \\( X > 0 \\), gradient is unchanged.\n",
    "    - For inputs where \\( X \\leq 0 \\), gradient is zero.\n",
    "\n",
    "- **Theory:**\n",
    "  - **ReLU Function:**\n",
    "    - \\( \\text{ReLU}(x) = \\max(0, x) \\)\n",
    "\n",
    "  - **Derivative:**\n",
    "    - \\( \\frac{d}{dx} \\text{ReLU}(x) = \\begin{cases} 1, & x > 0 \\\\ 0, & x \\leq 0 \\end{cases} \\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_input = grad_output.copy()\n",
    "        grad_input[self.input <= 0] = 0\n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "- Regularization technique to prevent overfitting.\n",
    "- During training, randomly drops units along with their connections.\n",
    "- **Scaling**:\n",
    "- - By scaling the activations during training, there's no need to adjust them during inference.\n",
    "- **Parameters:**\n",
    "  - `dropout_rate`:\n",
    "    - Probability of dropping a neuron (setting its output to zero).\n",
    "\n",
    "- **Forward Pass:**\n",
    "  - **Training Mode:**\n",
    "    - **Creating the Mask:**\n",
    "      - Randomly sets neurons to zero with probability `dropout_rate`.\n",
    "      - **Scaling:** Divides by \\( (1 - \\text{dropout_rate}) \\) to keep the expected value of the activations the same.\n",
    "      \n",
    "    - **Applying the Mask:**\n",
    "      - Multiplies the input \\( X \\) by the mask.\n",
    "\n",
    "  - **Inference Mode:**\n",
    "    - No dropout is applied during testing.\n",
    "\n",
    "- **Backward Pass:**\n",
    "  - **Gradient w.r.t Input (`grad_output * self.mask`):**\n",
    "    - Propagates gradients only through the neurons that were not dropped during the forward pass.\n",
    "- **Scaling**\n",
    "  By scaling the activations during training, there's no need to adjust them during inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            self.mask = (np.random.rand(*X.shape) > self.dropout_rate) / (1.0 - self.dropout_rate)\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X  # During inference, no dropout applied\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * self.mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax and Cross-Entropy Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_loss(logits, labels):\n",
    "    # logits: output of the network before softmax, shape (N, C)\n",
    "    # labels: one-hot encoded true labels, shape (N, C)\n",
    "    # returns loss and gradient with respect to logits\n",
    "    \n",
    "    # Compute softmax probabilities\n",
    "    exps = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    softmax_probs = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute loss\n",
    "    N = logits.shape[0]\n",
    "    loss = -np.sum(labels * np.log(softmax_probs + 1e-15)) / N\n",
    "    \n",
    "    # Compute gradient\n",
    "    grad_logits = (softmax_probs - labels) / N\n",
    "    return loss, grad_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization:\n",
    "- self.layers:\n",
    "  - List of layers constituting the network.\n",
    "- self.t:\n",
    "  - Global timestep used for Adam optimizer.\n",
    "\n",
    "### Forward Pass:\n",
    "- Iterates through each layer, passing the output of one as the input to the next.\n",
    "- **Conditional Forward Pass:**: For layers that behave differently during training and inference (Dropout, BatchNormalization), passes the training flag.\n",
    "\n",
    "### Backward Pass:\n",
    "- Iterates through layers in reverse order.\n",
    "- Passes the gradient from one layer to the previous.\n",
    "\n",
    "### Parameter Update:\n",
    "- Updates parameters of each layer that has an `update_params` method.\n",
    "- Increments the global timestep `self.t` after each update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.t = 1  # Timestep for Adam optimizer\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, (Dropout, BatchNormalization)):\n",
    "                X = layer.forward(X, training=training)\n",
    "            else:\n",
    "                X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(grad_output)\n",
    "    \n",
    "    def update_params(self, learning_rate, beta1, beta2, epsilon):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'update_params'):\n",
    "                layer.update_params(learning_rate, beta1, beta2, epsilon, self.t)\n",
    "        self.t += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Structure:\n",
    "- Input Layer: Receives flattened 28x28 images `(input_dim = 784)`.\n",
    "  \n",
    "### First Hidden Layer:\n",
    "- Dense Layer: 512 units. \n",
    "- Batch Normalization.\n",
    "- ReLU Activation.\n",
    "- Dropout with 30% rate.\n",
    "\n",
    "### Second Hidden Layer:\n",
    "- Dense Layer: 256 units. \n",
    "- Batch Normalization.\n",
    "- ReLU Activation.\n",
    "- Dropout with 30% rate.\n",
    "\n",
    "### Third Hidden Layer:\n",
    "- Dense Layer: 128 units. \n",
    "- Batch Normalization.\n",
    "- ReLU Activation.\n",
    "- Dropout with 30% rate.\n",
    "\n",
    "### Output Layer\n",
    "- Dense Layer: 10 units (one for each class).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = {\n",
    "    'Arch1': {\n",
    "        'layers': [\n",
    "            {'units': 512},\n",
    "            {'units': 256},\n",
    "            {'units': 128},\n",
    "        ],\n",
    "    },\n",
    "    'Arch2': {\n",
    "        'layers': [\n",
    "            {'units': 1024},\n",
    "            {'units': 512},\n",
    "            {'units': 256},\n",
    "            {'units': 128},\n",
    "        ],\n",
    "    },\n",
    "    'Arch3': {\n",
    "        'layers': [\n",
    "            {'units': 256},\n",
    "            {'units': 128},\n",
    "        ],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting Hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Learning Rate (`learning_rate`):**\n",
    "  - Step size for parameter updates.\n",
    "\n",
    "- **Adam Hyperparameters:**\n",
    "  - `beta1`: Decay rate for first moment estimates.\n",
    "  - `beta2`: Decay rate for second moment estimates.\n",
    "  - `epsilon`: Small constant to prevent division by zero.\n",
    "\n",
    "- **Training Configuration:**\n",
    "  - `num_epochs`: Number of times the entire training dataset is passed through the network.\n",
    "  - `batch_size`: Number of samples processed before updating the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8\n",
    "num_epochs = 25  \n",
    "batch_size = 64\n",
    "dropout_rate = 0.3 \n",
    "learning_rates = [0.005, 0.001, 0.0009, 0.0006]\n",
    "\n",
    "best_test_accuracy = 0.0\n",
    "best_model = None\n",
    "best_model_info = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, X_train, y_train_one_hot, X_val, y_val_one_hot, num_epochs, batch_size, learning_rate):\n",
    "    num_train_samples = X_train.shape[0]\n",
    "    num_batches = int(np.ceil(num_train_samples / batch_size))\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    train_f1s = []\n",
    "    val_f1s = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the training data\n",
    "        indices = np.arange(num_train_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_train_shuffled = X_train[indices]\n",
    "        y_train_shuffled = y_train_one_hot[indices]\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        all_predictions = []\n",
    "        all_true_labels = []\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, num_train_samples)\n",
    "            X_batch = X_train_shuffled[start:end]\n",
    "            y_batch = y_train_shuffled[start:end]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = network.forward(X_batch, training=True)\n",
    "            \n",
    "            # Compute loss and gradient\n",
    "            loss, grad_logits = softmax_cross_entropy_loss(logits, y_batch)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            # Backward pass\n",
    "            network.backward(grad_logits)\n",
    "            \n",
    "            # Update parameters\n",
    "            network.update_params(learning_rate, beta1, beta2, epsilon)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            predictions = np.argmax(logits, axis=1)\n",
    "            true_labels = np.argmax(y_batch, axis=1)\n",
    "            correct_predictions += np.sum(predictions == true_labels)\n",
    "            total_samples += predictions.shape[0]\n",
    "            \n",
    "            all_predictions.extend(predictions)\n",
    "            all_true_labels.extend(true_labels)\n",
    "        \n",
    "        # Compute average training loss and accuracy\n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "        train_accuracy = correct_predictions / total_samples\n",
    "        train_f1 = f1_score(all_true_labels, all_predictions, average='macro')\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        train_f1s.append(train_f1)\n",
    "        \n",
    "        # Validation\n",
    "        logits_val = network.forward(X_val, training=False)\n",
    "        val_loss, _ = softmax_cross_entropy_loss(logits_val, y_val_one_hot)\n",
    "        \n",
    "        predictions_val = np.argmax(logits_val, axis=1)\n",
    "        true_labels_val = np.argmax(y_val_one_hot, axis=1)\n",
    "        val_accuracy = np.mean(predictions_val == true_labels_val)\n",
    "        val_f1 = f1_score(true_labels_val, predictions_val, average='macro')\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_f1s.append(val_f1)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Training Accuracy: {train_accuracy:.4f}, Training F1: {train_f1:.4f}, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, \"\n",
    "              f\"Validation F1: {val_f1:.4f}\")\n",
    "        \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies, train_f1s, val_f1s\n",
    "\n",
    "def plot_metrics(arch_name, learning_rate, train_losses, val_losses,\n",
    "                 train_accuracies, val_accuracies, train_f1s, val_f1s):\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "    plt.title(f'Loss vs. Epochs ({arch_name}, Learning Rate: {learning_rate})')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'loss_{arch_name}_lr_{learning_rate}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, 'r-', label='Validation Accuracy')\n",
    "    plt.title(f'Accuracy vs. Epochs ({arch_name}, Learning Rate: {learning_rate})')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'accuracy_{arch_name}_lr_{learning_rate}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot F1 Score\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_f1s, 'b-', label='Training F1 Score')\n",
    "    plt.plot(epochs, val_f1s, 'r-', label='Validation F1 Score')\n",
    "    plt.title(f'F1 Score vs. Epochs ({arch_name}, Learning Rate: {learning_rate})')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'f1_{arch_name}_lr_{learning_rate}.png')\n",
    "    plt.close()\n",
    "\n",
    "# def generate_confusion_matrix(network, X_test, y_test):\n",
    "#     logits = network.forward(X_test, training=False)\n",
    "#     predictions = np.argmax(logits, axis=1)\n",
    "    \n",
    "#     cm = confusion_matrix(y_test, predictions)\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "#                 xticklabels=range(10), yticklabels=range(10))\n",
    "#     plt.title('Confusion Matrix')\n",
    "#     plt.xlabel('Predicted Label')\n",
    "#     plt.ylabel('True Label')\n",
    "#     plt.savefig('confusion_matrix.png')\n",
    "#     plt.close()\n",
    "\n",
    "def generate_confusion_matrix(network, X_test, y_test, arch_name, learning_rate):\n",
    "    logits = network.forward(X_test, training=False)\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=range(10), yticklabels=range(10))\n",
    "    plt.title(f'Confusion Matrix - Architecture: {arch_name}, Learning Rate: {learning_rate}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    \n",
    "    # Save the confusion matrix with architecture and learning rate in the filename\n",
    "    filename = f'confusion_matrix_{arch_name}_lr_{learning_rate}.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(network, X_test, y_test_one_hot):\n",
    "    logits = network.forward(X_test, training=False)\n",
    "    test_loss, _ = softmax_cross_entropy_loss(logits, y_test_one_hot)\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    true_labels = np.argmax(y_test_one_hot, axis=1)\n",
    "    test_accuracy = np.mean(predictions == true_labels)\n",
    "    test_f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")   \n",
    "    return test_loss, test_accuracy, test_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models for architecture: Arch1\n",
      "  Training with learning rate: 0.005\n",
      "Epoch 1/25, Training Loss: 0.5565, Training Accuracy: 0.8008, Training F1: 0.7989, Validation Loss: 0.3970, Validation Accuracy: 0.8510, Validation F1: 0.8485\n",
      "Epoch 2/25, Training Loss: 0.4408, Training Accuracy: 0.8421, Training F1: 0.8410, Validation Loss: 0.3622, Validation Accuracy: 0.8670, Validation F1: 0.8634\n",
      "Epoch 3/25, Training Loss: 0.3984, Training Accuracy: 0.8577, Training F1: 0.8568, Validation Loss: 0.3399, Validation Accuracy: 0.8723, Validation F1: 0.8719\n",
      "Epoch 4/25, Training Loss: 0.3752, Training Accuracy: 0.8645, Training F1: 0.8635, Validation Loss: 0.3305, Validation Accuracy: 0.8748, Validation F1: 0.8781\n",
      "Epoch 5/25, Training Loss: 0.3566, Training Accuracy: 0.8719, Training F1: 0.8710, Validation Loss: 0.2986, Validation Accuracy: 0.8847, Validation F1: 0.8854\n",
      "Epoch 6/25, Training Loss: 0.3422, Training Accuracy: 0.8755, Training F1: 0.8747, Validation Loss: 0.3164, Validation Accuracy: 0.8837, Validation F1: 0.8833\n",
      "Epoch 7/25, Training Loss: 0.3307, Training Accuracy: 0.8799, Training F1: 0.8791, Validation Loss: 0.2940, Validation Accuracy: 0.8908, Validation F1: 0.8927\n",
      "Epoch 8/25, Training Loss: 0.3188, Training Accuracy: 0.8835, Training F1: 0.8828, Validation Loss: 0.2949, Validation Accuracy: 0.8930, Validation F1: 0.8940\n",
      "Epoch 9/25, Training Loss: 0.3073, Training Accuracy: 0.8885, Training F1: 0.8877, Validation Loss: 0.2954, Validation Accuracy: 0.8920, Validation F1: 0.8927\n",
      "Epoch 10/25, Training Loss: 0.2962, Training Accuracy: 0.8927, Training F1: 0.8921, Validation Loss: 0.2847, Validation Accuracy: 0.8988, Validation F1: 0.9004\n",
      "Epoch 11/25, Training Loss: 0.2864, Training Accuracy: 0.8937, Training F1: 0.8930, Validation Loss: 0.2833, Validation Accuracy: 0.8947, Validation F1: 0.8938\n",
      "Epoch 12/25, Training Loss: 0.2802, Training Accuracy: 0.8988, Training F1: 0.8982, Validation Loss: 0.2783, Validation Accuracy: 0.9005, Validation F1: 0.9012\n",
      "Epoch 13/25, Training Loss: 0.2727, Training Accuracy: 0.9000, Training F1: 0.8994, Validation Loss: 0.2892, Validation Accuracy: 0.8935, Validation F1: 0.8943\n",
      "Epoch 14/25, Training Loss: 0.2658, Training Accuracy: 0.9033, Training F1: 0.9027, Validation Loss: 0.2753, Validation Accuracy: 0.8995, Validation F1: 0.9008\n",
      "Epoch 15/25, Training Loss: 0.2612, Training Accuracy: 0.9053, Training F1: 0.9048, Validation Loss: 0.2765, Validation Accuracy: 0.8992, Validation F1: 0.8992\n",
      "Epoch 16/25, Training Loss: 0.2505, Training Accuracy: 0.9081, Training F1: 0.9076, Validation Loss: 0.2798, Validation Accuracy: 0.8948, Validation F1: 0.8961\n",
      "Epoch 17/25, Training Loss: 0.2453, Training Accuracy: 0.9086, Training F1: 0.9082, Validation Loss: 0.2920, Validation Accuracy: 0.8973, Validation F1: 0.8964\n",
      "Epoch 18/25, Training Loss: 0.2429, Training Accuracy: 0.9122, Training F1: 0.9117, Validation Loss: 0.2780, Validation Accuracy: 0.8987, Validation F1: 0.8987\n",
      "Epoch 19/25, Training Loss: 0.2349, Training Accuracy: 0.9134, Training F1: 0.9129, Validation Loss: 0.2803, Validation Accuracy: 0.8977, Validation F1: 0.8994\n",
      "Epoch 20/25, Training Loss: 0.2298, Training Accuracy: 0.9147, Training F1: 0.9143, Validation Loss: 0.2785, Validation Accuracy: 0.9010, Validation F1: 0.9022\n",
      "Epoch 21/25, Training Loss: 0.2255, Training Accuracy: 0.9172, Training F1: 0.9167, Validation Loss: 0.2909, Validation Accuracy: 0.8958, Validation F1: 0.8975\n",
      "Epoch 22/25, Training Loss: 0.2199, Training Accuracy: 0.9189, Training F1: 0.9184, Validation Loss: 0.2741, Validation Accuracy: 0.9007, Validation F1: 0.9013\n",
      "Epoch 23/25, Training Loss: 0.2169, Training Accuracy: 0.9202, Training F1: 0.9198, Validation Loss: 0.2797, Validation Accuracy: 0.9012, Validation F1: 0.9035\n",
      "Epoch 24/25, Training Loss: 0.2096, Training Accuracy: 0.9215, Training F1: 0.9211, Validation Loss: 0.2784, Validation Accuracy: 0.8980, Validation F1: 0.9000\n",
      "Epoch 25/25, Training Loss: 0.2087, Training Accuracy: 0.9226, Training F1: 0.9223, Validation Loss: 0.2789, Validation Accuracy: 0.9052, Validation F1: 0.9066\n",
      "Test Loss: 0.2767, Test Accuracy: 0.9042, Test F1 Score: 0.9042\n",
      "  Training with learning rate: 0.001\n",
      "Epoch 1/25, Training Loss: 0.5835, Training Accuracy: 0.7938, Training F1: 0.7922, Validation Loss: 0.3601, Validation Accuracy: 0.8662, Validation F1: 0.8662\n",
      "Epoch 2/25, Training Loss: 0.4285, Training Accuracy: 0.8452, Training F1: 0.8441, Validation Loss: 0.3354, Validation Accuracy: 0.8770, Validation F1: 0.8790\n",
      "Epoch 3/25, Training Loss: 0.3897, Training Accuracy: 0.8584, Training F1: 0.8575, Validation Loss: 0.3258, Validation Accuracy: 0.8767, Validation F1: 0.8777\n",
      "Epoch 4/25, Training Loss: 0.3674, Training Accuracy: 0.8662, Training F1: 0.8654, Validation Loss: 0.3411, Validation Accuracy: 0.8653, Validation F1: 0.8661\n",
      "Epoch 5/25, Training Loss: 0.3491, Training Accuracy: 0.8741, Training F1: 0.8732, Validation Loss: 0.2878, Validation Accuracy: 0.8903, Validation F1: 0.8918\n",
      "Epoch 6/25, Training Loss: 0.3320, Training Accuracy: 0.8789, Training F1: 0.8782, Validation Loss: 0.3150, Validation Accuracy: 0.8788, Validation F1: 0.8786\n",
      "Epoch 7/25, Training Loss: 0.3171, Training Accuracy: 0.8846, Training F1: 0.8839, Validation Loss: 0.2859, Validation Accuracy: 0.8888, Validation F1: 0.8907\n",
      "Epoch 8/25, Training Loss: 0.3056, Training Accuracy: 0.8880, Training F1: 0.8874, Validation Loss: 0.2767, Validation Accuracy: 0.8982, Validation F1: 0.8993\n",
      "Epoch 9/25, Training Loss: 0.2949, Training Accuracy: 0.8919, Training F1: 0.8912, Validation Loss: 0.2752, Validation Accuracy: 0.8978, Validation F1: 0.8989\n",
      "Epoch 10/25, Training Loss: 0.2867, Training Accuracy: 0.8939, Training F1: 0.8933, Validation Loss: 0.2913, Validation Accuracy: 0.8903, Validation F1: 0.8901\n",
      "Epoch 11/25, Training Loss: 0.2814, Training Accuracy: 0.8959, Training F1: 0.8953, Validation Loss: 0.2784, Validation Accuracy: 0.8930, Validation F1: 0.8945\n",
      "Epoch 12/25, Training Loss: 0.2700, Training Accuracy: 0.8994, Training F1: 0.8988, Validation Loss: 0.2849, Validation Accuracy: 0.8943, Validation F1: 0.8964\n",
      "Epoch 13/25, Training Loss: 0.2615, Training Accuracy: 0.9038, Training F1: 0.9033, Validation Loss: 0.2723, Validation Accuracy: 0.9013, Validation F1: 0.9029\n",
      "Epoch 14/25, Training Loss: 0.2562, Training Accuracy: 0.9045, Training F1: 0.9039, Validation Loss: 0.2903, Validation Accuracy: 0.8958, Validation F1: 0.8954\n",
      "Epoch 15/25, Training Loss: 0.2468, Training Accuracy: 0.9097, Training F1: 0.9092, Validation Loss: 0.2733, Validation Accuracy: 0.8965, Validation F1: 0.8983\n",
      "Epoch 16/25, Training Loss: 0.2382, Training Accuracy: 0.9121, Training F1: 0.9117, Validation Loss: 0.2725, Validation Accuracy: 0.8975, Validation F1: 0.8977\n",
      "Epoch 17/25, Training Loss: 0.2373, Training Accuracy: 0.9121, Training F1: 0.9116, Validation Loss: 0.2724, Validation Accuracy: 0.9008, Validation F1: 0.9007\n",
      "Epoch 18/25, Training Loss: 0.2293, Training Accuracy: 0.9147, Training F1: 0.9142, Validation Loss: 0.2737, Validation Accuracy: 0.8990, Validation F1: 0.9005\n",
      "Epoch 19/25, Training Loss: 0.2246, Training Accuracy: 0.9169, Training F1: 0.9165, Validation Loss: 0.2789, Validation Accuracy: 0.8962, Validation F1: 0.8974\n",
      "Epoch 20/25, Training Loss: 0.2202, Training Accuracy: 0.9171, Training F1: 0.9167, Validation Loss: 0.2711, Validation Accuracy: 0.9080, Validation F1: 0.9092\n",
      "Epoch 21/25, Training Loss: 0.2135, Training Accuracy: 0.9212, Training F1: 0.9209, Validation Loss: 0.2716, Validation Accuracy: 0.9017, Validation F1: 0.9031\n",
      "Epoch 22/25, Training Loss: 0.2086, Training Accuracy: 0.9235, Training F1: 0.9231, Validation Loss: 0.2674, Validation Accuracy: 0.9090, Validation F1: 0.9098\n",
      "Epoch 23/25, Training Loss: 0.2076, Training Accuracy: 0.9249, Training F1: 0.9245, Validation Loss: 0.2676, Validation Accuracy: 0.9030, Validation F1: 0.9044\n",
      "Epoch 24/25, Training Loss: 0.2050, Training Accuracy: 0.9244, Training F1: 0.9240, Validation Loss: 0.2762, Validation Accuracy: 0.9042, Validation F1: 0.9058\n",
      "Epoch 25/25, Training Loss: 0.1976, Training Accuracy: 0.9259, Training F1: 0.9255, Validation Loss: 0.2687, Validation Accuracy: 0.9023, Validation F1: 0.9039\n",
      "Test Loss: 0.2744, Test Accuracy: 0.9047, Test F1 Score: 0.9050\n",
      "  Training with learning rate: 0.0009\n",
      "Epoch 1/25, Training Loss: 0.5883, Training Accuracy: 0.7937, Training F1: 0.7918, Validation Loss: 0.3738, Validation Accuracy: 0.8612, Validation F1: 0.8623\n",
      "Epoch 2/25, Training Loss: 0.4361, Training Accuracy: 0.8431, Training F1: 0.8420, Validation Loss: 0.3440, Validation Accuracy: 0.8745, Validation F1: 0.8736\n",
      "Epoch 3/25, Training Loss: 0.3900, Training Accuracy: 0.8598, Training F1: 0.8588, Validation Loss: 0.3226, Validation Accuracy: 0.8803, Validation F1: 0.8822\n",
      "Epoch 4/25, Training Loss: 0.3656, Training Accuracy: 0.8681, Training F1: 0.8673, Validation Loss: 0.3230, Validation Accuracy: 0.8768, Validation F1: 0.8785\n",
      "Epoch 5/25, Training Loss: 0.3499, Training Accuracy: 0.8736, Training F1: 0.8727, Validation Loss: 0.3127, Validation Accuracy: 0.8832, Validation F1: 0.8824\n",
      "Epoch 6/25, Training Loss: 0.3330, Training Accuracy: 0.8796, Training F1: 0.8788, Validation Loss: 0.3101, Validation Accuracy: 0.8835, Validation F1: 0.8853\n",
      "Epoch 7/25, Training Loss: 0.3171, Training Accuracy: 0.8842, Training F1: 0.8835, Validation Loss: 0.2935, Validation Accuracy: 0.8920, Validation F1: 0.8924\n",
      "Epoch 8/25, Training Loss: 0.3069, Training Accuracy: 0.8869, Training F1: 0.8862, Validation Loss: 0.2858, Validation Accuracy: 0.8927, Validation F1: 0.8935\n",
      "Epoch 9/25, Training Loss: 0.2990, Training Accuracy: 0.8905, Training F1: 0.8899, Validation Loss: 0.2896, Validation Accuracy: 0.8915, Validation F1: 0.8934\n",
      "Epoch 10/25, Training Loss: 0.2847, Training Accuracy: 0.8961, Training F1: 0.8955, Validation Loss: 0.2803, Validation Accuracy: 0.8960, Validation F1: 0.8977\n",
      "Epoch 11/25, Training Loss: 0.2774, Training Accuracy: 0.8983, Training F1: 0.8978, Validation Loss: 0.2947, Validation Accuracy: 0.8913, Validation F1: 0.8917\n",
      "Epoch 12/25, Training Loss: 0.2685, Training Accuracy: 0.9004, Training F1: 0.8998, Validation Loss: 0.2870, Validation Accuracy: 0.8965, Validation F1: 0.8972\n",
      "Epoch 13/25, Training Loss: 0.2617, Training Accuracy: 0.9023, Training F1: 0.9017, Validation Loss: 0.2816, Validation Accuracy: 0.8978, Validation F1: 0.8997\n",
      "Epoch 14/25, Training Loss: 0.2523, Training Accuracy: 0.9063, Training F1: 0.9058, Validation Loss: 0.2823, Validation Accuracy: 0.8953, Validation F1: 0.8955\n",
      "Epoch 15/25, Training Loss: 0.2512, Training Accuracy: 0.9072, Training F1: 0.9066, Validation Loss: 0.2739, Validation Accuracy: 0.9023, Validation F1: 0.9031\n",
      "Epoch 16/25, Training Loss: 0.2424, Training Accuracy: 0.9105, Training F1: 0.9099, Validation Loss: 0.2929, Validation Accuracy: 0.8862, Validation F1: 0.8884\n",
      "Epoch 17/25, Training Loss: 0.2347, Training Accuracy: 0.9120, Training F1: 0.9115, Validation Loss: 0.2739, Validation Accuracy: 0.8997, Validation F1: 0.9009\n",
      "Epoch 18/25, Training Loss: 0.2300, Training Accuracy: 0.9156, Training F1: 0.9152, Validation Loss: 0.2699, Validation Accuracy: 0.9010, Validation F1: 0.9027\n",
      "Epoch 19/25, Training Loss: 0.2273, Training Accuracy: 0.9167, Training F1: 0.9163, Validation Loss: 0.2733, Validation Accuracy: 0.9022, Validation F1: 0.9030\n",
      "Epoch 20/25, Training Loss: 0.2215, Training Accuracy: 0.9175, Training F1: 0.9171, Validation Loss: 0.2794, Validation Accuracy: 0.9003, Validation F1: 0.9018\n",
      "Epoch 21/25, Training Loss: 0.2184, Training Accuracy: 0.9190, Training F1: 0.9186, Validation Loss: 0.2830, Validation Accuracy: 0.8985, Validation F1: 0.9001\n",
      "Epoch 22/25, Training Loss: 0.2071, Training Accuracy: 0.9240, Training F1: 0.9236, Validation Loss: 0.2787, Validation Accuracy: 0.9003, Validation F1: 0.9009\n",
      "Epoch 23/25, Training Loss: 0.2055, Training Accuracy: 0.9239, Training F1: 0.9235, Validation Loss: 0.2839, Validation Accuracy: 0.8997, Validation F1: 0.9006\n",
      "Epoch 24/25, Training Loss: 0.2039, Training Accuracy: 0.9241, Training F1: 0.9238, Validation Loss: 0.2753, Validation Accuracy: 0.9017, Validation F1: 0.9035\n",
      "Epoch 25/25, Training Loss: 0.1975, Training Accuracy: 0.9272, Training F1: 0.9268, Validation Loss: 0.2925, Validation Accuracy: 0.9020, Validation F1: 0.9035\n",
      "Test Loss: 0.2808, Test Accuracy: 0.9030, Test F1 Score: 0.9030\n",
      "  Training with learning rate: 0.0006\n",
      "Epoch 1/25, Training Loss: 0.6285, Training Accuracy: 0.7806, Training F1: 0.7788, Validation Loss: 0.3802, Validation Accuracy: 0.8572, Validation F1: 0.8543\n",
      "Epoch 2/25, Training Loss: 0.4451, Training Accuracy: 0.8411, Training F1: 0.8399, Validation Loss: 0.3638, Validation Accuracy: 0.8642, Validation F1: 0.8664\n",
      "Epoch 3/25, Training Loss: 0.4017, Training Accuracy: 0.8554, Training F1: 0.8544, Validation Loss: 0.3261, Validation Accuracy: 0.8785, Validation F1: 0.8792\n",
      "Epoch 4/25, Training Loss: 0.3713, Training Accuracy: 0.8659, Training F1: 0.8650, Validation Loss: 0.3187, Validation Accuracy: 0.8843, Validation F1: 0.8831\n",
      "Epoch 5/25, Training Loss: 0.3499, Training Accuracy: 0.8729, Training F1: 0.8720, Validation Loss: 0.3041, Validation Accuracy: 0.8840, Validation F1: 0.8853\n",
      "Epoch 6/25, Training Loss: 0.3367, Training Accuracy: 0.8774, Training F1: 0.8765, Validation Loss: 0.2929, Validation Accuracy: 0.8888, Validation F1: 0.8899\n",
      "Epoch 7/25, Training Loss: 0.3246, Training Accuracy: 0.8819, Training F1: 0.8813, Validation Loss: 0.2962, Validation Accuracy: 0.8873, Validation F1: 0.8898\n",
      "Epoch 8/25, Training Loss: 0.3135, Training Accuracy: 0.8855, Training F1: 0.8848, Validation Loss: 0.2918, Validation Accuracy: 0.8918, Validation F1: 0.8927\n",
      "Epoch 9/25, Training Loss: 0.2984, Training Accuracy: 0.8891, Training F1: 0.8885, Validation Loss: 0.2802, Validation Accuracy: 0.8965, Validation F1: 0.8965\n",
      "Epoch 10/25, Training Loss: 0.2899, Training Accuracy: 0.8929, Training F1: 0.8923, Validation Loss: 0.2789, Validation Accuracy: 0.8950, Validation F1: 0.8967\n",
      "Epoch 11/25, Training Loss: 0.2800, Training Accuracy: 0.8970, Training F1: 0.8964, Validation Loss: 0.2983, Validation Accuracy: 0.8912, Validation F1: 0.8934\n",
      "Epoch 12/25, Training Loss: 0.2735, Training Accuracy: 0.8985, Training F1: 0.8979, Validation Loss: 0.2764, Validation Accuracy: 0.8933, Validation F1: 0.8950\n",
      "Epoch 13/25, Training Loss: 0.2658, Training Accuracy: 0.9023, Training F1: 0.9017, Validation Loss: 0.2898, Validation Accuracy: 0.8922, Validation F1: 0.8921\n",
      "Epoch 14/25, Training Loss: 0.2614, Training Accuracy: 0.9045, Training F1: 0.9040, Validation Loss: 0.2732, Validation Accuracy: 0.8993, Validation F1: 0.9004\n",
      "Epoch 15/25, Training Loss: 0.2495, Training Accuracy: 0.9070, Training F1: 0.9064, Validation Loss: 0.2664, Validation Accuracy: 0.8995, Validation F1: 0.9007\n",
      "Epoch 16/25, Training Loss: 0.2445, Training Accuracy: 0.9081, Training F1: 0.9076, Validation Loss: 0.2685, Validation Accuracy: 0.8962, Validation F1: 0.8971\n",
      "Epoch 17/25, Training Loss: 0.2365, Training Accuracy: 0.9125, Training F1: 0.9120, Validation Loss: 0.2762, Validation Accuracy: 0.8978, Validation F1: 0.8993\n",
      "Epoch 18/25, Training Loss: 0.2315, Training Accuracy: 0.9131, Training F1: 0.9127, Validation Loss: 0.2854, Validation Accuracy: 0.8928, Validation F1: 0.8952\n",
      "Epoch 19/25, Training Loss: 0.2241, Training Accuracy: 0.9168, Training F1: 0.9164, Validation Loss: 0.2695, Validation Accuracy: 0.9043, Validation F1: 0.9044\n",
      "Epoch 20/25, Training Loss: 0.2187, Training Accuracy: 0.9190, Training F1: 0.9185, Validation Loss: 0.2778, Validation Accuracy: 0.8973, Validation F1: 0.8987\n",
      "Epoch 21/25, Training Loss: 0.2185, Training Accuracy: 0.9192, Training F1: 0.9188, Validation Loss: 0.2648, Validation Accuracy: 0.9003, Validation F1: 0.9023\n",
      "Epoch 22/25, Training Loss: 0.2148, Training Accuracy: 0.9195, Training F1: 0.9191, Validation Loss: 0.2768, Validation Accuracy: 0.9000, Validation F1: 0.9026\n",
      "Epoch 23/25, Training Loss: 0.2069, Training Accuracy: 0.9218, Training F1: 0.9214, Validation Loss: 0.2752, Validation Accuracy: 0.8910, Validation F1: 0.8933\n",
      "Epoch 24/25, Training Loss: 0.1997, Training Accuracy: 0.9260, Training F1: 0.9256, Validation Loss: 0.2809, Validation Accuracy: 0.8992, Validation F1: 0.9001\n",
      "Epoch 25/25, Training Loss: 0.1957, Training Accuracy: 0.9269, Training F1: 0.9265, Validation Loss: 0.2770, Validation Accuracy: 0.9015, Validation F1: 0.9033\n",
      "Test Loss: 0.2760, Test Accuracy: 0.9037, Test F1 Score: 0.9039\n",
      "\n",
      "Training models for architecture: Arch2\n",
      "  Training with learning rate: 0.005\n",
      "Epoch 1/25, Training Loss: 0.5739, Training Accuracy: 0.7968, Training F1: 0.7945, Validation Loss: 0.3818, Validation Accuracy: 0.8560, Validation F1: 0.8569\n",
      "Epoch 2/25, Training Loss: 0.4481, Training Accuracy: 0.8419, Training F1: 0.8406, Validation Loss: 0.3452, Validation Accuracy: 0.8725, Validation F1: 0.8719\n",
      "Epoch 3/25, Training Loss: 0.4084, Training Accuracy: 0.8561, Training F1: 0.8550, Validation Loss: 0.3305, Validation Accuracy: 0.8813, Validation F1: 0.8805\n",
      "Epoch 4/25, Training Loss: 0.3815, Training Accuracy: 0.8646, Training F1: 0.8637, Validation Loss: 0.3198, Validation Accuracy: 0.8813, Validation F1: 0.8826\n",
      "Epoch 5/25, Training Loss: 0.3606, Training Accuracy: 0.8696, Training F1: 0.8688, Validation Loss: 0.3069, Validation Accuracy: 0.8832, Validation F1: 0.8853\n",
      "Epoch 6/25, Training Loss: 0.3483, Training Accuracy: 0.8748, Training F1: 0.8740, Validation Loss: 0.3015, Validation Accuracy: 0.8828, Validation F1: 0.8838\n",
      "Epoch 7/25, Training Loss: 0.3319, Training Accuracy: 0.8799, Training F1: 0.8791, Validation Loss: 0.3219, Validation Accuracy: 0.8793, Validation F1: 0.8809\n",
      "Epoch 8/25, Training Loss: 0.3166, Training Accuracy: 0.8848, Training F1: 0.8840, Validation Loss: 0.2925, Validation Accuracy: 0.8898, Validation F1: 0.8897\n",
      "Epoch 9/25, Training Loss: 0.3085, Training Accuracy: 0.8890, Training F1: 0.8883, Validation Loss: 0.2979, Validation Accuracy: 0.8868, Validation F1: 0.8879\n",
      "Epoch 10/25, Training Loss: 0.2971, Training Accuracy: 0.8922, Training F1: 0.8915, Validation Loss: 0.2781, Validation Accuracy: 0.9002, Validation F1: 0.9004\n",
      "Epoch 11/25, Training Loss: 0.2881, Training Accuracy: 0.8955, Training F1: 0.8949, Validation Loss: 0.2828, Validation Accuracy: 0.8942, Validation F1: 0.8960\n",
      "Epoch 12/25, Training Loss: 0.2765, Training Accuracy: 0.8992, Training F1: 0.8986, Validation Loss: 0.3009, Validation Accuracy: 0.8918, Validation F1: 0.8931\n",
      "Epoch 13/25, Training Loss: 0.2716, Training Accuracy: 0.9012, Training F1: 0.9006, Validation Loss: 0.2934, Validation Accuracy: 0.8907, Validation F1: 0.8901\n",
      "Epoch 14/25, Training Loss: 0.2613, Training Accuracy: 0.9036, Training F1: 0.9031, Validation Loss: 0.2700, Validation Accuracy: 0.8995, Validation F1: 0.9007\n",
      "Epoch 15/25, Training Loss: 0.2529, Training Accuracy: 0.9077, Training F1: 0.9072, Validation Loss: 0.2788, Validation Accuracy: 0.8975, Validation F1: 0.8992\n",
      "Epoch 16/25, Training Loss: 0.2461, Training Accuracy: 0.9114, Training F1: 0.9109, Validation Loss: 0.2796, Validation Accuracy: 0.8915, Validation F1: 0.8924\n",
      "Epoch 17/25, Training Loss: 0.2408, Training Accuracy: 0.9104, Training F1: 0.9099, Validation Loss: 0.2789, Validation Accuracy: 0.8955, Validation F1: 0.8968\n",
      "Epoch 18/25, Training Loss: 0.2311, Training Accuracy: 0.9152, Training F1: 0.9148, Validation Loss: 0.2999, Validation Accuracy: 0.8947, Validation F1: 0.8942\n",
      "Epoch 19/25, Training Loss: 0.2275, Training Accuracy: 0.9150, Training F1: 0.9145, Validation Loss: 0.2738, Validation Accuracy: 0.9022, Validation F1: 0.9036\n",
      "Epoch 20/25, Training Loss: 0.2197, Training Accuracy: 0.9196, Training F1: 0.9192, Validation Loss: 0.2711, Validation Accuracy: 0.9037, Validation F1: 0.9044\n",
      "Epoch 21/25, Training Loss: 0.2145, Training Accuracy: 0.9216, Training F1: 0.9212, Validation Loss: 0.2788, Validation Accuracy: 0.8977, Validation F1: 0.8995\n",
      "Epoch 22/25, Training Loss: 0.2085, Training Accuracy: 0.9237, Training F1: 0.9233, Validation Loss: 0.2742, Validation Accuracy: 0.9088, Validation F1: 0.9101\n",
      "Epoch 23/25, Training Loss: 0.2089, Training Accuracy: 0.9234, Training F1: 0.9231, Validation Loss: 0.2721, Validation Accuracy: 0.9028, Validation F1: 0.9034\n",
      "Epoch 24/25, Training Loss: 0.2020, Training Accuracy: 0.9257, Training F1: 0.9253, Validation Loss: 0.2681, Validation Accuracy: 0.9053, Validation F1: 0.9068\n",
      "Epoch 25/25, Training Loss: 0.1970, Training Accuracy: 0.9280, Training F1: 0.9276, Validation Loss: 0.2960, Validation Accuracy: 0.8963, Validation F1: 0.8995\n",
      "Test Loss: 0.2919, Test Accuracy: 0.8974, Test F1 Score: 0.8985\n",
      "  Training with learning rate: 0.001\n",
      "Epoch 1/25, Training Loss: 0.5955, Training Accuracy: 0.7941, Training F1: 0.7917, Validation Loss: 0.3628, Validation Accuracy: 0.8662, Validation F1: 0.8653\n",
      "Epoch 2/25, Training Loss: 0.4321, Training Accuracy: 0.8456, Training F1: 0.8443, Validation Loss: 0.3320, Validation Accuracy: 0.8783, Validation F1: 0.8802\n",
      "Epoch 3/25, Training Loss: 0.3922, Training Accuracy: 0.8595, Training F1: 0.8586, Validation Loss: 0.3180, Validation Accuracy: 0.8850, Validation F1: 0.8864\n",
      "Epoch 4/25, Training Loss: 0.3702, Training Accuracy: 0.8671, Training F1: 0.8662, Validation Loss: 0.3232, Validation Accuracy: 0.8793, Validation F1: 0.8800\n",
      "Epoch 5/25, Training Loss: 0.3474, Training Accuracy: 0.8735, Training F1: 0.8728, Validation Loss: 0.2969, Validation Accuracy: 0.8887, Validation F1: 0.8909\n",
      "Epoch 6/25, Training Loss: 0.3301, Training Accuracy: 0.8806, Training F1: 0.8800, Validation Loss: 0.2984, Validation Accuracy: 0.8898, Validation F1: 0.8907\n",
      "Epoch 7/25, Training Loss: 0.3179, Training Accuracy: 0.8842, Training F1: 0.8836, Validation Loss: 0.2895, Validation Accuracy: 0.8925, Validation F1: 0.8939\n",
      "Epoch 8/25, Training Loss: 0.3073, Training Accuracy: 0.8886, Training F1: 0.8880, Validation Loss: 0.2991, Validation Accuracy: 0.8857, Validation F1: 0.8880\n",
      "Epoch 9/25, Training Loss: 0.2946, Training Accuracy: 0.8928, Training F1: 0.8922, Validation Loss: 0.3001, Validation Accuracy: 0.8895, Validation F1: 0.8896\n",
      "Epoch 10/25, Training Loss: 0.2867, Training Accuracy: 0.8958, Training F1: 0.8952, Validation Loss: 0.2971, Validation Accuracy: 0.8908, Validation F1: 0.8917\n",
      "Epoch 11/25, Training Loss: 0.2749, Training Accuracy: 0.9017, Training F1: 0.9012, Validation Loss: 0.3315, Validation Accuracy: 0.8757, Validation F1: 0.8812\n",
      "Epoch 12/25, Training Loss: 0.2660, Training Accuracy: 0.9023, Training F1: 0.9017, Validation Loss: 0.2732, Validation Accuracy: 0.8993, Validation F1: 0.9010\n",
      "Epoch 13/25, Training Loss: 0.2604, Training Accuracy: 0.9044, Training F1: 0.9039, Validation Loss: 0.2902, Validation Accuracy: 0.8943, Validation F1: 0.8953\n",
      "Epoch 14/25, Training Loss: 0.2481, Training Accuracy: 0.9077, Training F1: 0.9072, Validation Loss: 0.2735, Validation Accuracy: 0.9040, Validation F1: 0.9051\n",
      "Epoch 15/25, Training Loss: 0.2411, Training Accuracy: 0.9113, Training F1: 0.9108, Validation Loss: 0.2909, Validation Accuracy: 0.8925, Validation F1: 0.8912\n",
      "Epoch 16/25, Training Loss: 0.2363, Training Accuracy: 0.9141, Training F1: 0.9137, Validation Loss: 0.2704, Validation Accuracy: 0.9003, Validation F1: 0.9017\n",
      "Epoch 17/25, Training Loss: 0.2295, Training Accuracy: 0.9155, Training F1: 0.9151, Validation Loss: 0.2901, Validation Accuracy: 0.8987, Validation F1: 0.8989\n",
      "Epoch 18/25, Training Loss: 0.2227, Training Accuracy: 0.9186, Training F1: 0.9182, Validation Loss: 0.2803, Validation Accuracy: 0.8977, Validation F1: 0.8992\n",
      "Epoch 19/25, Training Loss: 0.2179, Training Accuracy: 0.9190, Training F1: 0.9185, Validation Loss: 0.2934, Validation Accuracy: 0.8980, Validation F1: 0.8974\n",
      "Epoch 20/25, Training Loss: 0.2064, Training Accuracy: 0.9239, Training F1: 0.9236, Validation Loss: 0.2707, Validation Accuracy: 0.9037, Validation F1: 0.9050\n",
      "Epoch 21/25, Training Loss: 0.2037, Training Accuracy: 0.9246, Training F1: 0.9241, Validation Loss: 0.2872, Validation Accuracy: 0.9032, Validation F1: 0.9027\n",
      "Epoch 22/25, Training Loss: 0.2003, Training Accuracy: 0.9266, Training F1: 0.9263, Validation Loss: 0.2782, Validation Accuracy: 0.9023, Validation F1: 0.9021\n",
      "Epoch 23/25, Training Loss: 0.1927, Training Accuracy: 0.9294, Training F1: 0.9291, Validation Loss: 0.2746, Validation Accuracy: 0.9047, Validation F1: 0.9058\n",
      "Epoch 24/25, Training Loss: 0.1875, Training Accuracy: 0.9303, Training F1: 0.9300, Validation Loss: 0.2739, Validation Accuracy: 0.9032, Validation F1: 0.9045\n",
      "Epoch 25/25, Training Loss: 0.1845, Training Accuracy: 0.9318, Training F1: 0.9315, Validation Loss: 0.2811, Validation Accuracy: 0.9063, Validation F1: 0.9070\n",
      "Test Loss: 0.2755, Test Accuracy: 0.9047, Test F1 Score: 0.9033\n",
      "  Training with learning rate: 0.0009\n",
      "Epoch 1/25, Training Loss: 0.5995, Training Accuracy: 0.7938, Training F1: 0.7918, Validation Loss: 0.3649, Validation Accuracy: 0.8680, Validation F1: 0.8700\n",
      "Epoch 2/25, Training Loss: 0.4348, Training Accuracy: 0.8456, Training F1: 0.8446, Validation Loss: 0.3325, Validation Accuracy: 0.8823, Validation F1: 0.8823\n",
      "Epoch 3/25, Training Loss: 0.3949, Training Accuracy: 0.8585, Training F1: 0.8575, Validation Loss: 0.3286, Validation Accuracy: 0.8777, Validation F1: 0.8781\n",
      "Epoch 4/25, Training Loss: 0.3677, Training Accuracy: 0.8686, Training F1: 0.8675, Validation Loss: 0.3170, Validation Accuracy: 0.8817, Validation F1: 0.8819\n",
      "Epoch 5/25, Training Loss: 0.3484, Training Accuracy: 0.8743, Training F1: 0.8734, Validation Loss: 0.3148, Validation Accuracy: 0.8810, Validation F1: 0.8811\n",
      "Epoch 6/25, Training Loss: 0.3329, Training Accuracy: 0.8801, Training F1: 0.8794, Validation Loss: 0.3056, Validation Accuracy: 0.8852, Validation F1: 0.8864\n",
      "Epoch 7/25, Training Loss: 0.3153, Training Accuracy: 0.8863, Training F1: 0.8856, Validation Loss: 0.3102, Validation Accuracy: 0.8822, Validation F1: 0.8778\n",
      "Epoch 8/25, Training Loss: 0.3060, Training Accuracy: 0.8882, Training F1: 0.8874, Validation Loss: 0.3007, Validation Accuracy: 0.8875, Validation F1: 0.8870\n",
      "Epoch 9/25, Training Loss: 0.2939, Training Accuracy: 0.8933, Training F1: 0.8926, Validation Loss: 0.2907, Validation Accuracy: 0.8945, Validation F1: 0.8952\n",
      "Epoch 10/25, Training Loss: 0.2858, Training Accuracy: 0.8957, Training F1: 0.8951, Validation Loss: 0.2844, Validation Accuracy: 0.8925, Validation F1: 0.8925\n",
      "Epoch 11/25, Training Loss: 0.2753, Training Accuracy: 0.9001, Training F1: 0.8995, Validation Loss: 0.2847, Validation Accuracy: 0.8915, Validation F1: 0.8937\n",
      "Epoch 12/25, Training Loss: 0.2667, Training Accuracy: 0.9024, Training F1: 0.9018, Validation Loss: 0.3204, Validation Accuracy: 0.8853, Validation F1: 0.8818\n",
      "Epoch 13/25, Training Loss: 0.2568, Training Accuracy: 0.9061, Training F1: 0.9056, Validation Loss: 0.2724, Validation Accuracy: 0.8958, Validation F1: 0.8980\n",
      "Epoch 14/25, Training Loss: 0.2497, Training Accuracy: 0.9086, Training F1: 0.9081, Validation Loss: 0.2701, Validation Accuracy: 0.8962, Validation F1: 0.8969\n",
      "Epoch 15/25, Training Loss: 0.2424, Training Accuracy: 0.9114, Training F1: 0.9109, Validation Loss: 0.2833, Validation Accuracy: 0.8967, Validation F1: 0.8967\n",
      "Epoch 16/25, Training Loss: 0.2338, Training Accuracy: 0.9134, Training F1: 0.9129, Validation Loss: 0.2691, Validation Accuracy: 0.9008, Validation F1: 0.9030\n",
      "Epoch 17/25, Training Loss: 0.2259, Training Accuracy: 0.9161, Training F1: 0.9157, Validation Loss: 0.2783, Validation Accuracy: 0.8980, Validation F1: 0.9008\n",
      "Epoch 18/25, Training Loss: 0.2222, Training Accuracy: 0.9181, Training F1: 0.9176, Validation Loss: 0.2756, Validation Accuracy: 0.9012, Validation F1: 0.9021\n",
      "Epoch 19/25, Training Loss: 0.2159, Training Accuracy: 0.9209, Training F1: 0.9205, Validation Loss: 0.2873, Validation Accuracy: 0.8942, Validation F1: 0.8962\n",
      "Epoch 20/25, Training Loss: 0.2124, Training Accuracy: 0.9216, Training F1: 0.9212, Validation Loss: 0.2670, Validation Accuracy: 0.9070, Validation F1: 0.9085\n",
      "Epoch 21/25, Training Loss: 0.2030, Training Accuracy: 0.9259, Training F1: 0.9256, Validation Loss: 0.2783, Validation Accuracy: 0.9010, Validation F1: 0.9022\n",
      "Epoch 22/25, Training Loss: 0.1974, Training Accuracy: 0.9264, Training F1: 0.9259, Validation Loss: 0.2996, Validation Accuracy: 0.8997, Validation F1: 0.8992\n",
      "Epoch 23/25, Training Loss: 0.1953, Training Accuracy: 0.9284, Training F1: 0.9280, Validation Loss: 0.2735, Validation Accuracy: 0.9060, Validation F1: 0.9074\n",
      "Epoch 24/25, Training Loss: 0.1885, Training Accuracy: 0.9303, Training F1: 0.9299, Validation Loss: 0.2777, Validation Accuracy: 0.9077, Validation F1: 0.9085\n",
      "Epoch 25/25, Training Loss: 0.1834, Training Accuracy: 0.9325, Training F1: 0.9322, Validation Loss: 0.2922, Validation Accuracy: 0.8990, Validation F1: 0.9014\n",
      "Test Loss: 0.2898, Test Accuracy: 0.8986, Test F1 Score: 0.8989\n",
      "  Training with learning rate: 0.0006\n",
      "Epoch 1/25, Training Loss: 0.6249, Training Accuracy: 0.7866, Training F1: 0.7849, Validation Loss: 0.3676, Validation Accuracy: 0.8658, Validation F1: 0.8642\n",
      "Epoch 2/25, Training Loss: 0.4413, Training Accuracy: 0.8439, Training F1: 0.8428, Validation Loss: 0.3355, Validation Accuracy: 0.8718, Validation F1: 0.8715\n",
      "Epoch 3/25, Training Loss: 0.3999, Training Accuracy: 0.8581, Training F1: 0.8572, Validation Loss: 0.3099, Validation Accuracy: 0.8860, Validation F1: 0.8856\n",
      "Epoch 4/25, Training Loss: 0.3712, Training Accuracy: 0.8661, Training F1: 0.8653, Validation Loss: 0.3146, Validation Accuracy: 0.8837, Validation F1: 0.8828\n",
      "Epoch 5/25, Training Loss: 0.3462, Training Accuracy: 0.8761, Training F1: 0.8753, Validation Loss: 0.3065, Validation Accuracy: 0.8882, Validation F1: 0.8880\n",
      "Epoch 6/25, Training Loss: 0.3329, Training Accuracy: 0.8793, Training F1: 0.8785, Validation Loss: 0.3258, Validation Accuracy: 0.8777, Validation F1: 0.8802\n",
      "Epoch 7/25, Training Loss: 0.3183, Training Accuracy: 0.8835, Training F1: 0.8828, Validation Loss: 0.2945, Validation Accuracy: 0.8903, Validation F1: 0.8907\n",
      "Epoch 8/25, Training Loss: 0.3051, Training Accuracy: 0.8886, Training F1: 0.8880, Validation Loss: 0.3050, Validation Accuracy: 0.8877, Validation F1: 0.8893\n",
      "Epoch 9/25, Training Loss: 0.2971, Training Accuracy: 0.8925, Training F1: 0.8918, Validation Loss: 0.3047, Validation Accuracy: 0.8865, Validation F1: 0.8861\n",
      "Epoch 10/25, Training Loss: 0.2847, Training Accuracy: 0.8946, Training F1: 0.8940, Validation Loss: 0.2738, Validation Accuracy: 0.9013, Validation F1: 0.9021\n",
      "Epoch 11/25, Training Loss: 0.2767, Training Accuracy: 0.8988, Training F1: 0.8983, Validation Loss: 0.2878, Validation Accuracy: 0.8928, Validation F1: 0.8937\n",
      "Epoch 12/25, Training Loss: 0.2664, Training Accuracy: 0.9025, Training F1: 0.9019, Validation Loss: 0.2649, Validation Accuracy: 0.9020, Validation F1: 0.9035\n",
      "Epoch 13/25, Training Loss: 0.2565, Training Accuracy: 0.9055, Training F1: 0.9050, Validation Loss: 0.2983, Validation Accuracy: 0.8912, Validation F1: 0.8903\n",
      "Epoch 14/25, Training Loss: 0.2505, Training Accuracy: 0.9077, Training F1: 0.9071, Validation Loss: 0.2793, Validation Accuracy: 0.8977, Validation F1: 0.8979\n",
      "Epoch 15/25, Training Loss: 0.2398, Training Accuracy: 0.9116, Training F1: 0.9112, Validation Loss: 0.2903, Validation Accuracy: 0.8963, Validation F1: 0.8956\n",
      "Epoch 16/25, Training Loss: 0.2339, Training Accuracy: 0.9140, Training F1: 0.9135, Validation Loss: 0.2606, Validation Accuracy: 0.9000, Validation F1: 0.9014\n",
      "Epoch 17/25, Training Loss: 0.2293, Training Accuracy: 0.9163, Training F1: 0.9159, Validation Loss: 0.2600, Validation Accuracy: 0.9052, Validation F1: 0.9060\n",
      "Epoch 18/25, Training Loss: 0.2225, Training Accuracy: 0.9185, Training F1: 0.9180, Validation Loss: 0.2836, Validation Accuracy: 0.8992, Validation F1: 0.8996\n",
      "Epoch 19/25, Training Loss: 0.2177, Training Accuracy: 0.9206, Training F1: 0.9202, Validation Loss: 0.2653, Validation Accuracy: 0.9032, Validation F1: 0.9049\n",
      "Epoch 20/25, Training Loss: 0.2074, Training Accuracy: 0.9231, Training F1: 0.9227, Validation Loss: 0.2671, Validation Accuracy: 0.9065, Validation F1: 0.9076\n",
      "Epoch 21/25, Training Loss: 0.2085, Training Accuracy: 0.9233, Training F1: 0.9229, Validation Loss: 0.2589, Validation Accuracy: 0.9093, Validation F1: 0.9103\n",
      "Epoch 22/25, Training Loss: 0.2018, Training Accuracy: 0.9256, Training F1: 0.9252, Validation Loss: 0.2756, Validation Accuracy: 0.9047, Validation F1: 0.9058\n",
      "Epoch 23/25, Training Loss: 0.1940, Training Accuracy: 0.9289, Training F1: 0.9285, Validation Loss: 0.2681, Validation Accuracy: 0.9000, Validation F1: 0.9011\n",
      "Epoch 24/25, Training Loss: 0.1894, Training Accuracy: 0.9301, Training F1: 0.9297, Validation Loss: 0.2908, Validation Accuracy: 0.8923, Validation F1: 0.8955\n",
      "Epoch 25/25, Training Loss: 0.1845, Training Accuracy: 0.9321, Training F1: 0.9318, Validation Loss: 0.3115, Validation Accuracy: 0.8977, Validation F1: 0.8969\n",
      "Test Loss: 0.2981, Test Accuracy: 0.8986, Test F1 Score: 0.8967\n",
      "\n",
      "Training models for architecture: Arch3\n",
      "  Training with learning rate: 0.005\n",
      "Epoch 1/25, Training Loss: 0.5420, Training Accuracy: 0.8067, Training F1: 0.8053, Validation Loss: 0.3835, Validation Accuracy: 0.8565, Validation F1: 0.8578\n",
      "Epoch 2/25, Training Loss: 0.4264, Training Accuracy: 0.8454, Training F1: 0.8444, Validation Loss: 0.3732, Validation Accuracy: 0.8597, Validation F1: 0.8582\n",
      "Epoch 3/25, Training Loss: 0.3912, Training Accuracy: 0.8583, Training F1: 0.8573, Validation Loss: 0.3348, Validation Accuracy: 0.8777, Validation F1: 0.8764\n",
      "Epoch 4/25, Training Loss: 0.3700, Training Accuracy: 0.8653, Training F1: 0.8644, Validation Loss: 0.3667, Validation Accuracy: 0.8647, Validation F1: 0.8644\n",
      "Epoch 5/25, Training Loss: 0.3507, Training Accuracy: 0.8710, Training F1: 0.8703, Validation Loss: 0.3102, Validation Accuracy: 0.8833, Validation F1: 0.8850\n",
      "Epoch 6/25, Training Loss: 0.3335, Training Accuracy: 0.8767, Training F1: 0.8760, Validation Loss: 0.3014, Validation Accuracy: 0.8890, Validation F1: 0.8893\n",
      "Epoch 7/25, Training Loss: 0.3256, Training Accuracy: 0.8799, Training F1: 0.8791, Validation Loss: 0.2990, Validation Accuracy: 0.8880, Validation F1: 0.8893\n",
      "Epoch 8/25, Training Loss: 0.3181, Training Accuracy: 0.8827, Training F1: 0.8819, Validation Loss: 0.2905, Validation Accuracy: 0.8978, Validation F1: 0.8984\n",
      "Epoch 9/25, Training Loss: 0.3051, Training Accuracy: 0.8878, Training F1: 0.8871, Validation Loss: 0.2993, Validation Accuracy: 0.8890, Validation F1: 0.8905\n",
      "Epoch 10/25, Training Loss: 0.2998, Training Accuracy: 0.8895, Training F1: 0.8888, Validation Loss: 0.2846, Validation Accuracy: 0.8948, Validation F1: 0.8947\n",
      "Epoch 11/25, Training Loss: 0.2915, Training Accuracy: 0.8919, Training F1: 0.8912, Validation Loss: 0.2823, Validation Accuracy: 0.8942, Validation F1: 0.8950\n",
      "Epoch 12/25, Training Loss: 0.2829, Training Accuracy: 0.8955, Training F1: 0.8950, Validation Loss: 0.2978, Validation Accuracy: 0.8883, Validation F1: 0.8889\n",
      "Epoch 13/25, Training Loss: 0.2778, Training Accuracy: 0.8972, Training F1: 0.8966, Validation Loss: 0.2802, Validation Accuracy: 0.8948, Validation F1: 0.8968\n",
      "Epoch 14/25, Training Loss: 0.2677, Training Accuracy: 0.9009, Training F1: 0.9003, Validation Loss: 0.3030, Validation Accuracy: 0.8905, Validation F1: 0.8898\n",
      "Epoch 15/25, Training Loss: 0.2632, Training Accuracy: 0.9014, Training F1: 0.9009, Validation Loss: 0.2830, Validation Accuracy: 0.8958, Validation F1: 0.8968\n",
      "Epoch 16/25, Training Loss: 0.2599, Training Accuracy: 0.9030, Training F1: 0.9025, Validation Loss: 0.2796, Validation Accuracy: 0.9035, Validation F1: 0.9042\n",
      "Epoch 17/25, Training Loss: 0.2541, Training Accuracy: 0.9047, Training F1: 0.9041, Validation Loss: 0.2861, Validation Accuracy: 0.8958, Validation F1: 0.8973\n",
      "Epoch 18/25, Training Loss: 0.2470, Training Accuracy: 0.9072, Training F1: 0.9067, Validation Loss: 0.2853, Validation Accuracy: 0.8998, Validation F1: 0.9004\n",
      "Epoch 19/25, Training Loss: 0.2483, Training Accuracy: 0.9059, Training F1: 0.9053, Validation Loss: 0.2779, Validation Accuracy: 0.9018, Validation F1: 0.9025\n",
      "Epoch 20/25, Training Loss: 0.2398, Training Accuracy: 0.9105, Training F1: 0.9100, Validation Loss: 0.2833, Validation Accuracy: 0.8985, Validation F1: 0.8998\n",
      "Epoch 21/25, Training Loss: 0.2372, Training Accuracy: 0.9111, Training F1: 0.9106, Validation Loss: 0.2829, Validation Accuracy: 0.9015, Validation F1: 0.9035\n",
      "Epoch 22/25, Training Loss: 0.2344, Training Accuracy: 0.9124, Training F1: 0.9119, Validation Loss: 0.2875, Validation Accuracy: 0.8975, Validation F1: 0.8978\n",
      "Epoch 23/25, Training Loss: 0.2302, Training Accuracy: 0.9143, Training F1: 0.9138, Validation Loss: 0.2847, Validation Accuracy: 0.8990, Validation F1: 0.9008\n",
      "Epoch 24/25, Training Loss: 0.2285, Training Accuracy: 0.9145, Training F1: 0.9140, Validation Loss: 0.2835, Validation Accuracy: 0.9028, Validation F1: 0.9040\n",
      "Epoch 25/25, Training Loss: 0.2225, Training Accuracy: 0.9167, Training F1: 0.9162, Validation Loss: 0.2780, Validation Accuracy: 0.9025, Validation F1: 0.9037\n",
      "Test Loss: 0.2782, Test Accuracy: 0.9030, Test F1 Score: 0.9026\n",
      "  Training with learning rate: 0.001\n",
      "Epoch 1/25, Training Loss: 0.5711, Training Accuracy: 0.7985, Training F1: 0.7967, Validation Loss: 0.3716, Validation Accuracy: 0.8632, Validation F1: 0.8640\n",
      "Epoch 2/25, Training Loss: 0.4306, Training Accuracy: 0.8447, Training F1: 0.8437, Validation Loss: 0.3425, Validation Accuracy: 0.8695, Validation F1: 0.8722\n",
      "Epoch 3/25, Training Loss: 0.3873, Training Accuracy: 0.8601, Training F1: 0.8593, Validation Loss: 0.3196, Validation Accuracy: 0.8812, Validation F1: 0.8825\n",
      "Epoch 4/25, Training Loss: 0.3634, Training Accuracy: 0.8680, Training F1: 0.8672, Validation Loss: 0.3310, Validation Accuracy: 0.8790, Validation F1: 0.8786\n",
      "Epoch 5/25, Training Loss: 0.3476, Training Accuracy: 0.8736, Training F1: 0.8728, Validation Loss: 0.3094, Validation Accuracy: 0.8832, Validation F1: 0.8852\n",
      "Epoch 6/25, Training Loss: 0.3330, Training Accuracy: 0.8772, Training F1: 0.8765, Validation Loss: 0.2989, Validation Accuracy: 0.8848, Validation F1: 0.8856\n",
      "Epoch 7/25, Training Loss: 0.3176, Training Accuracy: 0.8844, Training F1: 0.8837, Validation Loss: 0.2870, Validation Accuracy: 0.8927, Validation F1: 0.8936\n",
      "Epoch 8/25, Training Loss: 0.3118, Training Accuracy: 0.8845, Training F1: 0.8838, Validation Loss: 0.2959, Validation Accuracy: 0.8917, Validation F1: 0.8919\n",
      "Epoch 9/25, Training Loss: 0.2986, Training Accuracy: 0.8903, Training F1: 0.8897, Validation Loss: 0.2856, Validation Accuracy: 0.8900, Validation F1: 0.8920\n",
      "Epoch 10/25, Training Loss: 0.2863, Training Accuracy: 0.8946, Training F1: 0.8941, Validation Loss: 0.2815, Validation Accuracy: 0.8963, Validation F1: 0.8967\n",
      "Epoch 11/25, Training Loss: 0.2797, Training Accuracy: 0.8981, Training F1: 0.8975, Validation Loss: 0.2804, Validation Accuracy: 0.8965, Validation F1: 0.8976\n",
      "Epoch 12/25, Training Loss: 0.2721, Training Accuracy: 0.8997, Training F1: 0.8991, Validation Loss: 0.2901, Validation Accuracy: 0.8962, Validation F1: 0.8961\n",
      "Epoch 13/25, Training Loss: 0.2674, Training Accuracy: 0.9006, Training F1: 0.9001, Validation Loss: 0.2770, Validation Accuracy: 0.8988, Validation F1: 0.8997\n",
      "Epoch 14/25, Training Loss: 0.2611, Training Accuracy: 0.9042, Training F1: 0.9037, Validation Loss: 0.2832, Validation Accuracy: 0.8950, Validation F1: 0.8965\n",
      "Epoch 15/25, Training Loss: 0.2559, Training Accuracy: 0.9053, Training F1: 0.9049, Validation Loss: 0.2770, Validation Accuracy: 0.8982, Validation F1: 0.8994\n",
      "Epoch 16/25, Training Loss: 0.2490, Training Accuracy: 0.9076, Training F1: 0.9071, Validation Loss: 0.2699, Validation Accuracy: 0.9005, Validation F1: 0.9024\n",
      "Epoch 17/25, Training Loss: 0.2399, Training Accuracy: 0.9101, Training F1: 0.9096, Validation Loss: 0.2710, Validation Accuracy: 0.9037, Validation F1: 0.9044\n",
      "Epoch 18/25, Training Loss: 0.2371, Training Accuracy: 0.9126, Training F1: 0.9121, Validation Loss: 0.2767, Validation Accuracy: 0.9023, Validation F1: 0.9038\n",
      "Epoch 19/25, Training Loss: 0.2319, Training Accuracy: 0.9139, Training F1: 0.9135, Validation Loss: 0.2734, Validation Accuracy: 0.9022, Validation F1: 0.9031\n",
      "Epoch 20/25, Training Loss: 0.2291, Training Accuracy: 0.9140, Training F1: 0.9136, Validation Loss: 0.2726, Validation Accuracy: 0.9030, Validation F1: 0.9037\n",
      "Epoch 21/25, Training Loss: 0.2256, Training Accuracy: 0.9158, Training F1: 0.9153, Validation Loss: 0.2746, Validation Accuracy: 0.9027, Validation F1: 0.9033\n",
      "Epoch 22/25, Training Loss: 0.2215, Training Accuracy: 0.9170, Training F1: 0.9166, Validation Loss: 0.2833, Validation Accuracy: 0.9027, Validation F1: 0.9028\n",
      "Epoch 23/25, Training Loss: 0.2159, Training Accuracy: 0.9190, Training F1: 0.9186, Validation Loss: 0.2785, Validation Accuracy: 0.9020, Validation F1: 0.9036\n",
      "Epoch 24/25, Training Loss: 0.2158, Training Accuracy: 0.9197, Training F1: 0.9193, Validation Loss: 0.2829, Validation Accuracy: 0.9015, Validation F1: 0.9025\n",
      "Epoch 25/25, Training Loss: 0.2103, Training Accuracy: 0.9215, Training F1: 0.9212, Validation Loss: 0.2683, Validation Accuracy: 0.9048, Validation F1: 0.9062\n",
      "Test Loss: 0.2727, Test Accuracy: 0.9041, Test F1 Score: 0.9039\n",
      "  Training with learning rate: 0.0009\n",
      "Epoch 1/25, Training Loss: 0.5748, Training Accuracy: 0.7978, Training F1: 0.7963, Validation Loss: 0.3892, Validation Accuracy: 0.8563, Validation F1: 0.8553\n",
      "Epoch 2/25, Training Loss: 0.4306, Training Accuracy: 0.8437, Training F1: 0.8426, Validation Loss: 0.3393, Validation Accuracy: 0.8757, Validation F1: 0.8755\n",
      "Epoch 3/25, Training Loss: 0.3917, Training Accuracy: 0.8587, Training F1: 0.8578, Validation Loss: 0.3366, Validation Accuracy: 0.8725, Validation F1: 0.8731\n",
      "Epoch 4/25, Training Loss: 0.3673, Training Accuracy: 0.8672, Training F1: 0.8665, Validation Loss: 0.3282, Validation Accuracy: 0.8758, Validation F1: 0.8735\n",
      "Epoch 5/25, Training Loss: 0.3456, Training Accuracy: 0.8740, Training F1: 0.8734, Validation Loss: 0.3073, Validation Accuracy: 0.8848, Validation F1: 0.8864\n",
      "Epoch 6/25, Training Loss: 0.3343, Training Accuracy: 0.8766, Training F1: 0.8759, Validation Loss: 0.3020, Validation Accuracy: 0.8853, Validation F1: 0.8842\n",
      "Epoch 7/25, Training Loss: 0.3212, Training Accuracy: 0.8828, Training F1: 0.8821, Validation Loss: 0.3022, Validation Accuracy: 0.8870, Validation F1: 0.8862\n",
      "Epoch 8/25, Training Loss: 0.3064, Training Accuracy: 0.8880, Training F1: 0.8874, Validation Loss: 0.2955, Validation Accuracy: 0.8897, Validation F1: 0.8908\n",
      "Epoch 9/25, Training Loss: 0.2998, Training Accuracy: 0.8897, Training F1: 0.8891, Validation Loss: 0.2884, Validation Accuracy: 0.8923, Validation F1: 0.8920\n",
      "Epoch 10/25, Training Loss: 0.2901, Training Accuracy: 0.8925, Training F1: 0.8919, Validation Loss: 0.2785, Validation Accuracy: 0.8987, Validation F1: 0.9003\n",
      "Epoch 11/25, Training Loss: 0.2817, Training Accuracy: 0.8944, Training F1: 0.8938, Validation Loss: 0.2825, Validation Accuracy: 0.8960, Validation F1: 0.8969\n",
      "Epoch 12/25, Training Loss: 0.2737, Training Accuracy: 0.8979, Training F1: 0.8974, Validation Loss: 0.2661, Validation Accuracy: 0.9000, Validation F1: 0.9004\n",
      "Epoch 13/25, Training Loss: 0.2685, Training Accuracy: 0.9014, Training F1: 0.9009, Validation Loss: 0.2799, Validation Accuracy: 0.8977, Validation F1: 0.8988\n",
      "Epoch 14/25, Training Loss: 0.2608, Training Accuracy: 0.9031, Training F1: 0.9025, Validation Loss: 0.2705, Validation Accuracy: 0.8987, Validation F1: 0.9003\n",
      "Epoch 15/25, Training Loss: 0.2529, Training Accuracy: 0.9051, Training F1: 0.9046, Validation Loss: 0.2805, Validation Accuracy: 0.8922, Validation F1: 0.8948\n",
      "Epoch 16/25, Training Loss: 0.2494, Training Accuracy: 0.9078, Training F1: 0.9073, Validation Loss: 0.2695, Validation Accuracy: 0.8980, Validation F1: 0.8998\n",
      "Epoch 17/25, Training Loss: 0.2459, Training Accuracy: 0.9080, Training F1: 0.9075, Validation Loss: 0.2691, Validation Accuracy: 0.9008, Validation F1: 0.9022\n",
      "Epoch 18/25, Training Loss: 0.2409, Training Accuracy: 0.9111, Training F1: 0.9107, Validation Loss: 0.2905, Validation Accuracy: 0.8948, Validation F1: 0.8941\n",
      "Epoch 19/25, Training Loss: 0.2344, Training Accuracy: 0.9113, Training F1: 0.9108, Validation Loss: 0.2657, Validation Accuracy: 0.9003, Validation F1: 0.9016\n",
      "Epoch 20/25, Training Loss: 0.2295, Training Accuracy: 0.9153, Training F1: 0.9148, Validation Loss: 0.2830, Validation Accuracy: 0.8960, Validation F1: 0.8978\n",
      "Epoch 21/25, Training Loss: 0.2235, Training Accuracy: 0.9162, Training F1: 0.9158, Validation Loss: 0.2742, Validation Accuracy: 0.9027, Validation F1: 0.9044\n",
      "Epoch 22/25, Training Loss: 0.2198, Training Accuracy: 0.9176, Training F1: 0.9172, Validation Loss: 0.2772, Validation Accuracy: 0.9037, Validation F1: 0.9037\n",
      "Epoch 23/25, Training Loss: 0.2183, Training Accuracy: 0.9181, Training F1: 0.9176, Validation Loss: 0.2757, Validation Accuracy: 0.8983, Validation F1: 0.8999\n",
      "Epoch 24/25, Training Loss: 0.2133, Training Accuracy: 0.9200, Training F1: 0.9196, Validation Loss: 0.2747, Validation Accuracy: 0.9003, Validation F1: 0.9022\n",
      "Epoch 25/25, Training Loss: 0.2088, Training Accuracy: 0.9230, Training F1: 0.9226, Validation Loss: 0.2807, Validation Accuracy: 0.9007, Validation F1: 0.9030\n",
      "Test Loss: 0.2799, Test Accuracy: 0.8994, Test F1 Score: 0.9000\n",
      "  Training with learning rate: 0.0006\n",
      "Epoch 1/25, Training Loss: 0.5981, Training Accuracy: 0.7904, Training F1: 0.7883, Validation Loss: 0.3923, Validation Accuracy: 0.8563, Validation F1: 0.8588\n",
      "Epoch 2/25, Training Loss: 0.4394, Training Accuracy: 0.8428, Training F1: 0.8416, Validation Loss: 0.3428, Validation Accuracy: 0.8737, Validation F1: 0.8764\n",
      "Epoch 3/25, Training Loss: 0.3941, Training Accuracy: 0.8577, Training F1: 0.8567, Validation Loss: 0.3247, Validation Accuracy: 0.8777, Validation F1: 0.8786\n",
      "Epoch 4/25, Training Loss: 0.3721, Training Accuracy: 0.8653, Training F1: 0.8645, Validation Loss: 0.3095, Validation Accuracy: 0.8868, Validation F1: 0.8888\n",
      "Epoch 5/25, Training Loss: 0.3533, Training Accuracy: 0.8717, Training F1: 0.8709, Validation Loss: 0.3170, Validation Accuracy: 0.8797, Validation F1: 0.8803\n",
      "Epoch 6/25, Training Loss: 0.3370, Training Accuracy: 0.8775, Training F1: 0.8768, Validation Loss: 0.3007, Validation Accuracy: 0.8860, Validation F1: 0.8877\n",
      "Epoch 7/25, Training Loss: 0.3209, Training Accuracy: 0.8832, Training F1: 0.8825, Validation Loss: 0.3010, Validation Accuracy: 0.8865, Validation F1: 0.8852\n",
      "Epoch 8/25, Training Loss: 0.3124, Training Accuracy: 0.8849, Training F1: 0.8841, Validation Loss: 0.2979, Validation Accuracy: 0.8862, Validation F1: 0.8864\n",
      "Epoch 9/25, Training Loss: 0.3041, Training Accuracy: 0.8875, Training F1: 0.8868, Validation Loss: 0.2880, Validation Accuracy: 0.8930, Validation F1: 0.8933\n",
      "Epoch 10/25, Training Loss: 0.2931, Training Accuracy: 0.8925, Training F1: 0.8918, Validation Loss: 0.2888, Validation Accuracy: 0.8933, Validation F1: 0.8934\n",
      "Epoch 11/25, Training Loss: 0.2865, Training Accuracy: 0.8941, Training F1: 0.8934, Validation Loss: 0.2833, Validation Accuracy: 0.8895, Validation F1: 0.8901\n",
      "Epoch 12/25, Training Loss: 0.2803, Training Accuracy: 0.8973, Training F1: 0.8967, Validation Loss: 0.2883, Validation Accuracy: 0.8937, Validation F1: 0.8946\n",
      "Epoch 13/25, Training Loss: 0.2731, Training Accuracy: 0.8993, Training F1: 0.8986, Validation Loss: 0.2790, Validation Accuracy: 0.8957, Validation F1: 0.8971\n",
      "Epoch 14/25, Training Loss: 0.2642, Training Accuracy: 0.9026, Training F1: 0.9021, Validation Loss: 0.2763, Validation Accuracy: 0.8957, Validation F1: 0.8974\n",
      "Epoch 15/25, Training Loss: 0.2590, Training Accuracy: 0.9031, Training F1: 0.9026, Validation Loss: 0.2761, Validation Accuracy: 0.8953, Validation F1: 0.8969\n",
      "Epoch 16/25, Training Loss: 0.2523, Training Accuracy: 0.9070, Training F1: 0.9065, Validation Loss: 0.2925, Validation Accuracy: 0.8900, Validation F1: 0.8923\n",
      "Epoch 17/25, Training Loss: 0.2503, Training Accuracy: 0.9083, Training F1: 0.9078, Validation Loss: 0.2753, Validation Accuracy: 0.8980, Validation F1: 0.8985\n",
      "Epoch 18/25, Training Loss: 0.2428, Training Accuracy: 0.9104, Training F1: 0.9100, Validation Loss: 0.2865, Validation Accuracy: 0.8943, Validation F1: 0.8971\n",
      "Epoch 19/25, Training Loss: 0.2381, Training Accuracy: 0.9114, Training F1: 0.9109, Validation Loss: 0.2918, Validation Accuracy: 0.8910, Validation F1: 0.8902\n",
      "Epoch 20/25, Training Loss: 0.2345, Training Accuracy: 0.9134, Training F1: 0.9130, Validation Loss: 0.2736, Validation Accuracy: 0.9017, Validation F1: 0.9023\n",
      "Epoch 21/25, Training Loss: 0.2295, Training Accuracy: 0.9142, Training F1: 0.9139, Validation Loss: 0.2753, Validation Accuracy: 0.8940, Validation F1: 0.8945\n",
      "Epoch 22/25, Training Loss: 0.2288, Training Accuracy: 0.9163, Training F1: 0.9158, Validation Loss: 0.2698, Validation Accuracy: 0.9043, Validation F1: 0.9056\n",
      "Epoch 23/25, Training Loss: 0.2224, Training Accuracy: 0.9166, Training F1: 0.9162, Validation Loss: 0.2766, Validation Accuracy: 0.9008, Validation F1: 0.9025\n",
      "Epoch 24/25, Training Loss: 0.2178, Training Accuracy: 0.9171, Training F1: 0.9166, Validation Loss: 0.2729, Validation Accuracy: 0.9017, Validation F1: 0.9027\n",
      "Epoch 25/25, Training Loss: 0.2137, Training Accuracy: 0.9195, Training F1: 0.9190, Validation Loss: 0.2776, Validation Accuracy: 0.8967, Validation F1: 0.8990\n",
      "Test Loss: 0.2833, Test Accuracy: 0.8990, Test F1 Score: 0.8996\n",
      "*******************************************************************************************************************************\n",
      "\n",
      "Best Model:\n",
      "Architecture: Arch1\n",
      "Learning Rate: 0.001\n",
      "Test Accuracy: 0.9047\n",
      "Test F1 Score: 0.9050\n",
      "*******************************************************************************************************************************\n",
      "*******************************************************************************************************************************\n",
      "\n",
      "All Model Test Accuracies:\n",
      "Architecture: Arch1, Learning Rate: 0.005, Test Accuracy: 0.9042\n",
      "Architecture: Arch1, Learning Rate: 0.001, Test Accuracy: 0.9047\n",
      "Architecture: Arch1, Learning Rate: 0.0009, Test Accuracy: 0.9030\n",
      "Architecture: Arch1, Learning Rate: 0.0006, Test Accuracy: 0.9037\n",
      "Architecture: Arch2, Learning Rate: 0.005, Test Accuracy: 0.8974\n",
      "Architecture: Arch2, Learning Rate: 0.001, Test Accuracy: 0.9047\n",
      "Architecture: Arch2, Learning Rate: 0.0009, Test Accuracy: 0.8986\n",
      "Architecture: Arch2, Learning Rate: 0.0006, Test Accuracy: 0.8986\n",
      "Architecture: Arch3, Learning Rate: 0.005, Test Accuracy: 0.9030\n",
      "Architecture: Arch3, Learning Rate: 0.001, Test Accuracy: 0.9041\n",
      "Architecture: Arch3, Learning Rate: 0.0009, Test Accuracy: 0.8994\n",
      "Architecture: Arch3, Learning Rate: 0.0006, Test Accuracy: 0.8990\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model_results = []\n",
    "\n",
    "\n",
    "for arch_name, arch_info in architectures.items():\n",
    "    print(f\"\\nTraining models for architecture: {arch_name}\")\n",
    "    for lr in learning_rates:\n",
    "        print(f\"  Training with learning rate: {lr}\")\n",
    "        \n",
    "        # Build the network for the current configuration\n",
    "        layers = []\n",
    "        input_dim = 784  # 28x28 images flattened\n",
    "        for layer_info in arch_info['layers']:\n",
    "            units = layer_info['units']\n",
    "            \n",
    "            # Dense layer\n",
    "            layers.append(DenseLayer(input_dim, units))\n",
    "            layers.append(BatchNormalization(units))\n",
    "            layers.append(ReLU())\n",
    "            \n",
    "            # Apply dropout\n",
    "            layers.append(Dropout(dropout_rate))\n",
    "            \n",
    "            # Update input dimension for next layer\n",
    "            input_dim = units\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(DenseLayer(input_dim, num_classes))\n",
    "        \n",
    "        # Initialize the network\n",
    "        network = NeuralNetwork(layers)\n",
    "        \n",
    "        # Train the model and capture metrics\n",
    "        train_losses, val_losses, train_accuracies, val_accuracies, train_f1s, val_f1s = train(\n",
    "            network, X_train, y_train_one_hot, X_val, y_val_one_hot, num_epochs, batch_size, lr\n",
    "        )\n",
    "        \n",
    "        # Evaluate on the test set\n",
    "        test_loss, test_accuracy, test_f1 = evaluate(network, X_test, y_test_one_hot)\n",
    "        \n",
    "        # Log the results\n",
    "        model_info = {\n",
    "            'architecture': arch_name,\n",
    "            'learning_rate': lr,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'val_accuracies': val_accuracies,\n",
    "            'train_f1s': train_f1s,\n",
    "            'val_f1s': val_f1s,\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'test_f1': test_f1,\n",
    "        }\n",
    "        model_results.append(model_info)\n",
    "\n",
    "        generate_confusion_matrix(network, X_test, y_test, arch_name, lr)\n",
    "        \n",
    "        # Check if this model is the best so far\n",
    "        if test_accuracy > best_test_accuracy:\n",
    "            best_test_accuracy = test_accuracy\n",
    "            best_model = network\n",
    "            best_model_info = model_info\n",
    "        \n",
    "        # Generate and save plots for this model\n",
    "        plot_metrics(\n",
    "            arch_name, lr, train_losses, val_losses,\n",
    "            train_accuracies, val_accuracies, train_f1s, val_f1s\n",
    "        )\n",
    "\n",
    "# Save the best model using pickle\n",
    "with open('best_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "print(\"*******************************************************************************************************************************\")\n",
    "print(\"\\nBest Model:\")\n",
    "print(f\"Architecture: {best_model_info['architecture']}\")\n",
    "print(f\"Learning Rate: {best_model_info['learning_rate']}\")\n",
    "print(f\"Test Accuracy: {best_model_info['test_accuracy']:.4f}\")\n",
    "print(f\"Test F1 Score: {best_model_info['test_f1']:.4f}\")\n",
    "print(\"*******************************************************************************************************************************\")\n",
    "\n",
    "\n",
    "plot_metrics(\n",
    "    best_model_info['architecture'],\n",
    "    best_model_info['learning_rate'],\n",
    "    best_model_info['train_losses'],\n",
    "    best_model_info['val_losses'],\n",
    "    best_model_info['train_accuracies'],\n",
    "    best_model_info['val_accuracies'],\n",
    "    best_model_info['train_f1s'],\n",
    "    best_model_info['val_f1s']\n",
    ")\n",
    "\n",
    "# Generate confusion matrix for the best model with appropriate filename\n",
    "generate_confusion_matrix(best_model, X_test, y_test, best_model_info['architecture'], best_model_info['learning_rate'])\n",
    "\n",
    "\n",
    "# Log all test accuracies\n",
    "print(\"*******************************************************************************************************************************\")\n",
    "print(\"\\nAll Model Test Accuracies:\")\n",
    "for info in model_results:\n",
    "    print(f\"Architecture: {info['architecture']}, Learning Rate: {info['learning_rate']}, \"\n",
    "          f\"Test Accuracy: {info['test_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model(model, filepath):\n",
    "    model_params = {}\n",
    "    for idx, layer in enumerate(model.layers):\n",
    "        if isinstance(layer, DenseLayer):\n",
    "            model_params[f'weight_{idx}'] = layer.W\n",
    "            model_params[f'bias_{idx}'] = layer.b\n",
    "    with open(filepath, 'wb') as file:\n",
    "        pickle.dump(model_params, file)\n",
    "\n",
    "# Assuming you are currently saving the model with `pickle`\n",
    "# Replace this line:\n",
    "# with open('best_model.pkl', 'wb') as f:\n",
    "#     pickle.dump(best_model, f)\n",
    "\n",
    "# With this line:\n",
    "save_model(best_model, 'best_model_str.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
