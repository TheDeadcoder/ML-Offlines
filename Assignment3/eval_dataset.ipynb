{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n",
      "Number of test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "train_data_path = 'data/train.csv'\n",
    "test_data_path = 'data/test.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "print(\"Number of training examples:\", train_data.shape[0])\n",
    "print(\"Number of test examples:\", test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3611, 785)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle_path = 'b1.pkl'\n",
    "with open(pickle_path, 'rb') as file:\n",
    "    test_data = pickle.load(file)\n",
    "\n",
    "inputs, labels = test_data.tensors\n",
    "\n",
    "\n",
    "inputs = inputs.squeeze(1) \n",
    "\n",
    "\n",
    "inputs_np = inputs.numpy()  \n",
    "labels_np = labels.numpy()  \n",
    "\n",
    "num_samples = inputs_np.shape[0]\n",
    "inputs_flat = inputs_np.reshape(num_samples, -1)  \n",
    "pixel_columns = [f'pixel{i}' for i in range(inputs_flat.shape[1])]\n",
    "\n",
    "test_data = pd.DataFrame(inputs_flat, columns=pixel_columns)\n",
    "test_data['label'] = labels_np\n",
    "\n",
    "print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate features and labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop('label', axis=1).values\n",
    "y_train = train_data['label'].values\n",
    "\n",
    "X_test = test_data.drop('label', axis=1).values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize the pixel values (scale to [0, 1])**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = (X_test+1) / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert labels to one-hot encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "num_classes = 10\n",
    "y_train_one_hot = one_hot_encode(y_train, num_classes)\n",
    "y_test_one_hot = one_hot_encode(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the training data into training and validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train_one_hot, y_val_one_hot = train_test_split(\n",
    "    X_train, y_train_one_hot, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Neural Network Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dense Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "\n",
    "### Initialization:\n",
    "- **Weights (`self.w`):**\n",
    "  - Initialized with a scaled random normal distribution.\n",
    "  - `np.sqrt(2.0 / input_dim)`: He initialization, suitable for ReLU activations.\n",
    "\n",
    "- **Biases (`self.b`):**\n",
    "  - Initialized to zeros.\n",
    "\n",
    "- **Gradients (`self.dw`, `self.db`):**\n",
    "  - Placeholders for gradients computed during backpropagation.\n",
    "\n",
    "### Adam Parameters:\n",
    "- **First Moment Vectors (`self.m_w`, `self.m_b`):**\n",
    "  - Moving averages of the gradients.\n",
    "\n",
    "- **Second Moment Vectors (`self.v_w`, `self.v_b`):**\n",
    "  - Moving averages of the squared gradients.\n",
    "\n",
    "### Cache (`self.input`):\n",
    "- Stores the input to the layer for use during backpropagation.\n",
    "\n",
    "## Theory:\n",
    "\n",
    "### Dense (Fully Connected) Layer:\n",
    "- Each neuron receives input from all neurons in the previous layer.\n",
    "- Computes `output = X * W + b`.\n",
    "\n",
    "### He Initialization:\n",
    "- Addresses the problem of vanishing/exploding gradients.\n",
    "- Suitable for layers followed by ReLU activation.\n",
    "\n",
    "\n",
    "### **Forward Pass Explanation:**\n",
    "  - Computes the linear transformation.\n",
    "  - Stores the input \\( X \\) for use in backpropagation.\n",
    "\n",
    "**Mathematical Operation:**\n",
    "  -  `output = X * W + b`.\n",
    "\n",
    "### **Backward Pass Explanation**:\n",
    "\n",
    "- **Gradients w.r.t Weights (`self.dw`):**\n",
    "  - Computed as the dot product of the transposed input and `grad_output`.\n",
    "\n",
    "- **Gradients w.r.t Biases (`self.db`):**\n",
    "  - Sum of `grad_output` along the batch dimension.\n",
    "\n",
    "- **Gradient w.r.t Input (`grad_input`):**\n",
    "  - Propagated backward to previous layers.\n",
    "  - Computed as the dot product of `grad_output` and transposed weights.\n",
    "\n",
    "## Theory:\n",
    "- Backpropagation involves computing the gradients of the loss with respect to each parameter.\n",
    "- Uses the chain rule from calculus.\n",
    "\n",
    "**Explanation for updating parameters:**\n",
    "  - **Computing Moving Averages:**\n",
    "    - `self.m_w`, `self.m_b`: Exponential moving averages of gradients (first moment).\n",
    "    - `self.v_w`, `self.v_b`: Exponential moving averages of squared gradients (second moment).\n",
    "  \n",
    "  - **Bias Correction:**\n",
    "    - Adjusts the moments to account for their initialization at zero.\n",
    "    - `m_w_hat`, `v_w_hat`: Corrected moments.\n",
    "\n",
    "  - **Parameter Updates:**\n",
    "    - Updates weights and biases using the Adam update rule.\n",
    "\n",
    "# Parameter Update Theory:\n",
    "\n",
    "- **Adam Optimizer:**\n",
    "  - Combines the advantages of both AdaGrad and RMSProp.\n",
    "\n",
    "  - **First Moment Estimate (Mean):**\n",
    "    - \\( m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\)\n",
    "\n",
    "  - **Second Moment Estimate (Variance):**\n",
    "    - \\( v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\)\n",
    "\n",
    "  - **Bias Correction:**\n",
    "    - Adjusts for the initial bias towards zero moments.\n",
    "\n",
    "  - **Parameter Update:**\n",
    "    - \\( \\theta_t = \\theta_{t-1} - \\alpha \\frac{m_t}{\\sqrt{v_t} + \\epsilon} \\)\n",
    "\n",
    "- **Parameters:**\n",
    "  - `beta1` and `beta2`:\n",
    "    - Hyperparameters controlling the decay rates of the moving averages.\n",
    "    - Commonly set to `beta1=0.9` and `beta2=0.999`.\n",
    "\n",
    "  - `epsilon`:\n",
    "    - Small constant to prevent division by zero.\n",
    "\n",
    "  - `t`:\n",
    "    - Timestep, incremented after each parameter update.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialize weights and biases\n",
    "        self.W = np.random.randn(input_dim, output_dim) * np.sqrt(2.0 / input_dim)\n",
    "        self.b = np.zeros((1, output_dim))\n",
    "        \n",
    "        # Gradients\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "        # Adam parameters\n",
    "        self.m_W = np.zeros_like(self.W)\n",
    "        self.v_W = np.zeros_like(self.W)\n",
    "        self.m_b = np.zeros_like(self.b)\n",
    "        self.v_b = np.zeros_like(self.b)\n",
    "        \n",
    "        # Cache for backpropagation\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        output = np.dot(X, self.W) + self.b\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # Compute gradients\n",
    "        self.dW = np.dot(self.input.T, grad_output)\n",
    "        self.db = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        grad_input = np.dot(grad_output, self.W.T)\n",
    "        return grad_input\n",
    "    \n",
    "    def update_params(self, learning_rate, beta1, beta2, epsilon, t):\n",
    "        # Update weights and biases using Adam optimizer\n",
    "        self.m_W = beta1 * self.m_W + (1 - beta1) * self.dW\n",
    "        self.v_W = beta2 * self.v_W + (1 - beta2) * (self.dW ** 2)\n",
    "        m_W_hat = self.m_W / (1 - beta1 ** t)\n",
    "        v_W_hat = self.v_W / (1 - beta2 ** t)\n",
    "        self.W -= learning_rate * m_W_hat / (np.sqrt(v_W_hat) + epsilon)\n",
    "        \n",
    "        self.m_b = beta1 * self.m_b + (1 - beta1) * self.db\n",
    "        self.v_b = beta2 * self.v_b + (1 - beta2) * (self.db ** 2)\n",
    "        m_b_hat = self.m_b / (1 - beta1 ** t)\n",
    "        v_b_hat = self.v_b / (1 - beta2 ** t)\n",
    "        self.b -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Scale Parameter (`self.gamma`):**\n",
    "  - Learns how much to scale the normalized output.\n",
    "\n",
    "- **Shift Parameter (`self.beta`):**\n",
    "  - Learns how much to shift the normalized output.\n",
    "\n",
    "- **`epsilon`:**\n",
    "  - Small constant to prevent division by zero.\n",
    "\n",
    "- **`momentum`:**\n",
    "  - Controls the updating of running estimates.\n",
    "\n",
    "- **Running Estimates:**\n",
    "  - `self.running_mean` and `self.running_var`:\n",
    "    - Used during inference to normalize data with global statistics.\n",
    "\n",
    "- **Gradients and Caches:**\n",
    "  - **Gradients (`self.dgamma`, `self.dbeta`):**\n",
    "    - For updating `gamma` and `beta`.\n",
    "  \n",
    "  - **Caches:**\n",
    "    - Stores intermediate variables needed for backpropagation.\n",
    "\n",
    "- **Adam Parameters:**\n",
    "  - For optimizing `gamma` and `beta`.\n",
    "\n",
    "\n",
    "## Batch Normalization:\n",
    "- Normalizes the input of each mini-batch to have zero mean and unit variance.\n",
    "- Helps in stabilizing the learning process and allows for higher learning rates.\n",
    "- Learns `gamma` and `beta` to restore the representation power.\n",
    "\n",
    "### Forward Pass:\n",
    "**Explanation:**\n",
    "  - **Training Mode:**\n",
    "    - **Compute Batch Statistics:**\n",
    "      - Mean (`self.batch_mean`) and variance (`self.batch_var`) over the mini-batch.\n",
    "      \n",
    "    - **Normalize the Batch:**\n",
    "      - Subtract mean and divide by standard deviation (`self.std_inv`).\n",
    "      \n",
    "    - **Scale and Shift:**\n",
    "      - Apply learned `gamma` and `beta`.\n",
    "      \n",
    "    - **Update Running Estimates:**\n",
    "      - For use during inference.\n",
    "\n",
    "  - **Inference Mode:**\n",
    "    - Uses the running mean and variance to normalize data.\n",
    "\n",
    "**Theory:**\n",
    "  - **Normalization Formula:**\n",
    "    - \\( \\hat{X} = \\frac{X - \\mu_{\\text{batch}}}{\\sqrt{\\sigma_{\\text{batch}}^2 + \\epsilon}} \\)\n",
    "\n",
    "  - **Scale and Shift:**\n",
    "    - \\( Y = \\gamma \\hat{X} + \\beta \\)\n",
    "\n",
    "  - **Running Estimates:**\n",
    "    - \\( \\text{running\\_mean} = \\text{momentum} \\times \\text{running\\_mean} + (1 - \\text{momentum}) \\times \\mu_{\\text{batch}} \\)\n",
    "    - Similar for running variance.\n",
    "\n",
    "## backward Pass\n",
    "- **Explanation:**\n",
    "  - **Gradients w.r.t Parameters:**\n",
    "    - `self.dgamma`:\n",
    "      - Gradient of the loss with respect to `gamma`.\n",
    "    - `self.dbeta`:\n",
    "      - Gradient of the loss with respect to `beta`.\n",
    "\n",
    "  - **Gradient w.r.t Input (`dX`):**\n",
    "    - Computed using chain rule and intermediate variables.\n",
    "    - Ensures that gradients flow correctly through the normalization step.\n",
    "\n",
    "- **Theory:**\n",
    "  - **Backpropagation Through Batch Norm:**\n",
    "    - Involves computing derivatives through the normalization and scaling steps.\n",
    "    - Requires careful calculation to maintain numerical stability.\n",
    "\n",
    "\n",
    "## update_params\n",
    "- Similar to the Dense Layer, but updates `gamma` and `beta` parameters.\n",
    "- Uses the Adam optimization algorithm for parameter updates.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, input_dim, epsilon=1e-5, momentum=0.9):\n",
    "        self.gamma = np.ones((1, input_dim))\n",
    "        self.beta = np.zeros((1, input_dim))\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.running_mean = np.zeros((1, input_dim))\n",
    "        self.running_var = np.zeros((1, input_dim))\n",
    "        \n",
    "        # Gradients\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "        \n",
    "        # Cache for backpropagation\n",
    "        self.X_centered = None\n",
    "        self.std_inv = None\n",
    "        self.batch_mean = None\n",
    "        self.batch_var = None\n",
    "        \n",
    "        # Adam parameters\n",
    "        self.m_gamma = np.zeros_like(self.gamma)\n",
    "        self.v_gamma = np.zeros_like(self.gamma)\n",
    "        self.m_beta = np.zeros_like(self.beta)\n",
    "        self.v_beta = np.zeros_like(self.beta)\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            self.batch_mean = np.mean(X, axis=0, keepdims=True)\n",
    "            self.batch_var = np.var(X, axis=0, keepdims=True)\n",
    "            \n",
    "            self.X_centered = X - self.batch_mean\n",
    "            self.std_inv = 1.0 / np.sqrt(self.batch_var + self.epsilon)\n",
    "            \n",
    "            X_norm = self.X_centered * self.std_inv\n",
    "            out = self.gamma * X_norm + self.beta\n",
    "            \n",
    "            # Update running estimates\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.batch_var\n",
    "        else:\n",
    "            # Use running mean and var during inference\n",
    "            X_centered = X - self.running_mean\n",
    "            std_inv = 1.0 / np.sqrt(self.running_var + self.epsilon)\n",
    "            X_norm = X_centered * std_inv\n",
    "            out = self.gamma * X_norm + self.beta\n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        N, D = grad_output.shape\n",
    "        \n",
    "        # Step-wise computation for gradients\n",
    "        X_norm = self.X_centered * self.std_inv\n",
    "        dX_norm = grad_output * self.gamma\n",
    "        \n",
    "        dvar = np.sum(dX_norm * self.X_centered * -0.5 * self.std_inv ** 3, axis=0)\n",
    "        dmean = np.sum(dX_norm * -self.std_inv, axis=0) + dvar * np.mean(-2.0 * self.X_centered, axis=0)\n",
    "        \n",
    "        dX = (dX_norm * self.std_inv) + (dvar * 2.0 * self.X_centered / N) + (dmean / N)\n",
    "        self.dgamma = np.sum(grad_output * X_norm, axis=0, keepdims=True)\n",
    "        self.dbeta = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def update_params(self, learning_rate, beta1, beta2, epsilon, t):\n",
    "        # Update gamma and beta using Adam optimizer\n",
    "        self.m_gamma = beta1 * self.m_gamma + (1 - beta1) * self.dgamma\n",
    "        self.v_gamma = beta2 * self.v_gamma + (1 - beta2) * (self.dgamma ** 2)\n",
    "        m_gamma_hat = self.m_gamma / (1 - beta1 ** t)\n",
    "        v_gamma_hat = self.v_gamma / (1 - beta2 ** t)\n",
    "        self.gamma -= learning_rate * m_gamma_hat / (np.sqrt(v_gamma_hat) + epsilon)\n",
    "        \n",
    "        self.m_beta = beta1 * self.m_beta + (1 - beta1) * self.dbeta\n",
    "        self.v_beta = beta2 * self.v_beta + (1 - beta2) * (self.dbeta ** 2)\n",
    "        m_beta_hat = self.m_beta / (1 - beta1 ** t)\n",
    "        v_beta_hat = self.v_beta / (1 - beta2 ** t)\n",
    "        self.beta -= learning_rate * m_beta_hat / (np.sqrt(v_beta_hat) + epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ReLU Activation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Activation\n",
    "- **Forward Pass:**\n",
    "  - `np.maximum(0, X)`:\n",
    "    - Applies the ReLU function element-wise.\n",
    "    - Sets all negative inputs to zero.\n",
    "\n",
    "- **Backward Pass:**\n",
    "  - **Gradient w.r.t Input (`grad_input`):**\n",
    "    - For inputs where \\( X > 0 \\), gradient is unchanged.\n",
    "    - For inputs where \\( X \\leq 0 \\), gradient is zero.\n",
    "\n",
    "- **Theory:**\n",
    "  - **ReLU Function:**\n",
    "    - \\( \\text{ReLU}(x) = \\max(0, x) \\)\n",
    "\n",
    "  - **Derivative:**\n",
    "    - \\( \\frac{d}{dx} \\text{ReLU}(x) = \\begin{cases} 1, & x > 0 \\\\ 0, & x \\leq 0 \\end{cases} \\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_input = grad_output.copy()\n",
    "        grad_input[self.input <= 0] = 0\n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "- Regularization technique to prevent overfitting.\n",
    "- During training, randomly drops units along with their connections.\n",
    "- **Scaling**:\n",
    "- - By scaling the activations during training, there's no need to adjust them during inference.\n",
    "- **Parameters:**\n",
    "  - `dropout_rate`:\n",
    "    - Probability of dropping a neuron (setting its output to zero).\n",
    "\n",
    "- **Forward Pass:**\n",
    "  - **Training Mode:**\n",
    "    - **Creating the Mask:**\n",
    "      - Randomly sets neurons to zero with probability `dropout_rate`.\n",
    "      - **Scaling:** Divides by \\( (1 - \\text{dropout_rate}) \\) to keep the expected value of the activations the same.\n",
    "      \n",
    "    - **Applying the Mask:**\n",
    "      - Multiplies the input \\( X \\) by the mask.\n",
    "\n",
    "  - **Inference Mode:**\n",
    "    - No dropout is applied during testing.\n",
    "\n",
    "- **Backward Pass:**\n",
    "  - **Gradient w.r.t Input (`grad_output * self.mask`):**\n",
    "    - Propagates gradients only through the neurons that were not dropped during the forward pass.\n",
    "- **Scaling**\n",
    "  By scaling the activations during training, there's no need to adjust them during inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            self.mask = (np.random.rand(*X.shape) > self.dropout_rate) / (1.0 - self.dropout_rate)\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X  # During inference, no dropout applied\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * self.mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax and Cross-Entropy Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_loss(logits, labels):\n",
    "    # logits: output of the network before softmax, shape (N, C)\n",
    "    # labels: one-hot encoded true labels, shape (N, C)\n",
    "    # returns loss and gradient with respect to logits\n",
    "    \n",
    "    # Compute softmax probabilities\n",
    "    exps = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    softmax_probs = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute loss\n",
    "    N = logits.shape[0]\n",
    "    loss = -np.sum(labels * np.log(softmax_probs + 1e-15)) / N\n",
    "    \n",
    "    # Compute gradient\n",
    "    grad_logits = (softmax_probs - labels) / N\n",
    "    return loss, grad_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization:\n",
    "- self.layers:\n",
    "  - List of layers constituting the network.\n",
    "- self.t:\n",
    "  - Global timestep used for Adam optimizer.\n",
    "\n",
    "### Forward Pass:\n",
    "- Iterates through each layer, passing the output of one as the input to the next.\n",
    "- **Conditional Forward Pass:**: For layers that behave differently during training and inference (Dropout, BatchNormalization), passes the training flag.\n",
    "\n",
    "### Backward Pass:\n",
    "- Iterates through layers in reverse order.\n",
    "- Passes the gradient from one layer to the previous.\n",
    "\n",
    "### Parameter Update:\n",
    "- Updates parameters of each layer that has an `update_params` method.\n",
    "- Increments the global timestep `self.t` after each update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.t = 1  # Timestep for Adam optimizer\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, (Dropout, BatchNormalization)):\n",
    "                X = layer.forward(X, training=training)\n",
    "            else:\n",
    "                X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(grad_output)\n",
    "    \n",
    "    def update_params(self, learning_rate, beta1, beta2, epsilon):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'update_params'):\n",
    "                layer.update_params(learning_rate, beta1, beta2, epsilon, self.t)\n",
    "        self.t += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Structure:\n",
    "- Input Layer: Receives flattened 28x28 images `(input_dim = 784)`.\n",
    "  \n",
    "### First Hidden Layer:\n",
    "- Dense Layer: 512 units. \n",
    "- Batch Normalization.\n",
    "- ReLU Activation.\n",
    "- Dropout with 30% rate.\n",
    "\n",
    "### Second Hidden Layer:\n",
    "- Dense Layer: 256 units. \n",
    "- Batch Normalization.\n",
    "- ReLU Activation.\n",
    "- Dropout with 30% rate.\n",
    "\n",
    "### Third Hidden Layer:\n",
    "- Dense Layer: 128 units. \n",
    "- Batch Normalization.\n",
    "- ReLU Activation.\n",
    "- Dropout with 30% rate.\n",
    "\n",
    "### Output Layer\n",
    "- Dense Layer: 10 units (one for each class).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = {\n",
    "    'Arch1': {\n",
    "        'layers': [\n",
    "            {'units': 512},\n",
    "            {'units': 256},\n",
    "            {'units': 128},\n",
    "        ],\n",
    "    },\n",
    "    'Arch2': {\n",
    "        'layers': [\n",
    "            {'units': 1024},\n",
    "            {'units': 512},\n",
    "            {'units': 256},\n",
    "            {'units': 128},\n",
    "        ],\n",
    "    },\n",
    "    'Arch3': {\n",
    "        'layers': [\n",
    "            {'units': 256},\n",
    "            {'units': 128},\n",
    "        ],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting Hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Learning Rate (`learning_rate`):**\n",
    "  - Step size for parameter updates.\n",
    "\n",
    "- **Adam Hyperparameters:**\n",
    "  - `beta1`: Decay rate for first moment estimates.\n",
    "  - `beta2`: Decay rate for second moment estimates.\n",
    "  - `epsilon`: Small constant to prevent division by zero.\n",
    "\n",
    "- **Training Configuration:**\n",
    "  - `num_epochs`: Number of times the entire training dataset is passed through the network.\n",
    "  - `batch_size`: Number of samples processed before updating the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8\n",
    "num_epochs = 25  \n",
    "batch_size = 64\n",
    "dropout_rate = 0.3 \n",
    "learning_rates = [0.005, 0.001, 0.0009, 0.0006]\n",
    "\n",
    "best_test_accuracy = 0.0\n",
    "best_model = None\n",
    "best_model_info = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, X_train, y_train_one_hot, X_val, y_val_one_hot, num_epochs, batch_size, learning_rate):\n",
    "    num_train_samples = X_train.shape[0]\n",
    "    num_batches = int(np.ceil(num_train_samples / batch_size))\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    train_f1s = []\n",
    "    val_f1s = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the training data\n",
    "        indices = np.arange(num_train_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_train_shuffled = X_train[indices]\n",
    "        y_train_shuffled = y_train_one_hot[indices]\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        all_predictions = []\n",
    "        all_true_labels = []\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, num_train_samples)\n",
    "            X_batch = X_train_shuffled[start:end]\n",
    "            y_batch = y_train_shuffled[start:end]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = network.forward(X_batch, training=True)\n",
    "            \n",
    "            # Compute loss and gradient\n",
    "            loss, grad_logits = softmax_cross_entropy_loss(logits, y_batch)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            # Backward pass\n",
    "            network.backward(grad_logits)\n",
    "            \n",
    "            # Update parameters\n",
    "            network.update_params(learning_rate, beta1, beta2, epsilon)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            predictions = np.argmax(logits, axis=1)\n",
    "            true_labels = np.argmax(y_batch, axis=1)\n",
    "            correct_predictions += np.sum(predictions == true_labels)\n",
    "            total_samples += predictions.shape[0]\n",
    "            \n",
    "            all_predictions.extend(predictions)\n",
    "            all_true_labels.extend(true_labels)\n",
    "        \n",
    "        # Compute average training loss and accuracy\n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "        train_accuracy = correct_predictions / total_samples\n",
    "        train_f1 = f1_score(all_true_labels, all_predictions, average='macro')\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        train_f1s.append(train_f1)\n",
    "        \n",
    "        # Validation\n",
    "        logits_val = network.forward(X_val, training=False)\n",
    "        val_loss, _ = softmax_cross_entropy_loss(logits_val, y_val_one_hot)\n",
    "        \n",
    "        predictions_val = np.argmax(logits_val, axis=1)\n",
    "        true_labels_val = np.argmax(y_val_one_hot, axis=1)\n",
    "        val_accuracy = np.mean(predictions_val == true_labels_val)\n",
    "        val_f1 = f1_score(true_labels_val, predictions_val, average='macro')\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_f1s.append(val_f1)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Training Accuracy: {train_accuracy:.4f}, Training F1: {train_f1:.4f}, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, \"\n",
    "              f\"Validation F1: {val_f1:.4f}\")\n",
    "        \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies, train_f1s, val_f1s\n",
    "\n",
    "def plot_metrics(arch_name, learning_rate, train_losses, val_losses,\n",
    "                 train_accuracies, val_accuracies, train_f1s, val_f1s):\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "    plt.title(f'Loss vs. Epochs ({arch_name}, Learning Rate: {learning_rate})')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'loss_{arch_name}_lr_{learning_rate}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, 'r-', label='Validation Accuracy')\n",
    "    plt.title(f'Accuracy vs. Epochs ({arch_name}, Learning Rate: {learning_rate})')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'accuracy_{arch_name}_lr_{learning_rate}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot F1 Score\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_f1s, 'b-', label='Training F1 Score')\n",
    "    plt.plot(epochs, val_f1s, 'r-', label='Validation F1 Score')\n",
    "    plt.title(f'F1 Score vs. Epochs ({arch_name}, Learning Rate: {learning_rate})')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'f1_{arch_name}_lr_{learning_rate}.png')\n",
    "    plt.close()\n",
    "\n",
    "# def generate_confusion_matrix(network, X_test, y_test):\n",
    "#     logits = network.forward(X_test, training=False)\n",
    "#     predictions = np.argmax(logits, axis=1)\n",
    "    \n",
    "#     cm = confusion_matrix(y_test, predictions)\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "#                 xticklabels=range(10), yticklabels=range(10))\n",
    "#     plt.title('Confusion Matrix')\n",
    "#     plt.xlabel('Predicted Label')\n",
    "#     plt.ylabel('True Label')\n",
    "#     plt.savefig('confusion_matrix.png')\n",
    "#     plt.close()\n",
    "\n",
    "def generate_confusion_matrix(network, X_test, y_test, arch_name, learning_rate):\n",
    "    logits = network.forward(X_test, training=False)\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=range(10), yticklabels=range(10))\n",
    "    plt.title(f'Confusion Matrix - Architecture: {arch_name}, Learning Rate: {learning_rate}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    \n",
    "    # Save the confusion matrix with architecture and learning rate in the filename\n",
    "    filename = f'confusion_matrix_{arch_name}_lr_{learning_rate}.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(network, X_test, y_test_one_hot):\n",
    "    logits = network.forward(X_test, training=False)\n",
    "    test_loss, _ = softmax_cross_entropy_loss(logits, y_test_one_hot)\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    true_labels = np.argmax(y_test_one_hot, axis=1)\n",
    "    test_accuracy = np.mean(predictions == true_labels)\n",
    "    test_f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")   \n",
    "    return test_loss, test_accuracy, test_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models for architecture: Arch1\n",
      "  Training with learning rate: 0.005\n",
      "Epoch 1/25, Training Loss: 0.5565, Training Accuracy: 0.8008, Training F1: 0.7989, Validation Loss: 0.3970, Validation Accuracy: 0.8510, Validation F1: 0.8485\n",
      "Epoch 2/25, Training Loss: 0.4408, Training Accuracy: 0.8421, Training F1: 0.8410, Validation Loss: 0.3622, Validation Accuracy: 0.8670, Validation F1: 0.8634\n",
      "Epoch 3/25, Training Loss: 0.3984, Training Accuracy: 0.8577, Training F1: 0.8568, Validation Loss: 0.3399, Validation Accuracy: 0.8723, Validation F1: 0.8719\n",
      "Epoch 4/25, Training Loss: 0.3752, Training Accuracy: 0.8645, Training F1: 0.8635, Validation Loss: 0.3305, Validation Accuracy: 0.8748, Validation F1: 0.8781\n",
      "Epoch 5/25, Training Loss: 0.3566, Training Accuracy: 0.8719, Training F1: 0.8710, Validation Loss: 0.2986, Validation Accuracy: 0.8847, Validation F1: 0.8854\n",
      "Epoch 6/25, Training Loss: 0.3422, Training Accuracy: 0.8755, Training F1: 0.8747, Validation Loss: 0.3164, Validation Accuracy: 0.8837, Validation F1: 0.8833\n",
      "Epoch 7/25, Training Loss: 0.3307, Training Accuracy: 0.8799, Training F1: 0.8791, Validation Loss: 0.2940, Validation Accuracy: 0.8908, Validation F1: 0.8927\n",
      "Epoch 8/25, Training Loss: 0.3188, Training Accuracy: 0.8835, Training F1: 0.8828, Validation Loss: 0.2949, Validation Accuracy: 0.8930, Validation F1: 0.8940\n",
      "Epoch 9/25, Training Loss: 0.3073, Training Accuracy: 0.8885, Training F1: 0.8877, Validation Loss: 0.2954, Validation Accuracy: 0.8920, Validation F1: 0.8927\n",
      "Epoch 10/25, Training Loss: 0.2962, Training Accuracy: 0.8927, Training F1: 0.8921, Validation Loss: 0.2847, Validation Accuracy: 0.8988, Validation F1: 0.9004\n",
      "Epoch 11/25, Training Loss: 0.2864, Training Accuracy: 0.8937, Training F1: 0.8930, Validation Loss: 0.2833, Validation Accuracy: 0.8947, Validation F1: 0.8938\n",
      "Epoch 12/25, Training Loss: 0.2802, Training Accuracy: 0.8988, Training F1: 0.8982, Validation Loss: 0.2783, Validation Accuracy: 0.9005, Validation F1: 0.9012\n",
      "Epoch 13/25, Training Loss: 0.2727, Training Accuracy: 0.9000, Training F1: 0.8994, Validation Loss: 0.2892, Validation Accuracy: 0.8935, Validation F1: 0.8943\n",
      "Epoch 14/25, Training Loss: 0.2658, Training Accuracy: 0.9033, Training F1: 0.9027, Validation Loss: 0.2753, Validation Accuracy: 0.8995, Validation F1: 0.9008\n",
      "Epoch 15/25, Training Loss: 0.2612, Training Accuracy: 0.9053, Training F1: 0.9048, Validation Loss: 0.2765, Validation Accuracy: 0.8992, Validation F1: 0.8992\n",
      "Epoch 16/25, Training Loss: 0.2505, Training Accuracy: 0.9081, Training F1: 0.9076, Validation Loss: 0.2798, Validation Accuracy: 0.8948, Validation F1: 0.8961\n",
      "Epoch 17/25, Training Loss: 0.2453, Training Accuracy: 0.9086, Training F1: 0.9082, Validation Loss: 0.2920, Validation Accuracy: 0.8973, Validation F1: 0.8964\n",
      "Epoch 18/25, Training Loss: 0.2429, Training Accuracy: 0.9122, Training F1: 0.9117, Validation Loss: 0.2780, Validation Accuracy: 0.8987, Validation F1: 0.8987\n",
      "Epoch 19/25, Training Loss: 0.2349, Training Accuracy: 0.9134, Training F1: 0.9129, Validation Loss: 0.2803, Validation Accuracy: 0.8977, Validation F1: 0.8994\n",
      "Epoch 20/25, Training Loss: 0.2298, Training Accuracy: 0.9147, Training F1: 0.9143, Validation Loss: 0.2785, Validation Accuracy: 0.9010, Validation F1: 0.9022\n",
      "Epoch 21/25, Training Loss: 0.2255, Training Accuracy: 0.9172, Training F1: 0.9167, Validation Loss: 0.2909, Validation Accuracy: 0.8958, Validation F1: 0.8975\n",
      "Epoch 22/25, Training Loss: 0.2199, Training Accuracy: 0.9189, Training F1: 0.9184, Validation Loss: 0.2741, Validation Accuracy: 0.9007, Validation F1: 0.9013\n",
      "Epoch 23/25, Training Loss: 0.2169, Training Accuracy: 0.9202, Training F1: 0.9198, Validation Loss: 0.2797, Validation Accuracy: 0.9012, Validation F1: 0.9035\n",
      "Epoch 24/25, Training Loss: 0.2096, Training Accuracy: 0.9215, Training F1: 0.9211, Validation Loss: 0.2784, Validation Accuracy: 0.8980, Validation F1: 0.9000\n",
      "Epoch 25/25, Training Loss: 0.2087, Training Accuracy: 0.9226, Training F1: 0.9223, Validation Loss: 0.2789, Validation Accuracy: 0.9052, Validation F1: 0.9066\n",
      "Test Loss: 0.2767, Test Accuracy: 0.9042, Test F1 Score: 0.9042\n",
      "  Training with learning rate: 0.001\n",
      "Epoch 1/25, Training Loss: 0.5835, Training Accuracy: 0.7938, Training F1: 0.7922, Validation Loss: 0.3601, Validation Accuracy: 0.8662, Validation F1: 0.8662\n",
      "Epoch 2/25, Training Loss: 0.4285, Training Accuracy: 0.8452, Training F1: 0.8441, Validation Loss: 0.3354, Validation Accuracy: 0.8770, Validation F1: 0.8790\n",
      "Epoch 3/25, Training Loss: 0.3897, Training Accuracy: 0.8584, Training F1: 0.8575, Validation Loss: 0.3258, Validation Accuracy: 0.8767, Validation F1: 0.8777\n",
      "Epoch 4/25, Training Loss: 0.3674, Training Accuracy: 0.8662, Training F1: 0.8654, Validation Loss: 0.3411, Validation Accuracy: 0.8653, Validation F1: 0.8661\n",
      "Epoch 5/25, Training Loss: 0.3491, Training Accuracy: 0.8741, Training F1: 0.8732, Validation Loss: 0.2878, Validation Accuracy: 0.8903, Validation F1: 0.8918\n",
      "Epoch 6/25, Training Loss: 0.3320, Training Accuracy: 0.8789, Training F1: 0.8782, Validation Loss: 0.3150, Validation Accuracy: 0.8788, Validation F1: 0.8786\n",
      "Epoch 7/25, Training Loss: 0.3171, Training Accuracy: 0.8846, Training F1: 0.8839, Validation Loss: 0.2859, Validation Accuracy: 0.8888, Validation F1: 0.8907\n",
      "Epoch 8/25, Training Loss: 0.3056, Training Accuracy: 0.8880, Training F1: 0.8874, Validation Loss: 0.2767, Validation Accuracy: 0.8982, Validation F1: 0.8993\n",
      "Epoch 9/25, Training Loss: 0.2949, Training Accuracy: 0.8919, Training F1: 0.8912, Validation Loss: 0.2752, Validation Accuracy: 0.8978, Validation F1: 0.8989\n",
      "Epoch 10/25, Training Loss: 0.2867, Training Accuracy: 0.8939, Training F1: 0.8933, Validation Loss: 0.2913, Validation Accuracy: 0.8903, Validation F1: 0.8901\n",
      "Epoch 11/25, Training Loss: 0.2814, Training Accuracy: 0.8959, Training F1: 0.8953, Validation Loss: 0.2784, Validation Accuracy: 0.8930, Validation F1: 0.8945\n",
      "Epoch 12/25, Training Loss: 0.2700, Training Accuracy: 0.8994, Training F1: 0.8988, Validation Loss: 0.2849, Validation Accuracy: 0.8943, Validation F1: 0.8964\n",
      "Epoch 13/25, Training Loss: 0.2615, Training Accuracy: 0.9038, Training F1: 0.9033, Validation Loss: 0.2723, Validation Accuracy: 0.9013, Validation F1: 0.9029\n",
      "Epoch 14/25, Training Loss: 0.2562, Training Accuracy: 0.9045, Training F1: 0.9039, Validation Loss: 0.2903, Validation Accuracy: 0.8958, Validation F1: 0.8954\n",
      "Epoch 15/25, Training Loss: 0.2468, Training Accuracy: 0.9097, Training F1: 0.9092, Validation Loss: 0.2733, Validation Accuracy: 0.8965, Validation F1: 0.8983\n",
      "Epoch 16/25, Training Loss: 0.2382, Training Accuracy: 0.9121, Training F1: 0.9117, Validation Loss: 0.2725, Validation Accuracy: 0.8975, Validation F1: 0.8977\n",
      "Epoch 17/25, Training Loss: 0.2373, Training Accuracy: 0.9121, Training F1: 0.9116, Validation Loss: 0.2724, Validation Accuracy: 0.9008, Validation F1: 0.9007\n",
      "Epoch 18/25, Training Loss: 0.2293, Training Accuracy: 0.9147, Training F1: 0.9142, Validation Loss: 0.2737, Validation Accuracy: 0.8990, Validation F1: 0.9005\n",
      "Epoch 19/25, Training Loss: 0.2246, Training Accuracy: 0.9169, Training F1: 0.9165, Validation Loss: 0.2789, Validation Accuracy: 0.8962, Validation F1: 0.8974\n",
      "Epoch 20/25, Training Loss: 0.2202, Training Accuracy: 0.9171, Training F1: 0.9167, Validation Loss: 0.2711, Validation Accuracy: 0.9080, Validation F1: 0.9092\n",
      "Epoch 21/25, Training Loss: 0.2135, Training Accuracy: 0.9212, Training F1: 0.9209, Validation Loss: 0.2716, Validation Accuracy: 0.9017, Validation F1: 0.9031\n",
      "Epoch 22/25, Training Loss: 0.2086, Training Accuracy: 0.9235, Training F1: 0.9231, Validation Loss: 0.2674, Validation Accuracy: 0.9090, Validation F1: 0.9098\n",
      "Epoch 23/25, Training Loss: 0.2076, Training Accuracy: 0.9249, Training F1: 0.9245, Validation Loss: 0.2676, Validation Accuracy: 0.9030, Validation F1: 0.9044\n",
      "Epoch 24/25, Training Loss: 0.2050, Training Accuracy: 0.9244, Training F1: 0.9240, Validation Loss: 0.2762, Validation Accuracy: 0.9042, Validation F1: 0.9058\n",
      "Epoch 25/25, Training Loss: 0.1976, Training Accuracy: 0.9259, Training F1: 0.9255, Validation Loss: 0.2687, Validation Accuracy: 0.9023, Validation F1: 0.9039\n",
      "Test Loss: 0.2744, Test Accuracy: 0.9047, Test F1 Score: 0.9050\n",
      "  Training with learning rate: 0.0009\n",
      "Epoch 1/25, Training Loss: 0.5883, Training Accuracy: 0.7937, Training F1: 0.7918, Validation Loss: 0.3738, Validation Accuracy: 0.8612, Validation F1: 0.8623\n",
      "Epoch 2/25, Training Loss: 0.4361, Training Accuracy: 0.8431, Training F1: 0.8420, Validation Loss: 0.3440, Validation Accuracy: 0.8745, Validation F1: 0.8736\n",
      "Epoch 3/25, Training Loss: 0.3900, Training Accuracy: 0.8598, Training F1: 0.8588, Validation Loss: 0.3226, Validation Accuracy: 0.8803, Validation F1: 0.8822\n",
      "Epoch 4/25, Training Loss: 0.3656, Training Accuracy: 0.8681, Training F1: 0.8673, Validation Loss: 0.3230, Validation Accuracy: 0.8768, Validation F1: 0.8785\n",
      "Epoch 5/25, Training Loss: 0.3499, Training Accuracy: 0.8736, Training F1: 0.8727, Validation Loss: 0.3127, Validation Accuracy: 0.8832, Validation F1: 0.8824\n",
      "Epoch 6/25, Training Loss: 0.3330, Training Accuracy: 0.8796, Training F1: 0.8788, Validation Loss: 0.3101, Validation Accuracy: 0.8835, Validation F1: 0.8853\n",
      "Epoch 7/25, Training Loss: 0.3171, Training Accuracy: 0.8842, Training F1: 0.8835, Validation Loss: 0.2935, Validation Accuracy: 0.8920, Validation F1: 0.8924\n",
      "Epoch 8/25, Training Loss: 0.3069, Training Accuracy: 0.8869, Training F1: 0.8862, Validation Loss: 0.2858, Validation Accuracy: 0.8927, Validation F1: 0.8935\n",
      "Epoch 9/25, Training Loss: 0.2990, Training Accuracy: 0.8905, Training F1: 0.8899, Validation Loss: 0.2896, Validation Accuracy: 0.8915, Validation F1: 0.8934\n",
      "Epoch 10/25, Training Loss: 0.2847, Training Accuracy: 0.8961, Training F1: 0.8955, Validation Loss: 0.2803, Validation Accuracy: 0.8960, Validation F1: 0.8977\n",
      "Epoch 11/25, Training Loss: 0.2774, Training Accuracy: 0.8983, Training F1: 0.8978, Validation Loss: 0.2947, Validation Accuracy: 0.8913, Validation F1: 0.8917\n",
      "Epoch 12/25, Training Loss: 0.2685, Training Accuracy: 0.9004, Training F1: 0.8998, Validation Loss: 0.2870, Validation Accuracy: 0.8965, Validation F1: 0.8972\n",
      "Epoch 13/25, Training Loss: 0.2617, Training Accuracy: 0.9023, Training F1: 0.9017, Validation Loss: 0.2816, Validation Accuracy: 0.8978, Validation F1: 0.8997\n",
      "Epoch 14/25, Training Loss: 0.2523, Training Accuracy: 0.9063, Training F1: 0.9058, Validation Loss: 0.2823, Validation Accuracy: 0.8953, Validation F1: 0.8955\n",
      "Epoch 15/25, Training Loss: 0.2512, Training Accuracy: 0.9072, Training F1: 0.9066, Validation Loss: 0.2739, Validation Accuracy: 0.9023, Validation F1: 0.9031\n",
      "Epoch 16/25, Training Loss: 0.2424, Training Accuracy: 0.9105, Training F1: 0.9099, Validation Loss: 0.2929, Validation Accuracy: 0.8862, Validation F1: 0.8884\n",
      "Epoch 17/25, Training Loss: 0.2347, Training Accuracy: 0.9120, Training F1: 0.9115, Validation Loss: 0.2739, Validation Accuracy: 0.8997, Validation F1: 0.9009\n",
      "Epoch 18/25, Training Loss: 0.2300, Training Accuracy: 0.9156, Training F1: 0.9152, Validation Loss: 0.2699, Validation Accuracy: 0.9010, Validation F1: 0.9027\n",
      "Epoch 19/25, Training Loss: 0.2273, Training Accuracy: 0.9167, Training F1: 0.9163, Validation Loss: 0.2733, Validation Accuracy: 0.9022, Validation F1: 0.9030\n",
      "Epoch 20/25, Training Loss: 0.2215, Training Accuracy: 0.9175, Training F1: 0.9171, Validation Loss: 0.2794, Validation Accuracy: 0.9003, Validation F1: 0.9018\n",
      "Epoch 21/25, Training Loss: 0.2184, Training Accuracy: 0.9190, Training F1: 0.9186, Validation Loss: 0.2830, Validation Accuracy: 0.8985, Validation F1: 0.9001\n",
      "Epoch 22/25, Training Loss: 0.2071, Training Accuracy: 0.9240, Training F1: 0.9236, Validation Loss: 0.2787, Validation Accuracy: 0.9003, Validation F1: 0.9009\n",
      "Epoch 23/25, Training Loss: 0.2055, Training Accuracy: 0.9239, Training F1: 0.9235, Validation Loss: 0.2839, Validation Accuracy: 0.8997, Validation F1: 0.9006\n",
      "Epoch 24/25, Training Loss: 0.2039, Training Accuracy: 0.9241, Training F1: 0.9238, Validation Loss: 0.2753, Validation Accuracy: 0.9017, Validation F1: 0.9035\n",
      "Epoch 25/25, Training Loss: 0.1975, Training Accuracy: 0.9272, Training F1: 0.9268, Validation Loss: 0.2925, Validation Accuracy: 0.9020, Validation F1: 0.9035\n",
      "Test Loss: 0.2808, Test Accuracy: 0.9030, Test F1 Score: 0.9030\n",
      "  Training with learning rate: 0.0006\n",
      "Epoch 1/25, Training Loss: 0.6285, Training Accuracy: 0.7806, Training F1: 0.7788, Validation Loss: 0.3802, Validation Accuracy: 0.8572, Validation F1: 0.8543\n",
      "Epoch 2/25, Training Loss: 0.4451, Training Accuracy: 0.8411, Training F1: 0.8399, Validation Loss: 0.3638, Validation Accuracy: 0.8642, Validation F1: 0.8664\n",
      "Epoch 3/25, Training Loss: 0.4017, Training Accuracy: 0.8554, Training F1: 0.8544, Validation Loss: 0.3261, Validation Accuracy: 0.8785, Validation F1: 0.8792\n",
      "Epoch 4/25, Training Loss: 0.3713, Training Accuracy: 0.8659, Training F1: 0.8650, Validation Loss: 0.3187, Validation Accuracy: 0.8843, Validation F1: 0.8831\n",
      "Epoch 5/25, Training Loss: 0.3499, Training Accuracy: 0.8729, Training F1: 0.8720, Validation Loss: 0.3041, Validation Accuracy: 0.8840, Validation F1: 0.8853\n",
      "Epoch 6/25, Training Loss: 0.3367, Training Accuracy: 0.8774, Training F1: 0.8765, Validation Loss: 0.2929, Validation Accuracy: 0.8888, Validation F1: 0.8899\n",
      "Epoch 7/25, Training Loss: 0.3246, Training Accuracy: 0.8819, Training F1: 0.8813, Validation Loss: 0.2962, Validation Accuracy: 0.8873, Validation F1: 0.8898\n",
      "Epoch 8/25, Training Loss: 0.3135, Training Accuracy: 0.8855, Training F1: 0.8848, Validation Loss: 0.2918, Validation Accuracy: 0.8918, Validation F1: 0.8927\n",
      "Epoch 9/25, Training Loss: 0.2984, Training Accuracy: 0.8891, Training F1: 0.8885, Validation Loss: 0.2802, Validation Accuracy: 0.8965, Validation F1: 0.8965\n",
      "Epoch 10/25, Training Loss: 0.2899, Training Accuracy: 0.8929, Training F1: 0.8923, Validation Loss: 0.2789, Validation Accuracy: 0.8950, Validation F1: 0.8967\n",
      "Epoch 11/25, Training Loss: 0.2800, Training Accuracy: 0.8970, Training F1: 0.8964, Validation Loss: 0.2983, Validation Accuracy: 0.8912, Validation F1: 0.8934\n",
      "Epoch 12/25, Training Loss: 0.2735, Training Accuracy: 0.8985, Training F1: 0.8979, Validation Loss: 0.2764, Validation Accuracy: 0.8933, Validation F1: 0.8950\n",
      "Epoch 13/25, Training Loss: 0.2658, Training Accuracy: 0.9023, Training F1: 0.9017, Validation Loss: 0.2898, Validation Accuracy: 0.8922, Validation F1: 0.8921\n",
      "Epoch 14/25, Training Loss: 0.2614, Training Accuracy: 0.9045, Training F1: 0.9040, Validation Loss: 0.2732, Validation Accuracy: 0.8993, Validation F1: 0.9004\n",
      "Epoch 15/25, Training Loss: 0.2495, Training Accuracy: 0.9070, Training F1: 0.9064, Validation Loss: 0.2664, Validation Accuracy: 0.8995, Validation F1: 0.9007\n",
      "Epoch 16/25, Training Loss: 0.2445, Training Accuracy: 0.9081, Training F1: 0.9076, Validation Loss: 0.2685, Validation Accuracy: 0.8962, Validation F1: 0.8971\n",
      "Epoch 17/25, Training Loss: 0.2365, Training Accuracy: 0.9125, Training F1: 0.9120, Validation Loss: 0.2762, Validation Accuracy: 0.8978, Validation F1: 0.8993\n",
      "Epoch 18/25, Training Loss: 0.2315, Training Accuracy: 0.9131, Training F1: 0.9127, Validation Loss: 0.2854, Validation Accuracy: 0.8928, Validation F1: 0.8952\n",
      "Epoch 19/25, Training Loss: 0.2241, Training Accuracy: 0.9168, Training F1: 0.9164, Validation Loss: 0.2695, Validation Accuracy: 0.9043, Validation F1: 0.9044\n",
      "Epoch 20/25, Training Loss: 0.2187, Training Accuracy: 0.9190, Training F1: 0.9185, Validation Loss: 0.2778, Validation Accuracy: 0.8973, Validation F1: 0.8987\n",
      "Epoch 21/25, Training Loss: 0.2185, Training Accuracy: 0.9192, Training F1: 0.9188, Validation Loss: 0.2648, Validation Accuracy: 0.9003, Validation F1: 0.9023\n",
      "Epoch 22/25, Training Loss: 0.2148, Training Accuracy: 0.9195, Training F1: 0.9191, Validation Loss: 0.2768, Validation Accuracy: 0.9000, Validation F1: 0.9026\n",
      "Epoch 23/25, Training Loss: 0.2069, Training Accuracy: 0.9218, Training F1: 0.9214, Validation Loss: 0.2752, Validation Accuracy: 0.8910, Validation F1: 0.8933\n",
      "Epoch 24/25, Training Loss: 0.1997, Training Accuracy: 0.9260, Training F1: 0.9256, Validation Loss: 0.2809, Validation Accuracy: 0.8992, Validation F1: 0.9001\n",
      "Epoch 25/25, Training Loss: 0.1957, Training Accuracy: 0.9269, Training F1: 0.9265, Validation Loss: 0.2770, Validation Accuracy: 0.9015, Validation F1: 0.9033\n",
      "Test Loss: 0.2760, Test Accuracy: 0.9037, Test F1 Score: 0.9039\n",
      "\n",
      "Training models for architecture: Arch2\n",
      "  Training with learning rate: 0.005\n",
      "Epoch 1/25, Training Loss: 0.5739, Training Accuracy: 0.7968, Training F1: 0.7945, Validation Loss: 0.3818, Validation Accuracy: 0.8560, Validation F1: 0.8569\n",
      "Epoch 2/25, Training Loss: 0.4481, Training Accuracy: 0.8419, Training F1: 0.8406, Validation Loss: 0.3452, Validation Accuracy: 0.8725, Validation F1: 0.8719\n",
      "Epoch 3/25, Training Loss: 0.4084, Training Accuracy: 0.8561, Training F1: 0.8550, Validation Loss: 0.3305, Validation Accuracy: 0.8813, Validation F1: 0.8805\n",
      "Epoch 4/25, Training Loss: 0.3815, Training Accuracy: 0.8646, Training F1: 0.8637, Validation Loss: 0.3198, Validation Accuracy: 0.8813, Validation F1: 0.8826\n",
      "Epoch 5/25, Training Loss: 0.3606, Training Accuracy: 0.8696, Training F1: 0.8688, Validation Loss: 0.3069, Validation Accuracy: 0.8832, Validation F1: 0.8853\n",
      "Epoch 6/25, Training Loss: 0.3483, Training Accuracy: 0.8748, Training F1: 0.8740, Validation Loss: 0.3015, Validation Accuracy: 0.8828, Validation F1: 0.8838\n",
      "Epoch 7/25, Training Loss: 0.3319, Training Accuracy: 0.8799, Training F1: 0.8791, Validation Loss: 0.3219, Validation Accuracy: 0.8793, Validation F1: 0.8809\n",
      "Epoch 8/25, Training Loss: 0.3166, Training Accuracy: 0.8848, Training F1: 0.8840, Validation Loss: 0.2925, Validation Accuracy: 0.8898, Validation F1: 0.8897\n",
      "Epoch 9/25, Training Loss: 0.3085, Training Accuracy: 0.8890, Training F1: 0.8883, Validation Loss: 0.2979, Validation Accuracy: 0.8868, Validation F1: 0.8879\n",
      "Epoch 10/25, Training Loss: 0.2971, Training Accuracy: 0.8922, Training F1: 0.8915, Validation Loss: 0.2781, Validation Accuracy: 0.9002, Validation F1: 0.9004\n",
      "Epoch 11/25, Training Loss: 0.2881, Training Accuracy: 0.8955, Training F1: 0.8949, Validation Loss: 0.2828, Validation Accuracy: 0.8942, Validation F1: 0.8960\n",
      "Epoch 12/25, Training Loss: 0.2765, Training Accuracy: 0.8992, Training F1: 0.8986, Validation Loss: 0.3009, Validation Accuracy: 0.8918, Validation F1: 0.8931\n",
      "Epoch 13/25, Training Loss: 0.2716, Training Accuracy: 0.9012, Training F1: 0.9006, Validation Loss: 0.2934, Validation Accuracy: 0.8907, Validation F1: 0.8901\n",
      "Epoch 14/25, Training Loss: 0.2613, Training Accuracy: 0.9036, Training F1: 0.9031, Validation Loss: 0.2700, Validation Accuracy: 0.8995, Validation F1: 0.9007\n",
      "Epoch 15/25, Training Loss: 0.2529, Training Accuracy: 0.9077, Training F1: 0.9072, Validation Loss: 0.2788, Validation Accuracy: 0.8975, Validation F1: 0.8992\n",
      "Epoch 16/25, Training Loss: 0.2461, Training Accuracy: 0.9114, Training F1: 0.9109, Validation Loss: 0.2796, Validation Accuracy: 0.8915, Validation F1: 0.8924\n",
      "Epoch 17/25, Training Loss: 0.2408, Training Accuracy: 0.9104, Training F1: 0.9099, Validation Loss: 0.2789, Validation Accuracy: 0.8955, Validation F1: 0.8968\n",
      "Epoch 18/25, Training Loss: 0.2311, Training Accuracy: 0.9152, Training F1: 0.9148, Validation Loss: 0.2999, Validation Accuracy: 0.8947, Validation F1: 0.8942\n",
      "Epoch 19/25, Training Loss: 0.2275, Training Accuracy: 0.9150, Training F1: 0.9145, Validation Loss: 0.2738, Validation Accuracy: 0.9022, Validation F1: 0.9036\n",
      "Epoch 20/25, Training Loss: 0.2197, Training Accuracy: 0.9196, Training F1: 0.9192, Validation Loss: 0.2711, Validation Accuracy: 0.9037, Validation F1: 0.9044\n",
      "Epoch 21/25, Training Loss: 0.2145, Training Accuracy: 0.9216, Training F1: 0.9212, Validation Loss: 0.2788, Validation Accuracy: 0.8977, Validation F1: 0.8995\n",
      "Epoch 22/25, Training Loss: 0.2085, Training Accuracy: 0.9237, Training F1: 0.9233, Validation Loss: 0.2742, Validation Accuracy: 0.9088, Validation F1: 0.9101\n",
      "Epoch 23/25, Training Loss: 0.2089, Training Accuracy: 0.9234, Training F1: 0.9231, Validation Loss: 0.2721, Validation Accuracy: 0.9028, Validation F1: 0.9034\n",
      "Epoch 24/25, Training Loss: 0.2020, Training Accuracy: 0.9257, Training F1: 0.9253, Validation Loss: 0.2681, Validation Accuracy: 0.9053, Validation F1: 0.9068\n",
      "Epoch 25/25, Training Loss: 0.1970, Training Accuracy: 0.9280, Training F1: 0.9276, Validation Loss: 0.2960, Validation Accuracy: 0.8963, Validation F1: 0.8995\n",
      "Test Loss: 0.2919, Test Accuracy: 0.8974, Test F1 Score: 0.8985\n",
      "  Training with learning rate: 0.001\n",
      "Epoch 1/25, Training Loss: 0.5955, Training Accuracy: 0.7941, Training F1: 0.7917, Validation Loss: 0.3628, Validation Accuracy: 0.8662, Validation F1: 0.8653\n",
      "Epoch 2/25, Training Loss: 0.4321, Training Accuracy: 0.8456, Training F1: 0.8443, Validation Loss: 0.3320, Validation Accuracy: 0.8783, Validation F1: 0.8802\n",
      "Epoch 3/25, Training Loss: 0.3922, Training Accuracy: 0.8595, Training F1: 0.8586, Validation Loss: 0.3180, Validation Accuracy: 0.8850, Validation F1: 0.8864\n",
      "Epoch 4/25, Training Loss: 0.3702, Training Accuracy: 0.8671, Training F1: 0.8662, Validation Loss: 0.3232, Validation Accuracy: 0.8793, Validation F1: 0.8800\n",
      "Epoch 5/25, Training Loss: 0.3474, Training Accuracy: 0.8735, Training F1: 0.8728, Validation Loss: 0.2969, Validation Accuracy: 0.8887, Validation F1: 0.8909\n",
      "Epoch 6/25, Training Loss: 0.3301, Training Accuracy: 0.8806, Training F1: 0.8800, Validation Loss: 0.2984, Validation Accuracy: 0.8898, Validation F1: 0.8907\n",
      "Epoch 7/25, Training Loss: 0.3179, Training Accuracy: 0.8842, Training F1: 0.8836, Validation Loss: 0.2895, Validation Accuracy: 0.8925, Validation F1: 0.8939\n",
      "Epoch 8/25, Training Loss: 0.3073, Training Accuracy: 0.8886, Training F1: 0.8880, Validation Loss: 0.2991, Validation Accuracy: 0.8857, Validation F1: 0.8880\n",
      "Epoch 9/25, Training Loss: 0.2946, Training Accuracy: 0.8928, Training F1: 0.8922, Validation Loss: 0.3001, Validation Accuracy: 0.8895, Validation F1: 0.8896\n",
      "Epoch 10/25, Training Loss: 0.2867, Training Accuracy: 0.8958, Training F1: 0.8952, Validation Loss: 0.2971, Validation Accuracy: 0.8908, Validation F1: 0.8917\n",
      "Epoch 11/25, Training Loss: 0.2749, Training Accuracy: 0.9017, Training F1: 0.9012, Validation Loss: 0.3315, Validation Accuracy: 0.8757, Validation F1: 0.8812\n",
      "Epoch 12/25, Training Loss: 0.2660, Training Accuracy: 0.9023, Training F1: 0.9017, Validation Loss: 0.2732, Validation Accuracy: 0.8993, Validation F1: 0.9010\n",
      "Epoch 13/25, Training Loss: 0.2604, Training Accuracy: 0.9044, Training F1: 0.9039, Validation Loss: 0.2902, Validation Accuracy: 0.8943, Validation F1: 0.8953\n",
      "Epoch 14/25, Training Loss: 0.2481, Training Accuracy: 0.9077, Training F1: 0.9072, Validation Loss: 0.2735, Validation Accuracy: 0.9040, Validation F1: 0.9051\n",
      "Epoch 15/25, Training Loss: 0.2411, Training Accuracy: 0.9113, Training F1: 0.9108, Validation Loss: 0.2909, Validation Accuracy: 0.8925, Validation F1: 0.8912\n",
      "Epoch 16/25, Training Loss: 0.2363, Training Accuracy: 0.9141, Training F1: 0.9137, Validation Loss: 0.2704, Validation Accuracy: 0.9003, Validation F1: 0.9017\n",
      "Epoch 17/25, Training Loss: 0.2295, Training Accuracy: 0.9155, Training F1: 0.9151, Validation Loss: 0.2901, Validation Accuracy: 0.8987, Validation F1: 0.8989\n",
      "Epoch 18/25, Training Loss: 0.2227, Training Accuracy: 0.9186, Training F1: 0.9182, Validation Loss: 0.2803, Validation Accuracy: 0.8977, Validation F1: 0.8992\n",
      "Epoch 19/25, Training Loss: 0.2179, Training Accuracy: 0.9190, Training F1: 0.9185, Validation Loss: 0.2934, Validation Accuracy: 0.8980, Validation F1: 0.8974\n",
      "Epoch 20/25, Training Loss: 0.2064, Training Accuracy: 0.9239, Training F1: 0.9236, Validation Loss: 0.2707, Validation Accuracy: 0.9037, Validation F1: 0.9050\n",
      "Epoch 21/25, Training Loss: 0.2037, Training Accuracy: 0.9246, Training F1: 0.9241, Validation Loss: 0.2872, Validation Accuracy: 0.9032, Validation F1: 0.9027\n",
      "Epoch 22/25, Training Loss: 0.2003, Training Accuracy: 0.9266, Training F1: 0.9263, Validation Loss: 0.2782, Validation Accuracy: 0.9023, Validation F1: 0.9021\n",
      "Epoch 23/25, Training Loss: 0.1927, Training Accuracy: 0.9294, Training F1: 0.9291, Validation Loss: 0.2746, Validation Accuracy: 0.9047, Validation F1: 0.9058\n",
      "Epoch 24/25, Training Loss: 0.1875, Training Accuracy: 0.9303, Training F1: 0.9300, Validation Loss: 0.2739, Validation Accuracy: 0.9032, Validation F1: 0.9045\n",
      "Epoch 25/25, Training Loss: 0.1845, Training Accuracy: 0.9318, Training F1: 0.9315, Validation Loss: 0.2811, Validation Accuracy: 0.9063, Validation F1: 0.9070\n",
      "Test Loss: 0.2755, Test Accuracy: 0.9047, Test F1 Score: 0.9033\n",
      "  Training with learning rate: 0.0009\n",
      "Epoch 1/25, Training Loss: 0.5995, Training Accuracy: 0.7938, Training F1: 0.7918, Validation Loss: 0.3649, Validation Accuracy: 0.8680, Validation F1: 0.8700\n",
      "Epoch 2/25, Training Loss: 0.4348, Training Accuracy: 0.8456, Training F1: 0.8446, Validation Loss: 0.3325, Validation Accuracy: 0.8823, Validation F1: 0.8823\n",
      "Epoch 3/25, Training Loss: 0.3949, Training Accuracy: 0.8585, Training F1: 0.8575, Validation Loss: 0.3286, Validation Accuracy: 0.8777, Validation F1: 0.8781\n",
      "Epoch 4/25, Training Loss: 0.3677, Training Accuracy: 0.8686, Training F1: 0.8675, Validation Loss: 0.3170, Validation Accuracy: 0.8817, Validation F1: 0.8819\n",
      "Epoch 5/25, Training Loss: 0.3484, Training Accuracy: 0.8743, Training F1: 0.8734, Validation Loss: 0.3148, Validation Accuracy: 0.8810, Validation F1: 0.8811\n",
      "Epoch 6/25, Training Loss: 0.3329, Training Accuracy: 0.8801, Training F1: 0.8794, Validation Loss: 0.3056, Validation Accuracy: 0.8852, Validation F1: 0.8864\n",
      "Epoch 7/25, Training Loss: 0.3153, Training Accuracy: 0.8863, Training F1: 0.8856, Validation Loss: 0.3102, Validation Accuracy: 0.8822, Validation F1: 0.8778\n",
      "Epoch 8/25, Training Loss: 0.3060, Training Accuracy: 0.8882, Training F1: 0.8874, Validation Loss: 0.3007, Validation Accuracy: 0.8875, Validation F1: 0.8870\n",
      "Epoch 9/25, Training Loss: 0.2939, Training Accuracy: 0.8933, Training F1: 0.8926, Validation Loss: 0.2907, Validation Accuracy: 0.8945, Validation F1: 0.8952\n",
      "Epoch 10/25, Training Loss: 0.2858, Training Accuracy: 0.8957, Training F1: 0.8951, Validation Loss: 0.2844, Validation Accuracy: 0.8925, Validation F1: 0.8925\n",
      "Epoch 11/25, Training Loss: 0.2753, Training Accuracy: 0.9001, Training F1: 0.8995, Validation Loss: 0.2847, Validation Accuracy: 0.8915, Validation F1: 0.8937\n",
      "Epoch 12/25, Training Loss: 0.2667, Training Accuracy: 0.9024, Training F1: 0.9018, Validation Loss: 0.3204, Validation Accuracy: 0.8853, Validation F1: 0.8818\n",
      "Epoch 13/25, Training Loss: 0.2568, Training Accuracy: 0.9061, Training F1: 0.9056, Validation Loss: 0.2724, Validation Accuracy: 0.8958, Validation F1: 0.8980\n",
      "Epoch 14/25, Training Loss: 0.2497, Training Accuracy: 0.9086, Training F1: 0.9081, Validation Loss: 0.2701, Validation Accuracy: 0.8962, Validation F1: 0.8969\n",
      "Epoch 15/25, Training Loss: 0.2424, Training Accuracy: 0.9114, Training F1: 0.9109, Validation Loss: 0.2833, Validation Accuracy: 0.8967, Validation F1: 0.8967\n",
      "Epoch 16/25, Training Loss: 0.2338, Training Accuracy: 0.9134, Training F1: 0.9129, Validation Loss: 0.2691, Validation Accuracy: 0.9008, Validation F1: 0.9030\n",
      "Epoch 17/25, Training Loss: 0.2259, Training Accuracy: 0.9161, Training F1: 0.9157, Validation Loss: 0.2783, Validation Accuracy: 0.8980, Validation F1: 0.9008\n",
      "Epoch 18/25, Training Loss: 0.2222, Training Accuracy: 0.9181, Training F1: 0.9176, Validation Loss: 0.2756, Validation Accuracy: 0.9012, Validation F1: 0.9021\n",
      "Epoch 19/25, Training Loss: 0.2159, Training Accuracy: 0.9209, Training F1: 0.9205, Validation Loss: 0.2873, Validation Accuracy: 0.8942, Validation F1: 0.8962\n",
      "Epoch 20/25, Training Loss: 0.2124, Training Accuracy: 0.9216, Training F1: 0.9212, Validation Loss: 0.2670, Validation Accuracy: 0.9070, Validation F1: 0.9085\n",
      "Epoch 21/25, Training Loss: 0.2030, Training Accuracy: 0.9259, Training F1: 0.9256, Validation Loss: 0.2783, Validation Accuracy: 0.9010, Validation F1: 0.9022\n",
      "Epoch 22/25, Training Loss: 0.1974, Training Accuracy: 0.9264, Training F1: 0.9259, Validation Loss: 0.2996, Validation Accuracy: 0.8997, Validation F1: 0.8992\n",
      "Epoch 23/25, Training Loss: 0.1953, Training Accuracy: 0.9284, Training F1: 0.9280, Validation Loss: 0.2735, Validation Accuracy: 0.9060, Validation F1: 0.9074\n",
      "Epoch 24/25, Training Loss: 0.1885, Training Accuracy: 0.9303, Training F1: 0.9299, Validation Loss: 0.2777, Validation Accuracy: 0.9077, Validation F1: 0.9085\n",
      "Epoch 25/25, Training Loss: 0.1834, Training Accuracy: 0.9325, Training F1: 0.9322, Validation Loss: 0.2922, Validation Accuracy: 0.8990, Validation F1: 0.9014\n",
      "Test Loss: 0.2898, Test Accuracy: 0.8986, Test F1 Score: 0.8989\n",
      "  Training with learning rate: 0.0006\n",
      "Epoch 1/25, Training Loss: 0.6249, Training Accuracy: 0.7866, Training F1: 0.7849, Validation Loss: 0.3676, Validation Accuracy: 0.8658, Validation F1: 0.8642\n",
      "Epoch 2/25, Training Loss: 0.4413, Training Accuracy: 0.8439, Training F1: 0.8428, Validation Loss: 0.3355, Validation Accuracy: 0.8718, Validation F1: 0.8715\n",
      "Epoch 3/25, Training Loss: 0.3999, Training Accuracy: 0.8581, Training F1: 0.8572, Validation Loss: 0.3099, Validation Accuracy: 0.8860, Validation F1: 0.8856\n",
      "Epoch 4/25, Training Loss: 0.3712, Training Accuracy: 0.8661, Training F1: 0.8653, Validation Loss: 0.3146, Validation Accuracy: 0.8837, Validation F1: 0.8828\n",
      "Epoch 5/25, Training Loss: 0.3462, Training Accuracy: 0.8761, Training F1: 0.8753, Validation Loss: 0.3065, Validation Accuracy: 0.8882, Validation F1: 0.8880\n",
      "Epoch 6/25, Training Loss: 0.3329, Training Accuracy: 0.8793, Training F1: 0.8785, Validation Loss: 0.3258, Validation Accuracy: 0.8777, Validation F1: 0.8802\n",
      "Epoch 7/25, Training Loss: 0.3183, Training Accuracy: 0.8835, Training F1: 0.8828, Validation Loss: 0.2945, Validation Accuracy: 0.8903, Validation F1: 0.8907\n",
      "Epoch 8/25, Training Loss: 0.3051, Training Accuracy: 0.8886, Training F1: 0.8880, Validation Loss: 0.3050, Validation Accuracy: 0.8877, Validation F1: 0.8893\n",
      "Epoch 9/25, Training Loss: 0.2971, Training Accuracy: 0.8925, Training F1: 0.8918, Validation Loss: 0.3047, Validation Accuracy: 0.8865, Validation F1: 0.8861\n",
      "Epoch 10/25, Training Loss: 0.2847, Training Accuracy: 0.8946, Training F1: 0.8940, Validation Loss: 0.2738, Validation Accuracy: 0.9013, Validation F1: 0.9021\n",
      "Epoch 11/25, Training Loss: 0.2767, Training Accuracy: 0.8988, Training F1: 0.8983, Validation Loss: 0.2878, Validation Accuracy: 0.8928, Validation F1: 0.8937\n",
      "Epoch 12/25, Training Loss: 0.2664, Training Accuracy: 0.9025, Training F1: 0.9019, Validation Loss: 0.2649, Validation Accuracy: 0.9020, Validation F1: 0.9035\n",
      "Epoch 13/25, Training Loss: 0.2565, Training Accuracy: 0.9055, Training F1: 0.9050, Validation Loss: 0.2983, Validation Accuracy: 0.8912, Validation F1: 0.8903\n",
      "Epoch 14/25, Training Loss: 0.2505, Training Accuracy: 0.9077, Training F1: 0.9071, Validation Loss: 0.2793, Validation Accuracy: 0.8977, Validation F1: 0.8979\n",
      "Epoch 15/25, Training Loss: 0.2398, Training Accuracy: 0.9116, Training F1: 0.9112, Validation Loss: 0.2903, Validation Accuracy: 0.8963, Validation F1: 0.8956\n",
      "Epoch 16/25, Training Loss: 0.2339, Training Accuracy: 0.9140, Training F1: 0.9135, Validation Loss: 0.2606, Validation Accuracy: 0.9000, Validation F1: 0.9014\n",
      "Epoch 17/25, Training Loss: 0.2293, Training Accuracy: 0.9163, Training F1: 0.9159, Validation Loss: 0.2600, Validation Accuracy: 0.9052, Validation F1: 0.9060\n",
      "Epoch 18/25, Training Loss: 0.2225, Training Accuracy: 0.9185, Training F1: 0.9180, Validation Loss: 0.2836, Validation Accuracy: 0.8992, Validation F1: 0.8996\n",
      "Epoch 19/25, Training Loss: 0.2177, Training Accuracy: 0.9206, Training F1: 0.9202, Validation Loss: 0.2653, Validation Accuracy: 0.9032, Validation F1: 0.9049\n",
      "Epoch 20/25, Training Loss: 0.2074, Training Accuracy: 0.9231, Training F1: 0.9227, Validation Loss: 0.2671, Validation Accuracy: 0.9065, Validation F1: 0.9076\n",
      "Epoch 21/25, Training Loss: 0.2085, Training Accuracy: 0.9233, Training F1: 0.9229, Validation Loss: 0.2589, Validation Accuracy: 0.9093, Validation F1: 0.9103\n",
      "Epoch 22/25, Training Loss: 0.2018, Training Accuracy: 0.9256, Training F1: 0.9252, Validation Loss: 0.2756, Validation Accuracy: 0.9047, Validation F1: 0.9058\n",
      "Epoch 23/25, Training Loss: 0.1940, Training Accuracy: 0.9289, Training F1: 0.9285, Validation Loss: 0.2681, Validation Accuracy: 0.9000, Validation F1: 0.9011\n",
      "Epoch 24/25, Training Loss: 0.1894, Training Accuracy: 0.9301, Training F1: 0.9297, Validation Loss: 0.2908, Validation Accuracy: 0.8923, Validation F1: 0.8955\n",
      "Epoch 25/25, Training Loss: 0.1845, Training Accuracy: 0.9321, Training F1: 0.9318, Validation Loss: 0.3115, Validation Accuracy: 0.8977, Validation F1: 0.8969\n",
      "Test Loss: 0.2981, Test Accuracy: 0.8986, Test F1 Score: 0.8967\n",
      "\n",
      "Training models for architecture: Arch3\n",
      "  Training with learning rate: 0.005\n",
      "Epoch 1/25, Training Loss: 0.5420, Training Accuracy: 0.8067, Training F1: 0.8053, Validation Loss: 0.3835, Validation Accuracy: 0.8565, Validation F1: 0.8578\n",
      "Epoch 2/25, Training Loss: 0.4264, Training Accuracy: 0.8454, Training F1: 0.8444, Validation Loss: 0.3732, Validation Accuracy: 0.8597, Validation F1: 0.8582\n",
      "Epoch 3/25, Training Loss: 0.3912, Training Accuracy: 0.8583, Training F1: 0.8573, Validation Loss: 0.3348, Validation Accuracy: 0.8777, Validation F1: 0.8764\n",
      "Epoch 4/25, Training Loss: 0.3700, Training Accuracy: 0.8653, Training F1: 0.8644, Validation Loss: 0.3667, Validation Accuracy: 0.8647, Validation F1: 0.8644\n",
      "Epoch 5/25, Training Loss: 0.3507, Training Accuracy: 0.8710, Training F1: 0.8703, Validation Loss: 0.3102, Validation Accuracy: 0.8833, Validation F1: 0.8850\n",
      "Epoch 6/25, Training Loss: 0.3335, Training Accuracy: 0.8767, Training F1: 0.8760, Validation Loss: 0.3014, Validation Accuracy: 0.8890, Validation F1: 0.8893\n",
      "Epoch 7/25, Training Loss: 0.3256, Training Accuracy: 0.8799, Training F1: 0.8791, Validation Loss: 0.2990, Validation Accuracy: 0.8880, Validation F1: 0.8893\n",
      "Epoch 8/25, Training Loss: 0.3181, Training Accuracy: 0.8827, Training F1: 0.8819, Validation Loss: 0.2905, Validation Accuracy: 0.8978, Validation F1: 0.8984\n",
      "Epoch 9/25, Training Loss: 0.3051, Training Accuracy: 0.8878, Training F1: 0.8871, Validation Loss: 0.2993, Validation Accuracy: 0.8890, Validation F1: 0.8905\n",
      "Epoch 10/25, Training Loss: 0.2998, Training Accuracy: 0.8895, Training F1: 0.8888, Validation Loss: 0.2846, Validation Accuracy: 0.8948, Validation F1: 0.8947\n",
      "Epoch 11/25, Training Loss: 0.2915, Training Accuracy: 0.8919, Training F1: 0.8912, Validation Loss: 0.2823, Validation Accuracy: 0.8942, Validation F1: 0.8950\n",
      "Epoch 12/25, Training Loss: 0.2829, Training Accuracy: 0.8955, Training F1: 0.8950, Validation Loss: 0.2978, Validation Accuracy: 0.8883, Validation F1: 0.8889\n",
      "Epoch 13/25, Training Loss: 0.2778, Training Accuracy: 0.8972, Training F1: 0.8966, Validation Loss: 0.2802, Validation Accuracy: 0.8948, Validation F1: 0.8968\n",
      "Epoch 14/25, Training Loss: 0.2677, Training Accuracy: 0.9009, Training F1: 0.9003, Validation Loss: 0.3030, Validation Accuracy: 0.8905, Validation F1: 0.8898\n",
      "Epoch 15/25, Training Loss: 0.2632, Training Accuracy: 0.9014, Training F1: 0.9009, Validation Loss: 0.2830, Validation Accuracy: 0.8958, Validation F1: 0.8968\n",
      "Epoch 16/25, Training Loss: 0.2599, Training Accuracy: 0.9030, Training F1: 0.9025, Validation Loss: 0.2796, Validation Accuracy: 0.9035, Validation F1: 0.9042\n",
      "Epoch 17/25, Training Loss: 0.2541, Training Accuracy: 0.9047, Training F1: 0.9041, Validation Loss: 0.2861, Validation Accuracy: 0.8958, Validation F1: 0.8973\n",
      "Epoch 18/25, Training Loss: 0.2470, Training Accuracy: 0.9072, Training F1: 0.9067, Validation Loss: 0.2853, Validation Accuracy: 0.8998, Validation F1: 0.9004\n",
      "Epoch 19/25, Training Loss: 0.2483, Training Accuracy: 0.9059, Training F1: 0.9053, Validation Loss: 0.2779, Validation Accuracy: 0.9018, Validation F1: 0.9025\n",
      "Epoch 20/25, Training Loss: 0.2398, Training Accuracy: 0.9105, Training F1: 0.9100, Validation Loss: 0.2833, Validation Accuracy: 0.8985, Validation F1: 0.8998\n",
      "Epoch 21/25, Training Loss: 0.2372, Training Accuracy: 0.9111, Training F1: 0.9106, Validation Loss: 0.2829, Validation Accuracy: 0.9015, Validation F1: 0.9035\n",
      "Epoch 22/25, Training Loss: 0.2344, Training Accuracy: 0.9124, Training F1: 0.9119, Validation Loss: 0.2875, Validation Accuracy: 0.8975, Validation F1: 0.8978\n",
      "Epoch 23/25, Training Loss: 0.2302, Training Accuracy: 0.9143, Training F1: 0.9138, Validation Loss: 0.2847, Validation Accuracy: 0.8990, Validation F1: 0.9008\n",
      "Epoch 24/25, Training Loss: 0.2285, Training Accuracy: 0.9145, Training F1: 0.9140, Validation Loss: 0.2835, Validation Accuracy: 0.9028, Validation F1: 0.9040\n",
      "Epoch 25/25, Training Loss: 0.2225, Training Accuracy: 0.9167, Training F1: 0.9162, Validation Loss: 0.2780, Validation Accuracy: 0.9025, Validation F1: 0.9037\n",
      "Test Loss: 0.2782, Test Accuracy: 0.9030, Test F1 Score: 0.9026\n",
      "  Training with learning rate: 0.001\n",
      "Epoch 1/25, Training Loss: 0.5711, Training Accuracy: 0.7985, Training F1: 0.7967, Validation Loss: 0.3716, Validation Accuracy: 0.8632, Validation F1: 0.8640\n",
      "Epoch 2/25, Training Loss: 0.4306, Training Accuracy: 0.8447, Training F1: 0.8437, Validation Loss: 0.3425, Validation Accuracy: 0.8695, Validation F1: 0.8722\n",
      "Epoch 3/25, Training Loss: 0.3873, Training Accuracy: 0.8601, Training F1: 0.8593, Validation Loss: 0.3196, Validation Accuracy: 0.8812, Validation F1: 0.8825\n",
      "Epoch 4/25, Training Loss: 0.3634, Training Accuracy: 0.8680, Training F1: 0.8672, Validation Loss: 0.3310, Validation Accuracy: 0.8790, Validation F1: 0.8786\n",
      "Epoch 5/25, Training Loss: 0.3476, Training Accuracy: 0.8736, Training F1: 0.8728, Validation Loss: 0.3094, Validation Accuracy: 0.8832, Validation F1: 0.8852\n",
      "Epoch 6/25, Training Loss: 0.3330, Training Accuracy: 0.8772, Training F1: 0.8765, Validation Loss: 0.2989, Validation Accuracy: 0.8848, Validation F1: 0.8856\n",
      "Epoch 7/25, Training Loss: 0.3176, Training Accuracy: 0.8844, Training F1: 0.8837, Validation Loss: 0.2870, Validation Accuracy: 0.8927, Validation F1: 0.8936\n",
      "Epoch 8/25, Training Loss: 0.3118, Training Accuracy: 0.8845, Training F1: 0.8838, Validation Loss: 0.2959, Validation Accuracy: 0.8917, Validation F1: 0.8919\n",
      "Epoch 9/25, Training Loss: 0.2986, Training Accuracy: 0.8903, Training F1: 0.8897, Validation Loss: 0.2856, Validation Accuracy: 0.8900, Validation F1: 0.8920\n",
      "Epoch 10/25, Training Loss: 0.2863, Training Accuracy: 0.8946, Training F1: 0.8941, Validation Loss: 0.2815, Validation Accuracy: 0.8963, Validation F1: 0.8967\n",
      "Epoch 11/25, Training Loss: 0.2797, Training Accuracy: 0.8981, Training F1: 0.8975, Validation Loss: 0.2804, Validation Accuracy: 0.8965, Validation F1: 0.8976\n",
      "Epoch 12/25, Training Loss: 0.2721, Training Accuracy: 0.8997, Training F1: 0.8991, Validation Loss: 0.2901, Validation Accuracy: 0.8962, Validation F1: 0.8961\n",
      "Epoch 13/25, Training Loss: 0.2674, Training Accuracy: 0.9006, Training F1: 0.9001, Validation Loss: 0.2770, Validation Accuracy: 0.8988, Validation F1: 0.8997\n",
      "Epoch 14/25, Training Loss: 0.2611, Training Accuracy: 0.9042, Training F1: 0.9037, Validation Loss: 0.2832, Validation Accuracy: 0.8950, Validation F1: 0.8965\n",
      "Epoch 15/25, Training Loss: 0.2559, Training Accuracy: 0.9053, Training F1: 0.9049, Validation Loss: 0.2770, Validation Accuracy: 0.8982, Validation F1: 0.8994\n",
      "Epoch 16/25, Training Loss: 0.2490, Training Accuracy: 0.9076, Training F1: 0.9071, Validation Loss: 0.2699, Validation Accuracy: 0.9005, Validation F1: 0.9024\n",
      "Epoch 17/25, Training Loss: 0.2399, Training Accuracy: 0.9101, Training F1: 0.9096, Validation Loss: 0.2710, Validation Accuracy: 0.9037, Validation F1: 0.9044\n",
      "Epoch 18/25, Training Loss: 0.2371, Training Accuracy: 0.9126, Training F1: 0.9121, Validation Loss: 0.2767, Validation Accuracy: 0.9023, Validation F1: 0.9038\n",
      "Epoch 19/25, Training Loss: 0.2319, Training Accuracy: 0.9139, Training F1: 0.9135, Validation Loss: 0.2734, Validation Accuracy: 0.9022, Validation F1: 0.9031\n",
      "Epoch 20/25, Training Loss: 0.2291, Training Accuracy: 0.9140, Training F1: 0.9136, Validation Loss: 0.2726, Validation Accuracy: 0.9030, Validation F1: 0.9037\n",
      "Epoch 21/25, Training Loss: 0.2256, Training Accuracy: 0.9158, Training F1: 0.9153, Validation Loss: 0.2746, Validation Accuracy: 0.9027, Validation F1: 0.9033\n",
      "Epoch 22/25, Training Loss: 0.2215, Training Accuracy: 0.9170, Training F1: 0.9166, Validation Loss: 0.2833, Validation Accuracy: 0.9027, Validation F1: 0.9028\n",
      "Epoch 23/25, Training Loss: 0.2159, Training Accuracy: 0.9190, Training F1: 0.9186, Validation Loss: 0.2785, Validation Accuracy: 0.9020, Validation F1: 0.9036\n",
      "Epoch 24/25, Training Loss: 0.2158, Training Accuracy: 0.9197, Training F1: 0.9193, Validation Loss: 0.2829, Validation Accuracy: 0.9015, Validation F1: 0.9025\n",
      "Epoch 25/25, Training Loss: 0.2103, Training Accuracy: 0.9215, Training F1: 0.9212, Validation Loss: 0.2683, Validation Accuracy: 0.9048, Validation F1: 0.9062\n",
      "Test Loss: 0.2727, Test Accuracy: 0.9041, Test F1 Score: 0.9039\n",
      "  Training with learning rate: 0.0009\n",
      "Epoch 1/25, Training Loss: 0.5748, Training Accuracy: 0.7978, Training F1: 0.7963, Validation Loss: 0.3892, Validation Accuracy: 0.8563, Validation F1: 0.8553\n",
      "Epoch 2/25, Training Loss: 0.4306, Training Accuracy: 0.8437, Training F1: 0.8426, Validation Loss: 0.3393, Validation Accuracy: 0.8757, Validation F1: 0.8755\n",
      "Epoch 3/25, Training Loss: 0.3917, Training Accuracy: 0.8587, Training F1: 0.8578, Validation Loss: 0.3366, Validation Accuracy: 0.8725, Validation F1: 0.8731\n",
      "Epoch 4/25, Training Loss: 0.3673, Training Accuracy: 0.8672, Training F1: 0.8665, Validation Loss: 0.3282, Validation Accuracy: 0.8758, Validation F1: 0.8735\n",
      "Epoch 5/25, Training Loss: 0.3456, Training Accuracy: 0.8740, Training F1: 0.8734, Validation Loss: 0.3073, Validation Accuracy: 0.8848, Validation F1: 0.8864\n",
      "Epoch 6/25, Training Loss: 0.3343, Training Accuracy: 0.8766, Training F1: 0.8759, Validation Loss: 0.3020, Validation Accuracy: 0.8853, Validation F1: 0.8842\n",
      "Epoch 7/25, Training Loss: 0.3212, Training Accuracy: 0.8828, Training F1: 0.8821, Validation Loss: 0.3022, Validation Accuracy: 0.8870, Validation F1: 0.8862\n",
      "Epoch 8/25, Training Loss: 0.3064, Training Accuracy: 0.8880, Training F1: 0.8874, Validation Loss: 0.2955, Validation Accuracy: 0.8897, Validation F1: 0.8908\n",
      "Epoch 9/25, Training Loss: 0.2998, Training Accuracy: 0.8897, Training F1: 0.8891, Validation Loss: 0.2884, Validation Accuracy: 0.8923, Validation F1: 0.8920\n",
      "Epoch 10/25, Training Loss: 0.2901, Training Accuracy: 0.8925, Training F1: 0.8919, Validation Loss: 0.2785, Validation Accuracy: 0.8987, Validation F1: 0.9003\n",
      "Epoch 11/25, Training Loss: 0.2817, Training Accuracy: 0.8944, Training F1: 0.8938, Validation Loss: 0.2825, Validation Accuracy: 0.8960, Validation F1: 0.8969\n",
      "Epoch 12/25, Training Loss: 0.2737, Training Accuracy: 0.8979, Training F1: 0.8974, Validation Loss: 0.2661, Validation Accuracy: 0.9000, Validation F1: 0.9004\n",
      "Epoch 13/25, Training Loss: 0.2685, Training Accuracy: 0.9014, Training F1: 0.9009, Validation Loss: 0.2799, Validation Accuracy: 0.8977, Validation F1: 0.8988\n",
      "Epoch 14/25, Training Loss: 0.2608, Training Accuracy: 0.9031, Training F1: 0.9025, Validation Loss: 0.2705, Validation Accuracy: 0.8987, Validation F1: 0.9003\n",
      "Epoch 15/25, Training Loss: 0.2529, Training Accuracy: 0.9051, Training F1: 0.9046, Validation Loss: 0.2805, Validation Accuracy: 0.8922, Validation F1: 0.8948\n",
      "Epoch 16/25, Training Loss: 0.2494, Training Accuracy: 0.9078, Training F1: 0.9073, Validation Loss: 0.2695, Validation Accuracy: 0.8980, Validation F1: 0.8998\n",
      "Epoch 17/25, Training Loss: 0.2459, Training Accuracy: 0.9080, Training F1: 0.9075, Validation Loss: 0.2691, Validation Accuracy: 0.9008, Validation F1: 0.9022\n",
      "Epoch 18/25, Training Loss: 0.2409, Training Accuracy: 0.9111, Training F1: 0.9107, Validation Loss: 0.2905, Validation Accuracy: 0.8948, Validation F1: 0.8941\n",
      "Epoch 19/25, Training Loss: 0.2344, Training Accuracy: 0.9113, Training F1: 0.9108, Validation Loss: 0.2657, Validation Accuracy: 0.9003, Validation F1: 0.9016\n",
      "Epoch 20/25, Training Loss: 0.2295, Training Accuracy: 0.9153, Training F1: 0.9148, Validation Loss: 0.2830, Validation Accuracy: 0.8960, Validation F1: 0.8978\n",
      "Epoch 21/25, Training Loss: 0.2235, Training Accuracy: 0.9162, Training F1: 0.9158, Validation Loss: 0.2742, Validation Accuracy: 0.9027, Validation F1: 0.9044\n",
      "Epoch 22/25, Training Loss: 0.2198, Training Accuracy: 0.9176, Training F1: 0.9172, Validation Loss: 0.2772, Validation Accuracy: 0.9037, Validation F1: 0.9037\n",
      "Epoch 23/25, Training Loss: 0.2183, Training Accuracy: 0.9181, Training F1: 0.9176, Validation Loss: 0.2757, Validation Accuracy: 0.8983, Validation F1: 0.8999\n",
      "Epoch 24/25, Training Loss: 0.2133, Training Accuracy: 0.9200, Training F1: 0.9196, Validation Loss: 0.2747, Validation Accuracy: 0.9003, Validation F1: 0.9022\n",
      "Epoch 25/25, Training Loss: 0.2088, Training Accuracy: 0.9230, Training F1: 0.9226, Validation Loss: 0.2807, Validation Accuracy: 0.9007, Validation F1: 0.9030\n",
      "Test Loss: 0.2799, Test Accuracy: 0.8994, Test F1 Score: 0.9000\n",
      "  Training with learning rate: 0.0006\n",
      "Epoch 1/25, Training Loss: 0.5981, Training Accuracy: 0.7904, Training F1: 0.7883, Validation Loss: 0.3923, Validation Accuracy: 0.8563, Validation F1: 0.8588\n",
      "Epoch 2/25, Training Loss: 0.4394, Training Accuracy: 0.8428, Training F1: 0.8416, Validation Loss: 0.3428, Validation Accuracy: 0.8737, Validation F1: 0.8764\n",
      "Epoch 3/25, Training Loss: 0.3941, Training Accuracy: 0.8577, Training F1: 0.8567, Validation Loss: 0.3247, Validation Accuracy: 0.8777, Validation F1: 0.8786\n",
      "Epoch 4/25, Training Loss: 0.3721, Training Accuracy: 0.8653, Training F1: 0.8645, Validation Loss: 0.3095, Validation Accuracy: 0.8868, Validation F1: 0.8888\n",
      "Epoch 5/25, Training Loss: 0.3533, Training Accuracy: 0.8717, Training F1: 0.8709, Validation Loss: 0.3170, Validation Accuracy: 0.8797, Validation F1: 0.8803\n",
      "Epoch 6/25, Training Loss: 0.3370, Training Accuracy: 0.8775, Training F1: 0.8768, Validation Loss: 0.3007, Validation Accuracy: 0.8860, Validation F1: 0.8877\n",
      "Epoch 7/25, Training Loss: 0.3209, Training Accuracy: 0.8832, Training F1: 0.8825, Validation Loss: 0.3010, Validation Accuracy: 0.8865, Validation F1: 0.8852\n",
      "Epoch 8/25, Training Loss: 0.3124, Training Accuracy: 0.8849, Training F1: 0.8841, Validation Loss: 0.2979, Validation Accuracy: 0.8862, Validation F1: 0.8864\n",
      "Epoch 9/25, Training Loss: 0.3041, Training Accuracy: 0.8875, Training F1: 0.8868, Validation Loss: 0.2880, Validation Accuracy: 0.8930, Validation F1: 0.8933\n",
      "Epoch 10/25, Training Loss: 0.2931, Training Accuracy: 0.8925, Training F1: 0.8918, Validation Loss: 0.2888, Validation Accuracy: 0.8933, Validation F1: 0.8934\n",
      "Epoch 11/25, Training Loss: 0.2865, Training Accuracy: 0.8941, Training F1: 0.8934, Validation Loss: 0.2833, Validation Accuracy: 0.8895, Validation F1: 0.8901\n",
      "Epoch 12/25, Training Loss: 0.2803, Training Accuracy: 0.8973, Training F1: 0.8967, Validation Loss: 0.2883, Validation Accuracy: 0.8937, Validation F1: 0.8946\n",
      "Epoch 13/25, Training Loss: 0.2731, Training Accuracy: 0.8993, Training F1: 0.8986, Validation Loss: 0.2790, Validation Accuracy: 0.8957, Validation F1: 0.8971\n",
      "Epoch 14/25, Training Loss: 0.2642, Training Accuracy: 0.9026, Training F1: 0.9021, Validation Loss: 0.2763, Validation Accuracy: 0.8957, Validation F1: 0.8974\n",
      "Epoch 15/25, Training Loss: 0.2590, Training Accuracy: 0.9031, Training F1: 0.9026, Validation Loss: 0.2761, Validation Accuracy: 0.8953, Validation F1: 0.8969\n",
      "Epoch 16/25, Training Loss: 0.2523, Training Accuracy: 0.9070, Training F1: 0.9065, Validation Loss: 0.2925, Validation Accuracy: 0.8900, Validation F1: 0.8923\n",
      "Epoch 17/25, Training Loss: 0.2503, Training Accuracy: 0.9083, Training F1: 0.9078, Validation Loss: 0.2753, Validation Accuracy: 0.8980, Validation F1: 0.8985\n",
      "Epoch 18/25, Training Loss: 0.2428, Training Accuracy: 0.9104, Training F1: 0.9100, Validation Loss: 0.2865, Validation Accuracy: 0.8943, Validation F1: 0.8971\n",
      "Epoch 19/25, Training Loss: 0.2381, Training Accuracy: 0.9114, Training F1: 0.9109, Validation Loss: 0.2918, Validation Accuracy: 0.8910, Validation F1: 0.8902\n",
      "Epoch 20/25, Training Loss: 0.2345, Training Accuracy: 0.9134, Training F1: 0.9130, Validation Loss: 0.2736, Validation Accuracy: 0.9017, Validation F1: 0.9023\n",
      "Epoch 21/25, Training Loss: 0.2295, Training Accuracy: 0.9142, Training F1: 0.9139, Validation Loss: 0.2753, Validation Accuracy: 0.8940, Validation F1: 0.8945\n",
      "Epoch 22/25, Training Loss: 0.2288, Training Accuracy: 0.9163, Training F1: 0.9158, Validation Loss: 0.2698, Validation Accuracy: 0.9043, Validation F1: 0.9056\n",
      "Epoch 23/25, Training Loss: 0.2224, Training Accuracy: 0.9166, Training F1: 0.9162, Validation Loss: 0.2766, Validation Accuracy: 0.9008, Validation F1: 0.9025\n",
      "Epoch 24/25, Training Loss: 0.2178, Training Accuracy: 0.9171, Training F1: 0.9166, Validation Loss: 0.2729, Validation Accuracy: 0.9017, Validation F1: 0.9027\n",
      "Epoch 25/25, Training Loss: 0.2137, Training Accuracy: 0.9195, Training F1: 0.9190, Validation Loss: 0.2776, Validation Accuracy: 0.8967, Validation F1: 0.8990\n",
      "Test Loss: 0.2833, Test Accuracy: 0.8990, Test F1 Score: 0.8996\n",
      "*******************************************************************************************************************************\n",
      "\n",
      "Best Model:\n",
      "Architecture: Arch1\n",
      "Learning Rate: 0.001\n",
      "Test Accuracy: 0.9047\n",
      "Test F1 Score: 0.9050\n",
      "*******************************************************************************************************************************\n",
      "*******************************************************************************************************************************\n",
      "\n",
      "All Model Test Accuracies:\n",
      "Architecture: Arch1, Learning Rate: 0.005, Test Accuracy: 0.9042\n",
      "Architecture: Arch1, Learning Rate: 0.001, Test Accuracy: 0.9047\n",
      "Architecture: Arch1, Learning Rate: 0.0009, Test Accuracy: 0.9030\n",
      "Architecture: Arch1, Learning Rate: 0.0006, Test Accuracy: 0.9037\n",
      "Architecture: Arch2, Learning Rate: 0.005, Test Accuracy: 0.8974\n",
      "Architecture: Arch2, Learning Rate: 0.001, Test Accuracy: 0.9047\n",
      "Architecture: Arch2, Learning Rate: 0.0009, Test Accuracy: 0.8986\n",
      "Architecture: Arch2, Learning Rate: 0.0006, Test Accuracy: 0.8986\n",
      "Architecture: Arch3, Learning Rate: 0.005, Test Accuracy: 0.9030\n",
      "Architecture: Arch3, Learning Rate: 0.001, Test Accuracy: 0.9041\n",
      "Architecture: Arch3, Learning Rate: 0.0009, Test Accuracy: 0.8994\n",
      "Architecture: Arch3, Learning Rate: 0.0006, Test Accuracy: 0.8990\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model_results = []\n",
    "\n",
    "\n",
    "for arch_name, arch_info in architectures.items():\n",
    "    print(f\"\\nTraining models for architecture: {arch_name}\")\n",
    "    for lr in learning_rates:\n",
    "        print(f\"  Training with learning rate: {lr}\")\n",
    "        \n",
    "        # Build the network for the current configuration\n",
    "        layers = []\n",
    "        input_dim = 784  # 28x28 images flattened\n",
    "        for layer_info in arch_info['layers']:\n",
    "            units = layer_info['units']\n",
    "            \n",
    "            # Dense layer\n",
    "            layers.append(DenseLayer(input_dim, units))\n",
    "            layers.append(BatchNormalization(units))\n",
    "            layers.append(ReLU())\n",
    "            \n",
    "            # Apply dropout\n",
    "            layers.append(Dropout(dropout_rate))\n",
    "            \n",
    "            # Update input dimension for next layer\n",
    "            input_dim = units\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(DenseLayer(input_dim, num_classes))\n",
    "        \n",
    "        # Initialize the network\n",
    "        network = NeuralNetwork(layers)\n",
    "        \n",
    "        # Train the model and capture metrics\n",
    "        train_losses, val_losses, train_accuracies, val_accuracies, train_f1s, val_f1s = train(\n",
    "            network, X_train, y_train_one_hot, X_val, y_val_one_hot, num_epochs, batch_size, lr\n",
    "        )\n",
    "        \n",
    "        # Evaluate on the test set\n",
    "        test_loss, test_accuracy, test_f1 = evaluate(network, X_test, y_test_one_hot)\n",
    "        \n",
    "        # Log the results\n",
    "        model_info = {\n",
    "            'architecture': arch_name,\n",
    "            'learning_rate': lr,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'val_accuracies': val_accuracies,\n",
    "            'train_f1s': train_f1s,\n",
    "            'val_f1s': val_f1s,\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'test_f1': test_f1,\n",
    "        }\n",
    "        model_results.append(model_info)\n",
    "\n",
    "        generate_confusion_matrix(network, X_test, y_test, arch_name, lr)\n",
    "        \n",
    "        # Check if this model is the best so far\n",
    "        if test_accuracy > best_test_accuracy:\n",
    "            best_test_accuracy = test_accuracy\n",
    "            best_model = network\n",
    "            best_model_info = model_info\n",
    "        \n",
    "        # Generate and save plots for this model\n",
    "        plot_metrics(\n",
    "            arch_name, lr, train_losses, val_losses,\n",
    "            train_accuracies, val_accuracies, train_f1s, val_f1s\n",
    "        )\n",
    "\n",
    "# Save the best model using pickle\n",
    "with open('best_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "print(\"*******************************************************************************************************************************\")\n",
    "print(\"\\nBest Model:\")\n",
    "print(f\"Architecture: {best_model_info['architecture']}\")\n",
    "print(f\"Learning Rate: {best_model_info['learning_rate']}\")\n",
    "print(f\"Test Accuracy: {best_model_info['test_accuracy']:.4f}\")\n",
    "print(f\"Test F1 Score: {best_model_info['test_f1']:.4f}\")\n",
    "print(\"*******************************************************************************************************************************\")\n",
    "\n",
    "\n",
    "plot_metrics(\n",
    "    best_model_info['architecture'],\n",
    "    best_model_info['learning_rate'],\n",
    "    best_model_info['train_losses'],\n",
    "    best_model_info['val_losses'],\n",
    "    best_model_info['train_accuracies'],\n",
    "    best_model_info['val_accuracies'],\n",
    "    best_model_info['train_f1s'],\n",
    "    best_model_info['val_f1s']\n",
    ")\n",
    "\n",
    "# Generate confusion matrix for the best model with appropriate filename\n",
    "generate_confusion_matrix(best_model, X_test, y_test, best_model_info['architecture'], best_model_info['learning_rate'])\n",
    "\n",
    "\n",
    "# Log all test accuracies\n",
    "print(\"*******************************************************************************************************************************\")\n",
    "print(\"\\nAll Model Test Accuracies:\")\n",
    "for info in model_results:\n",
    "    print(f\"Architecture: {info['architecture']}, Learning Rate: {info['learning_rate']}, \"\n",
    "          f\"Test Accuracy: {info['test_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model(model, filepath):\n",
    "    model_params = {}\n",
    "    for idx, layer in enumerate(model.layers):\n",
    "        if isinstance(layer, DenseLayer):\n",
    "            model_params[f'weight_{idx}'] = layer.W\n",
    "            model_params[f'bias_{idx}'] = layer.b\n",
    "    with open(filepath, 'wb') as file:\n",
    "        pickle.dump(model_params, file)\n",
    "\n",
    "save_model(best_model, 'best_model_str.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the best model and predict on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8577\n",
      "Test F1 Score: 0.3612\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "with open('best_model.pkl', 'rb') as f:\n",
    "    best_model = pickle.load(f)\n",
    "\n",
    "\n",
    "def evaluate_loaded_model(model, X_test, y_test_one_hot):\n",
    "    \n",
    "    logits = model.forward(X_test, training=False)\n",
    "    \n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    true_labels = np.argmax(y_test_one_hot, axis=1)\n",
    "    \n",
    "    test_accuracy = accuracy_score(true_labels, predictions)\n",
    "    test_f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    \n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=range(10), yticklabels=range(10))\n",
    "    plt.title('Confusion Matrix for Best Model')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "evaluate_loaded_model(best_model, X_test, y_test_one_hot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
